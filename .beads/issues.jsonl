{"id":"meta_skill-08m","title":"[P5] GitHub Integration","description":"# GitHub Integration\n\nPublish and install bundles from GitHub Releases.\n\n## Tasks\n1. GitHub API client for releases\n2. Publish bundle as release asset\n3. Download and install from release\n4. Version resolution from tags\n5. Authentication for private repos\n\n## Publish Workflow\n1. `ms bundle publish --repo user/skills`\n2. Create/update GitHub release\n3. Upload bundle as asset\n4. Update release notes\n\n## Install Workflow\n1. `ms bundle install user/skills`\n2. Fetch latest release (or specific version)\n3. Download bundle asset\n4. Verify checksum\n5. Extract and index\n\n## Version Tags\n- `v1.0.0` format\n- `ms bundle install user/skills@1.2.0`\n- Supports version ranges\n\n## Authentication\n- GITHUB_TOKEN environment variable\n- `gh auth` integration\n- Personal access tokens\n\n## Acceptance Criteria\n- Publish to GitHub works\n- Install from GitHub works\n- Version resolution works\n- Private repos supported","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:04.116814134-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:27:04.116814134-05:00","labels":["bundles","github","phase-5"],"dependencies":[{"issue_id":"meta_skill-08m","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.375049919-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-0an","title":"[P3] Micro-Slicing Engine","description":"# Micro-Slicing Engine\n\nPre-slice skills into atomic blocks for optimal packing.\n\n## Tasks\n1. Define SliceSpec struct (id, type, content, tokens, policy)\n2. Parse skills into slices during indexing\n3. Store slices in skill_slices table\n4. Tag slices with priority and policy flags\n5. Build slice dependency graph\n\n## Slice Types (from Section 5.2)\n- CriticalRule (always include, mandatory)\n- Command (executables, code snippets)\n- Example (demonstrations)\n- Pitfall (warnings, gotchas)\n- Checklist (step-by-step)\n- DecisionTree (conditional logic)\n- Reference (links, citations)\n\n## Slice Properties\n- id: Unique identifier\n- type: SliceType enum\n- content: Markdown content\n- token_count: Pre-computed\n- policy: mandatory | optional | conditional\n- predicates: Version/context conditions\n\n## Dependency Tracking\n- Some slices require others (e.g., example requires command)\n- Store in skill_slice_deps table\n\n## Acceptance Criteria\n- Skills sliced during indexing\n- Slice tokens counted accurately\n- Dependencies tracked","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:13.214317137-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:24:13.214317137-05:00","labels":["indexing","phase-3","slicing"],"dependencies":[{"issue_id":"meta_skill-0an","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:24:25.846060335-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-0ki","title":"[P2] ms search Command","description":"# ms search Command\n\nMain search interface for finding skills.\n\n## Subcommands\n- `ms search \"query\"` - Basic search\n- `ms search --interactive` - TUI-based search\n- `ms search --top N` - Limit results\n- `ms search --robot` - JSON output\n\n## Output Format (Human)\n```\n1. git/commit [P1] [practice]\n   \"Best practices for writing commit messages\"\n   Tags: git, workflow | Score: 0.87\n\n2. git/rebase [P2] [procedure]\n   \"Interactive rebase workflow\"\n   Tags: git, advanced | Score: 0.72\n```\n\n## Output Format (Robot)\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"query\": \"git commit\",\n    \"results\": [\n      {\n        \"id\": \"git/commit\",\n        \"score\": 0.87,\n        \"scores\": {\"bm25\": 0.9, \"embedding\": 0.8},\n        \"snippet\": \"...\"\n      }\n    ]\n  }\n}\n```\n\n## Interactive Mode\n- Arrow keys to navigate\n- Enter to select (copies/loads)\n- Preview pane shows skill content\n- Type to filter\n\n## Acceptance Criteria\n- Search returns ranked results\n- Robot mode outputs valid JSON\n- Interactive mode is intuitive","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:06.624504298-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:23:06.624504298-05:00","labels":["cli","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-93z","type":"blocks","created_at":"2026-01-13T22:23:13.620915337-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-5e6","type":"blocks","created_at":"2026-01-13T22:23:13.648348116-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-0ki","depends_on_id":"meta_skill-r6k","type":"blocks","created_at":"2026-01-13T22:23:13.674465537-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-14h","title":"[P1] CLI Commands: init, index, list, show","description":"# Basic CLI Commands\n\nImplement foundational CLI commands.\n\n## Commands\n1. `ms init` - Initialize ms in current directory or globally\n2. `ms index` - Discover and index skills from configured paths\n3. `ms list` - List indexed skills with filters\n4. `ms show \u003cid\u003e` - Display skill details\n\n## ms init\n- Create .ms/ directory\n- Initialize SQLite database\n- Create config.yaml with defaults\n- Optionally --global for ~/.ms/\n\n## ms index\n- Scan configured skill paths\n- Parse skill.spec.yaml files\n- Update SQLite registry\n- Build search indices\n- Emit .ms/skillpack.bin cache\n\n## ms list\n- Show skills table\n- Filters: --type, --layer, --tag, --status\n- Robot mode: --robot\n\n## ms show \u003cid\u003e\n- Full skill details\n- Metadata, body, dependencies\n- Robot mode: --robot\n\n## Acceptance Criteria\n- All commands work with --help\n- All commands support --robot JSON output\n- Error messages are helpful","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:05.275101079-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:22:05.275101079-05:00","labels":["cli","commands","phase-1"],"dependencies":[{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:22:14.981654513-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:22:15.008840067-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:22:15.034452446-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-14h","depends_on_id":"meta_skill-225","type":"blocks","created_at":"2026-01-13T22:54:04.187430476-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1bg","title":"[P6] Skill Versioning System","description":"# Skill Versioning System\n\nSemantic versions with migration support.\n\n## Tasks\n1. Define version schema (semver)\n2. Track version history\n3. Detect breaking changes\n4. Migration path generation\n5. Installed version tracking\n\n## Version Model (from Section 25)\n```rust\nstruct SkillVersion {\n    version: semver::Version,\n    changelog: String,\n    breaking_changes: Vec\u003cBreakingChange\u003e,\n    created_at: DateTime,\n    author: String,\n}\n```\n\n## Breaking Change Detection\n- Removed slices\n- Changed slice IDs\n- Renamed triggers\n- Modified predicates\n\n## Migration Runner\n1. Detect version gap\n2. Generate migration steps\n3. Preview changes\n4. Apply with rollback option\n\n## CLI\n- `ms version \u003cskill\u003e` - Show version info\n- `ms version \u003cskill\u003e bump minor` - Bump version\n- `ms version \u003cskill\u003e changelog` - Edit changelog\n- `ms migrate \u003cskill\u003e` - Run pending migrations\n\n## Acceptance Criteria\n- Versions tracked correctly\n- Breaking changes detected\n- Migrations work smoothly","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:24.898227549-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:28:24.898227549-05:00","labels":["migration","phase-6","versioning"],"dependencies":[{"issue_id":"meta_skill-1bg","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:28:37.089513634-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1bg","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:28:37.118898402-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1jl","title":"[P3] Conditional Predicates","description":"# Conditional Predicates\n\nVersion/context-specific content filtering.\n\n## Tasks\n1. Define Predicate expression language\n2. Parse predicates from skill spec\n3. Evaluate predicates at load time\n4. Filter content based on results\n5. Support common predicates: version, os, env\n\n## Predicate Syntax (from Section 5.4)\n```yaml\npredicates:\n  python\u003e=3.10:\n    - slice-id-1\n    - slice-id-2\n  rust:\n    - slice-id-3\n```\n\n## Built-in Predicates\n- `python\u003e=3.10` - Language version check\n- `os:linux` - Operating system\n- `env:production` - Environment variable\n- `has:docker` - Tool availability\n\n## Evaluation\n- Predicates evaluated when skill loaded\n- Non-matching slices filtered out\n- Agent never sees irrelevant content\n\n## Acceptance Criteria\n- Predicates parse correctly\n- Filtering works at load time\n- Agent gets clean content","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:24:15.035776247-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:24:15.035776247-05:00","labels":["filtering","phase-3","predicates"],"dependencies":[{"issue_id":"meta_skill-1jl","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T22:24:25.926673919-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1p7","title":"[P4] Provenance Graph","description":"# Provenance Graph\n\nLink skill rules to source evidence.\n\n## Tasks\n1. Define ProvenanceRef struct\n2. Store compressed pointers (session_id + byte range)\n3. Fetch-on-demand for full evidence\n4. Display in skill metadata\n5. Support multiple sources per rule\n\n## Provenance Storage (from Section 8.4)\n- Compressed: Just pointers (session_id, start, end)\n- Full evidence fetched on demand\n- Reduces skill size by 10x+\n\n## Data Model\n```rust\nstruct ProvenanceRef {\n    session_id: String,\n    byte_start: usize,\n    byte_end: usize,\n    cached_excerpt: Option\u003cString\u003e,  // Optional inline cache\n}\n\nstruct RuleProvenance {\n    rule_id: String,\n    sources: Vec\u003cProvenanceRef\u003e,\n}\n```\n\n## CLI\n- `ms show skill --provenance` - Show evidence for each rule\n- `ms trace rule-id` - Fetch full context for a rule\n\n## Acceptance Criteria\n- Provenance stored efficiently\n- Full evidence fetchable\n- Multiple sources per rule supported","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:48.342543267-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:48.342543267-05:00","labels":["evidence","phase-4","provenance"],"dependencies":[{"issue_id":"meta_skill-1p7","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:13.048393324-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-1w2","title":"[P6] Agent Mail Integration","description":"# Agent Mail Integration\n\nMulti-agent skill coordination via Agent Mail MCP.\n\n## Tasks\n1. Agent Mail client integration\n2. Skill announcement protocol\n3. Pattern sharing between agents\n4. Skill request/fulfillment\n5. Swarm coordination\n\n## Use Cases (from Section 20)\n1. Share discovered patterns in real-time\n2. Coordinate skill generation (avoid duplication)\n3. Request skills from specialized agents\n4. Notify when new skills ready\n\n## Message Types\n- skill_build_start: Agent starting skill generation\n- skill_build_complete: Skill ready for use\n- pattern_share: Sharing extracted patterns\n- skill_request: Requesting skill from others\n- skill_response: Responding to request\n\n## Integration Pattern\n```rust\nstruct AgentMailClient {\n    project_key: String,\n    agent_name: String,\n    mcp_endpoint: String,\n}\n\nimpl AgentMailClient {\n    async fn announce_build_start(\u0026self, topic: \u0026str);\n    async fn announce_build_complete(\u0026self, skill_id: \u0026str);\n    async fn share_patterns(\u0026self, patterns: \u0026[Pattern]);\n}\n```\n\n## Fallback\n- If Agent Mail unavailable, local-only operation\n- No blocking on network\n\n## Acceptance Criteria\n- Agents can coordinate skill building\n- Patterns shared successfully\n- Graceful fallback when offline","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-13T22:28:28.106184603-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:42:06.982463956-05:00","closed_at":"2026-01-13T23:42:06.982463956-05:00","close_reason":"Duplicate of meta_skill-tzu (Agent Mail Integration)","labels":["agent-mail","coordination","phase-6"],"dependencies":[{"issue_id":"meta_skill-1w2","depends_on_id":"meta_skill-ugf","type":"blocks","created_at":"2026-01-13T22:28:37.203040025-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-1w2","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:28:37.230093279-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-225","title":"Skill Layering \u0026 Conflict Resolution","description":"## Overview\n\nImplement the skill layering and conflict resolution system for meta_skill (Section 3.5 of PLAN_TO_MAKE_METASKILL_CLI.md). Skills exist in layers (system \u003c global \u003c project \u003c session) where higher layers override lower. The system must detect conflicts and resolve them according to configurable strategies.\n\n## Background \u0026 Rationale\n\n### Why Layering Matters\nDifferent contexts require different skill configurations:\n- **System layer**: Base skills shipped with ms, read-only\n- **Global layer**: User's personal customizations (~/.config/ms/skills/)\n- **Project layer**: Team/project-specific skills (.ms/skills/)\n- **Session layer**: Temporary overrides for current session\n\nWithout layering:\n- Users can't customize system defaults\n- Projects can't enforce team standards\n- No way to experiment without permanent changes\n\n### Why Conflict Resolution Matters\nWhen the same skill exists in multiple layers:\n- Which version wins?\n- Should they merge?\n- How do we alert users to conflicts?\n\n### Design Goals\n1. **Predictable Override Order**: Higher layers always win by default\n2. **Configurable Strategy**: Users can choose merge vs replace\n3. **Transparency**: Clear visibility into which layer provides each skill\n4. **Audit Trail**: Log all conflict resolution decisions\n\n## Key Data Structures (from Plan Section 3.5)\n\n```rust\n/// The four skill layers in priority order (lowest to highest)\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]\nenum SkillLayer {\n    /// Built-in skills shipped with ms\n    System = 0,\n    /// User's global skills (~/.config/ms/skills/)\n    Global = 1,\n    /// Project-specific skills (.ms/skills/)\n    Project = 2,\n    /// Temporary session overrides\n    Session = 3,\n}\n\n/// Registry that maintains skills across all layers\nstruct LayeredRegistry {\n    /// Ordered list of active layers (lowest to highest priority)\n    layers: Vec\u003cSkillLayer\u003e,\n    /// Per-layer skill registries\n    registries: HashMap\u003cSkillLayer, Registry\u003e,\n    /// Conflict resolution configuration\n    conflict_config: ConflictConfig,\n}\n\n/// Single-layer skill registry\nstruct Registry {\n    /// Skills indexed by ID\n    skills: HashMap\u003cSkillId, SkillSpec\u003e,\n    /// Source paths for each skill (for debugging)\n    sources: HashMap\u003cSkillId, PathBuf\u003e,\n}\n\n/// How to resolve conflicts between layers\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\nenum ConflictStrategy {\n    /// Higher layer completely replaces lower (default)\n    PreferHigher,\n    /// Lower layer wins (unusual, for pinning)\n    PreferLower,\n    /// Merge skills according to MergeStrategy\n    Merge,\n    /// Prompt user interactively (CLI only)\n    Interactive,\n}\n\n/// How to merge conflicting skills\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\nenum MergeStrategy {\n    /// Higher layer completely replaces all blocks\n    ReplaceAll,\n    /// Keep blocks from both, higher wins on collision\n    PreferSections,\n    /// Append examples/tips from higher to lower\n    AppendExamples,\n}\n\n/// Conflict resolution configuration\nstruct ConflictConfig {\n    /// Default strategy for all conflicts\n    default_strategy: ConflictStrategy,\n    /// Per-skill overrides\n    skill_overrides: HashMap\u003cSkillId, ConflictStrategy\u003e,\n    /// Merge strategy when using Merge conflict strategy\n    merge_strategy: MergeStrategy,\n}\n\n/// Information about a detected conflict\nstruct ConflictInfo {\n    /// The skill ID with conflicting definitions\n    skill_id: SkillId,\n    /// Layers that have this skill\n    conflicting_layers: Vec\u003cSkillLayer\u003e,\n    /// How it was resolved\n    resolution: ConflictResolution,\n}\n\n/// How a conflict was actually resolved\nstruct ConflictResolution {\n    /// Which layer's version was used\n    winning_layer: SkillLayer,\n    /// Strategy that was applied\n    strategy_applied: ConflictStrategy,\n    /// If merged, what changed\n    merge_details: Option\u003cMergeDetails\u003e,\n}\n```\n\n## Tasks\n\n### Task 1: Define Layer Types\n- [ ] Create `src/layers/mod.rs` module\n- [ ] Define `SkillLayer` enum with System, Global, Project, Session variants\n- [ ] Implement `Ord` for layer priority comparison\n- [ ] Implement `Display` for human-readable output\n- [ ] Add `SkillLayer::path()` method returning default path for each layer\n- [ ] Add `SkillLayer::from_path(path)` to detect layer from file location\n\n### Task 2: Implement Single-Layer Registry\n- [ ] Define `Registry` struct with skills HashMap\n- [ ] Implement `Registry::new()` constructor\n- [ ] Implement `Registry::add_skill(skill)` method\n- [ ] Implement `Registry::get_skill(id)` method\n- [ ] Implement `Registry::remove_skill(id)` method\n- [ ] Implement `Registry::list_skills()` iterator\n- [ ] Add source path tracking for debugging\n\n### Task 3: Implement Layered Registry\n- [ ] Define `LayeredRegistry` struct\n- [ ] Implement `LayeredRegistry::new()` with default layer order\n- [ ] Implement `add_layer(layer)` to add new layer\n- [ ] Implement `get_skill(id)` with layer resolution\n- [ ] Implement `list_skills()` returning merged view\n- [ ] Track which layer each skill comes from\n\n### Task 4: Implement Conflict Detection\n- [ ] Detect when skill exists in multiple layers\n- [ ] Create `ConflictInfo` struct with conflict details\n- [ ] Implement `detect_conflicts()` returning all conflicts\n- [ ] Implement `has_conflicts(skill_id)` check\n- [ ] Log all detected conflicts with layer details\n\n### Task 5: Implement Conflict Strategies\n- [ ] Implement `PreferHigher` strategy (default)\n- [ ] Implement `PreferLower` strategy\n- [ ] Implement `Merge` strategy with configurable merge behavior\n- [ ] Implement `Interactive` strategy (placeholder for CLI)\n- [ ] Make strategy configurable per-skill\n\n### Task 6: Implement Merge Strategies\n- [ ] Implement `ReplaceAll` merge (higher replaces entirely)\n- [ ] Implement `PreferSections` merge (block-level merging)\n- [ ] Implement `AppendExamples` merge (combine examples)\n- [ ] Handle block ID conflicts during merge\n- [ ] Preserve metadata from appropriate layer\n\n### Task 7: Implement Layer Loading\n- [ ] Load system skills from installation directory\n- [ ] Load global skills from ~/.config/ms/skills/\n- [ ] Load project skills from .ms/skills/\n- [ ] Handle missing directories gracefully\n- [ ] Validate skills on load\n\n### Task 8: Implement Layer Introspection\n- [ ] Create `layer_info(skill_id)` returning layer details\n- [ ] Create `list_by_layer(layer)` returning skills in layer\n- [ ] Create `effective_skill(id)` returning resolved skill with provenance\n- [ ] Add `--layer` flag support for CLI commands\n- [ ] Support `ms skills --show-layers` output\n\n## Acceptance Criteria\n\n1. **Layer Priority**: Higher layers always override lower by default\n2. **Conflict Detection**: All skill conflicts detected and reported\n3. **Strategy Selection**: ConflictStrategy variants work correctly\n4. **Merge Behavior**: MergeStrategy variants merge correctly\n5. **Path Detection**: Correctly detects layer from file path\n6. **Introspection**: Can query which layer provides each skill\n7. **Performance**: Layer resolution adds \u003c1ms overhead\n8. **Graceful Handling**: Missing layers don't cause errors\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[test]\nfn test_layer_ordering() {\n    assert!(SkillLayer::System \u003c SkillLayer::Global);\n    assert!(SkillLayer::Global \u003c SkillLayer::Project);\n    assert!(SkillLayer::Project \u003c SkillLayer::Session);\n}\n\n#[test]\nfn test_higher_layer_wins() {\n    let mut registry = LayeredRegistry::new();\n    \n    // Add skill to system layer\n    let system_skill = SkillSpec::new(\"test-skill\", \"System version\");\n    registry.add_skill(SkillLayer::System, system_skill);\n    \n    // Add same skill to project layer\n    let project_skill = SkillSpec::new(\"test-skill\", \"Project version\");\n    registry.add_skill(SkillLayer::Project, project_skill);\n    \n    // Project version should win\n    let resolved = registry.get_skill(\"test-skill\").unwrap();\n    assert_eq!(resolved.description(), \"Project version\");\n    assert_eq!(resolved.source_layer(), SkillLayer::Project);\n}\n\n#[test]\nfn test_conflict_detection() {\n    let mut registry = LayeredRegistry::new();\n    \n    registry.add_skill(SkillLayer::System, SkillSpec::new(\"conflicted\", \"v1\"));\n    registry.add_skill(SkillLayer::Global, SkillSpec::new(\"conflicted\", \"v2\"));\n    \n    let conflicts = registry.detect_conflicts();\n    assert_eq!(conflicts.len(), 1);\n    assert_eq!(conflicts[0].skill_id, \"conflicted\");\n    assert!(conflicts[0].conflicting_layers.contains(\u0026SkillLayer::System));\n    assert!(conflicts[0].conflicting_layers.contains(\u0026SkillLayer::Global));\n}\n\n#[test]\nfn test_prefer_lower_strategy() {\n    let mut registry = LayeredRegistry::new();\n    registry.set_strategy(\"pinned-skill\", ConflictStrategy::PreferLower);\n    \n    registry.add_skill(SkillLayer::System, SkillSpec::new(\"pinned-skill\", \"System\"));\n    registry.add_skill(SkillLayer::Global, SkillSpec::new(\"pinned-skill\", \"Global\"));\n    \n    let resolved = registry.get_skill(\"pinned-skill\").unwrap();\n    assert_eq!(resolved.description(), \"System\");\n}\n\n#[test]\nfn test_merge_append_examples() {\n    let mut registry = LayeredRegistry::new();\n    registry.set_merge_strategy(MergeStrategy::AppendExamples);\n    registry.set_strategy(\"merged-skill\", ConflictStrategy::Merge);\n    \n    let mut system_skill = SkillSpec::new(\"merged-skill\", \"Base\");\n    system_skill.add_example(\"Example 1\");\n    \n    let mut project_skill = SkillSpec::new(\"merged-skill\", \"Base\");\n    project_skill.add_example(\"Example 2\");\n    \n    registry.add_skill(SkillLayer::System, system_skill);\n    registry.add_skill(SkillLayer::Project, project_skill);\n    \n    let resolved = registry.get_skill(\"merged-skill\").unwrap();\n    assert_eq!(resolved.examples().len(), 2);\n}\n\n#[test]\nfn test_layer_from_path() {\n    assert_eq!(\n        SkillLayer::from_path(\"/usr/share/ms/skills/core.skill\"),\n        Some(SkillLayer::System)\n    );\n    assert_eq!(\n        SkillLayer::from_path(\"/home/user/.config/ms/skills/custom.skill\"),\n        Some(SkillLayer::Global)\n    );\n    assert_eq!(\n        SkillLayer::from_path(\"/project/.ms/skills/team.skill\"),\n        Some(SkillLayer::Project)\n    );\n}\n\n#[test]\nfn test_list_shows_all_layers() {\n    let mut registry = LayeredRegistry::new();\n    registry.add_skill(SkillLayer::System, SkillSpec::new(\"sys\", \"\"));\n    registry.add_skill(SkillLayer::Global, SkillSpec::new(\"global\", \"\"));\n    registry.add_skill(SkillLayer::Project, SkillSpec::new(\"proj\", \"\"));\n    \n    let all_skills = registry.list_skills();\n    assert_eq!(all_skills.len(), 3);\n}\n```\n\n### Logging Requirements\nAll operations must log with appropriate levels:\n- `DEBUG`: Layer resolution steps, merge operations\n- `INFO`: Conflict resolutions, layer loading complete\n- `WARN`: Skills overridden, unusual strategies applied\n- `ERROR`: Layer load failures, invalid merge attempts\n\nExample log output:\n```\n[INFO] Loading skill layers...\n[DEBUG] Loading system layer from /usr/share/ms/skills/\n[DEBUG] Found 15 skills in system layer\n[DEBUG] Loading global layer from /home/user/.config/ms/skills/\n[DEBUG] Found 3 skills in global layer\n[INFO] Layer loading complete: 18 skills across 2 layers\n[WARN] Conflict detected: 'rust-basics' exists in [System, Global]\n[DEBUG] Resolving conflict with PreferHigher strategy\n[INFO] Resolved: 'rust-basics' from Global layer wins\n```\n\n## References\n\n- Plan Section 3.5: Layering \u0026 Conflict Resolution\n- Plan Section 5.1: Skill discovery paths\n- Depends on: meta_skill-ik6 (SkillSpec Data Model)\n- Blocks: meta_skill-14h (CLI Commands)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:52:47.294634944-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:52:47.294634944-05:00","labels":["conflicts","layers","phase-1"],"dependencies":[{"issue_id":"meta_skill-225","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:54:03.394720272-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-237","title":"[P4] Pattern Extraction Pipeline","description":"# Pattern Extraction Pipeline\n\nMine patterns from CASS sessions.\n\n## Tasks\n1. Define Pattern IR (intermediate representation)\n2. Implement extractors for different pattern types\n3. Command extraction (shell commands, their context)\n4. Solution extraction (what worked)\n5. Pitfall extraction (what didn't work)\n6. Frequency counting across sessions\n\n## Pattern Types (from Section 8.2)\n- Commands: Executed commands with context\n- Solutions: Successful resolution patterns\n- Pitfalls: Failure modes and fixes\n- Workflows: Multi-step sequences\n- Rules: Derived guidelines\n\n## Pattern IR\n```rust\nstruct ExtractedPattern {\n    pattern_type: PatternType,\n    content: String,\n    frequency: u32,\n    sessions: Vec\u003cSessionRef\u003e,\n    confidence: f32,\n    context: PatternContext,\n}\n```\n\n## Extraction Algorithm\n1. Query CASS for topic\n2. Retrieve matching sessions\n3. Parse session content\n4. Identify pattern candidates\n5. Aggregate by similarity\n6. Rank by frequency + recency\n\n## Acceptance Criteria\n- Patterns extracted from sessions\n- Frequency counted correctly\n- Context preserved","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:46.2225012-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:46.2225012-05:00","labels":["extraction","patterns","phase-4"],"dependencies":[{"issue_id":"meta_skill-237","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T22:26:12.937095954-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-27c","title":"UBS (Ultimate Bug Scanner) Integration","description":"## Section Reference\nIntegration with existing tooling - UBS (Ultimate Bug Scanner)\n\n## Overview\n\nIntegrate UBS from /data/projects/ultimate_bug_scanner as the static analysis layer for ms. Per AGENTS.md golden rule: \"ubs \u003cchanged-files\u003e before every commit. Exit 0 = safe. Exit \u003e0 = fix \u0026 re-run.\"\n\n## Why UBS Integration\n\n| UBS Feature | ms Application |\n|------------|----------------|\n| **Static analysis** | Validate extracted code patterns |\n| **Multi-language** | Go, Rust, TypeScript support |\n| **Exit codes** | Clear pass/fail for CI |\n| **Suggested fixes** | Include in skill pitfalls |\n\n## Integration Architecture\n\n```rust\n/// UBS client for static analysis\nstruct UbsClient {\n    /// Path to ubs binary\n    ubs_path: PathBuf,\n}\n\nimpl UbsClient {\n    /// Run UBS on files\n    async fn check(\u0026self, files: \u0026[PathBuf]) -\u003e Result\u003cUbsResult\u003e {\n        // Call: ubs file1.go file2.go\n    }\n    \n    /// Run UBS on staged git files\n    async fn check_staged(\u0026self) -\u003e Result\u003cUbsResult\u003e {\n        // Call: ubs $(git diff --name-only --cached)\n    }\n    \n    /// Check entire directory\n    async fn check_dir(\u0026self, dir: \u0026Path, only: Option\u003c\u0026str\u003e) -\u003e Result\u003cUbsResult\u003e {\n        // Call: ubs --only=\u003clang\u003e \u003cdir\u003e\n    }\n}\n\nstruct UbsResult {\n    exit_code: i32,\n    findings: Vec\u003cUbsFinding\u003e,\n    summary: String,\n}\n\nstruct UbsFinding {\n    category: String,\n    severity: UbsSeverity,\n    file: PathBuf,\n    line: u32,\n    column: u32,\n    message: String,\n    suggested_fix: Option\u003cString\u003e,\n}\n\nenum UbsSeverity {\n    Critical,  // nil deref, div by zero, race conditions\n    Important, // error handling, unchecked assertions\n    Contextual, // TODOs, unused vars\n}\n```\n\n## ms Integration Points\n\n### 1. Pre-Commit Hook for Skills\n\nValidate skill code snippets before publishing:\n\n```rust\nimpl SkillValidator {\n    async fn validate_code_snippets(\u0026self, skill: \u0026SkillSpec) -\u003e Result\u003cValidationResult\u003e {\n        // Extract code blocks from skill\n        let code_blocks = skill.extract_code_blocks();\n        \n        // Write to temp files\n        let temp_files = self.write_temp_files(\u0026code_blocks)?;\n        \n        // Run UBS\n        let ubs_result = self.ubs.check(\u0026temp_files).await?;\n        \n        if ubs_result.exit_code != 0 {\n            return Err(ValidationError::UbsFindings(ubs_result.findings));\n        }\n        \n        Ok(ValidationResult::Clean)\n    }\n}\n```\n\n### 2. Pattern Extraction Quality Gate\n\nDon't extract patterns from code with UBS findings:\n\n```rust\nimpl PatternExtractor {\n    async fn extract(\u0026self, session: \u0026Session) -\u003e Result\u003cVec\u003cPattern\u003e\u003e {\n        let code_blocks = session.extract_code_changes();\n        \n        // Run UBS on extracted code\n        let ubs_result = self.ubs.check_code(\u0026code_blocks).await?;\n        \n        // Skip patterns from code with critical findings\n        if ubs_result.has_critical_findings() {\n            log::warn!(\"Skipping patterns from code with UBS findings\");\n            return Ok(vec![]);\n        }\n        \n        // Continue extraction\n        self.extract_patterns(session)\n    }\n}\n```\n\n### 3. CI Integration\n\nRun UBS as part of skill validation in CI:\n\n```yaml\n# .github/workflows/skill-validation.yml\n- name: Validate skill code\n  run: |\n    for skill in skills/*.skill.yaml; do\n      ms validate --ubs \"$skill\"\n    done\n```\n\n## CLI Commands\n\n```bash\n# Validate skill with UBS\nms validate --ubs \u003cskill\u003e\n\n# Check extracted code quality\nms build --ubs-check\n\n# Pre-commit hook\nms pre-commit  # Runs UBS on changed files\n```\n\n## Tasks\n\n1. [ ] Implement UbsClient wrapper\n2. [ ] Add --ubs flag to validate command\n3. [ ] Integrate UBS in pattern extraction\n4. [ ] Add pre-commit hook command\n5. [ ] Document UBS installation requirements\n\n## Testing Requirements\n\n- UBS integration tests\n- Code block extraction accuracy\n- Pre-commit hook tests\n- CI integration tests\n\n## Acceptance Criteria\n\n- UBS detected and integrated\n- Skills validated for code quality\n- Patterns not extracted from bad code\n- Pre-commit hook functional\n- Graceful fallback when UBS unavailable\n\n## References\n\n- UBS repository: /data/projects/ultimate_bug_scanner\n- AGENTS.md UBS section","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T23:09:53.868716824-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:09:53.868716824-05:00","labels":["cross-cutting quality static-analysis"],"dependencies":[{"issue_id":"meta_skill-27c","depends_on_id":"meta_skill-9ok","type":"blocks","created_at":"2026-01-13T23:09:59.044933274-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-2kd","title":"E2E Test Scripts","description":"## Overview\n\nCreate comprehensive end-to-end test scripts that exercise full workflows with detailed logging. These tests simulate real user scenarios from start to finish, verifying the entire system works correctly as an integrated whole.\n\n## Requirements\n\n### 1. Test Scenarios\n\n#### Scenario 1: Fresh Install to Search Workflow\n```rust\n#[tokio::test]\nasync fn test_fresh_install_to_search() {\n    let fixture = E2EFixture::new(\"fresh_install_to_search\").await;\n    \n    // Step 1: Initialize fresh installation\n    fixture.log_step(\"Initialize fresh installation\");\n    let output = fixture.run_ms(\u0026[\"init\"]).await;\n    fixture.assert_success(\u0026output, \"init\");\n    fixture.checkpoint(\"post_init\");\n    \n    // Step 2: Create test skills\n    fixture.log_step(\"Create test skills\");\n    fixture.create_skill(\"rust-patterns\", r#\"\n---\nname: rust-patterns\ndescription: Common Rust design patterns and idioms\ntags: [rust, patterns, design]\n---\n# Rust Patterns\nCommon patterns for Rust development including error handling, \nbuilder pattern, type state, and newtype pattern.\n\"#);\n    fixture.checkpoint(\"skills_created\");\n    \n    // Step 3: Index skills\n    fixture.log_step(\"Index skills\");\n    let output = fixture.run_ms(\u0026[\"index\"]).await;\n    fixture.assert_success(\u0026output, \"index\");\n    fixture.checkpoint(\"post_index\");\n    \n    // Step 4: Search for skills\n    fixture.log_step(\"Search for skills\");\n    let output = fixture.run_ms(\u0026[\"search\", \"rust patterns\"]).await;\n    fixture.assert_success(\u0026output, \"search\");\n    fixture.assert_output_contains(\u0026output, \"rust-patterns\");\n    fixture.checkpoint(\"post_search\");\n    \n    // Step 5: Load skill and verify output\n    fixture.log_step(\"Load skill\");\n    let output = fixture.run_ms(\u0026[\"load\", \"rust-patterns\"]).await;\n    fixture.assert_success(\u0026output, \"load\");\n    fixture.assert_output_contains(\u0026output, \"Rust Patterns\");\n    fixture.checkpoint(\"post_load\");\n    \n    // Generate report\n    fixture.generate_report();\n}\n```\n\n#### Scenario 2: Skill Creation Workflow (CASS Integration)\n```rust\n#[tokio::test]\nasync fn test_skill_creation_workflow() {\n    let fixture = E2EFixture::with_mock_cass(\"skill_creation_workflow\").await;\n    \n    // Step 1: Start session for skill mining\n    fixture.log_step(\"Start CASS session\");\n    let session_id = fixture.create_mock_session(r#\"\n        User is implementing a complex async state machine in Rust\n        with proper error handling and cancellation support.\n    \"#).await;\n    fixture.checkpoint(\"session_created\");\n    \n    // Step 2: Mine skill from session\n    fixture.log_step(\"Mine skill from session\");\n    let output = fixture.run_ms(\u0026[\"build\", \"--from-session\", \u0026session_id]).await;\n    fixture.assert_success(\u0026output, \"build\");\n    fixture.checkpoint(\"post_build\");\n    \n    // Step 3: Validate mined skill\n    fixture.log_step(\"Validate skill\");\n    let output = fixture.run_ms(\u0026[\"validate\", \"async-state-machine\"]).await;\n    fixture.assert_success(\u0026output, \"validate\");\n    fixture.checkpoint(\"post_validate\");\n    \n    // Step 4: Publish skill locally\n    fixture.log_step(\"Publish skill\");\n    let output = fixture.run_ms(\u0026[\"publish\", \"async-state-machine\", \"--local\"]).await;\n    fixture.assert_success(\u0026output, \"publish\");\n    fixture.checkpoint(\"post_publish\");\n    \n    // Step 5: Verify skill is searchable\n    fixture.log_step(\"Verify searchable\");\n    let output = fixture.run_ms(\u0026[\"search\", \"async state machine\"]).await;\n    fixture.assert_success(\u0026output, \"search\");\n    fixture.assert_output_contains(\u0026output, \"async-state-machine\");\n    fixture.checkpoint(\"verification_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n#### Scenario 3: Bundle Workflow\n```rust\n#[tokio::test]\nasync fn test_bundle_workflow() {\n    let fixture = E2EFixture::new(\"bundle_workflow\").await;\n    \n    // Setup: Create skills to bundle\n    fixture.log_step(\"Setup test skills\");\n    for i in 1..=5 {\n        fixture.create_skill(\n            \u0026format!(\"bundle-skill-{}\", i),\n            \u0026format!(\"Test skill {} for bundle testing\", i)\n        );\n    }\n    fixture.run_ms(\u0026[\"index\"]).await;\n    fixture.checkpoint(\"skills_indexed\");\n    \n    // Step 1: Create bundle\n    fixture.log_step(\"Create bundle\");\n    let output = fixture.run_ms(\u0026[\n        \"bundle\", \"create\", \"test-bundle\",\n        \"--skills\", \"bundle-skill-1,bundle-skill-2,bundle-skill-3\",\n        \"--description\", \"Test bundle for E2E testing\"\n    ]).await;\n    fixture.assert_success(\u0026output, \"bundle create\");\n    fixture.checkpoint(\"bundle_created\");\n    \n    // Step 2: Publish bundle\n    fixture.log_step(\"Publish bundle\");\n    let output = fixture.run_ms(\u0026[\"bundle\", \"publish\", \"test-bundle\", \"--local\"]).await;\n    fixture.assert_success(\u0026output, \"bundle publish\");\n    fixture.checkpoint(\"bundle_published\");\n    \n    // Step 3: Simulate fresh system install\n    fixture.log_step(\"Simulate fresh install\");\n    let fresh_fixture = E2EFixture::new(\"fresh_install\").await;\n    fresh_fixture.run_ms(\u0026[\"init\"]).await;\n    \n    // Step 4: Install bundle on fresh system\n    fixture.log_step(\"Install bundle on fresh system\");\n    let bundle_path = fixture.get_bundle_path(\"test-bundle\");\n    let output = fresh_fixture.run_ms(\u0026[\"bundle\", \"install\", bundle_path.to_str().unwrap()]).await;\n    fresh_fixture.assert_success(\u0026output, \"bundle install\");\n    fixture.checkpoint(\"bundle_installed\");\n    \n    // Step 5: Verify skills available on fresh system\n    fixture.log_step(\"Verify skills on fresh system\");\n    let output = fresh_fixture.run_ms(\u0026[\"list\"]).await;\n    fresh_fixture.assert_output_contains(\u0026output, \"bundle-skill-1\");\n    fresh_fixture.assert_output_contains(\u0026output, \"bundle-skill-2\");\n    fixture.checkpoint(\"verification_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n#### Scenario 4: Multi-Machine Sync\n```rust\n#[tokio::test]\nasync fn test_multi_machine_sync() {\n    // Machine A setup\n    let machine_a = E2EFixture::new(\"machine_a\").await;\n    machine_a.run_ms(\u0026[\"init\"]).await;\n    machine_a.create_skill(\"sync-test-skill\", \"Skill for sync testing\");\n    machine_a.run_ms(\u0026[\"index\"]).await;\n    machine_a.checkpoint(\"machine_a_setup\");\n    \n    // Export from Machine A\n    machine_a.log_step(\"Export from Machine A\");\n    let output = machine_a.run_ms(\u0026[\"export\", \"--format\", \"portable\"]).await;\n    machine_a.assert_success(\u0026output, \"export\");\n    let export_path = machine_a.get_export_path();\n    machine_a.checkpoint(\"exported\");\n    \n    // Machine B setup\n    let machine_b = E2EFixture::new(\"machine_b\").await;\n    machine_b.run_ms(\u0026[\"init\"]).await;\n    machine_b.checkpoint(\"machine_b_setup\");\n    \n    // Import on Machine B\n    machine_b.log_step(\"Import on Machine B\");\n    let output = machine_b.run_ms(\u0026[\"import\", export_path.to_str().unwrap()]).await;\n    machine_b.assert_success(\u0026output, \"import\");\n    machine_b.checkpoint(\"imported\");\n    \n    // Verify on Machine B\n    machine_b.log_step(\"Verify on Machine B\");\n    let output = machine_b.run_ms(\u0026[\"list\"]).await;\n    machine_b.assert_output_contains(\u0026output, \"sync-test-skill\");\n    machine_b.checkpoint(\"verification_complete\");\n    \n    machine_a.generate_report();\n    machine_b.generate_report();\n}\n```\n\n#### Scenario 5: Error Recovery\n```rust\n#[tokio::test]\nasync fn test_error_recovery() {\n    let fixture = E2EFixture::new(\"error_recovery\").await;\n    fixture.run_ms(\u0026[\"init\"]).await;\n    \n    // Create skills\n    for i in 1..=10 {\n        fixture.create_skill(\n            \u0026format!(\"recovery-skill-{}\", i),\n            \u0026format!(\"Skill {} for recovery testing\", i)\n        );\n    }\n    fixture.checkpoint(\"skills_created\");\n    \n    // Step 1: Start build process\n    fixture.log_step(\"Start build that will be interrupted\");\n    let build_handle = fixture.run_ms_async(\u0026[\"build\", \"--all\"]).await;\n    \n    // Step 2: Interrupt after partial completion\n    fixture.log_step(\"Interrupt build\");\n    tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n    fixture.interrupt_process(build_handle);\n    fixture.checkpoint(\"interrupted\");\n    \n    // Step 3: Check state after interruption\n    fixture.log_step(\"Check state after interruption\");\n    fixture.verify_no_corruption();\n    fixture.checkpoint(\"state_verified\");\n    \n    // Step 4: Resume build\n    fixture.log_step(\"Resume build\");\n    let output = fixture.run_ms(\u0026[\"build\", \"--resume\"]).await;\n    fixture.assert_success(\u0026output, \"build resume\");\n    fixture.checkpoint(\"resumed\");\n    \n    // Step 5: Verify final state\n    fixture.log_step(\"Verify final state\");\n    let output = fixture.run_ms(\u0026[\"list\"]).await;\n    for i in 1..=10 {\n        fixture.assert_output_contains(\u0026output, \u0026format!(\"recovery-skill-{}\", i));\n    }\n    fixture.checkpoint(\"verification_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n#### Scenario 6: Performance Regression\n```rust\n#[tokio::test]\nasync fn test_performance_regression() {\n    let fixture = E2EFixture::new(\"performance_regression\").await;\n    fixture.run_ms(\u0026[\"init\"]).await;\n    \n    // Create many skills for performance testing\n    fixture.log_step(\"Create 1000 skills\");\n    for i in 1..=1000 {\n        fixture.create_skill(\n            \u0026format!(\"perf-skill-{}\", i),\n            \u0026format!(\"Performance test skill number {} with various keywords\", i)\n        );\n    }\n    fixture.checkpoint(\"skills_created\");\n    \n    // Benchmark: Index time\n    fixture.log_step(\"Benchmark: Index\");\n    let start = std::time::Instant::now();\n    let output = fixture.run_ms(\u0026[\"index\"]).await;\n    let index_time = start.elapsed();\n    fixture.assert_success(\u0026output, \"index\");\n    fixture.record_benchmark(\"index_1000_skills\", index_time);\n    fixture.assert_under_threshold(\"index_1000_skills\", Duration::from_secs(10));\n    fixture.checkpoint(\"index_complete\");\n    \n    // Benchmark: Search time (p99)\n    fixture.log_step(\"Benchmark: Search p99\");\n    let mut search_times = Vec::new();\n    for query in \u0026[\"rust\", \"error handling\", \"async await\", \"performance\", \"testing\"] {\n        let start = std::time::Instant::now();\n        fixture.run_ms(\u0026[\"search\", query]).await;\n        search_times.push(start.elapsed());\n    }\n    search_times.sort();\n    let p99 = search_times[search_times.len() * 99 / 100];\n    fixture.record_benchmark(\"search_p99\", p99);\n    fixture.assert_under_threshold(\"search_p99\", Duration::from_millis(50));\n    fixture.checkpoint(\"search_benchmark_complete\");\n    \n    // Benchmark: Load time\n    fixture.log_step(\"Benchmark: Load\");\n    let start = std::time::Instant::now();\n    fixture.run_ms(\u0026[\"load\", \"perf-skill-500\"]).await;\n    let load_time = start.elapsed();\n    fixture.record_benchmark(\"load_skill\", load_time);\n    fixture.assert_under_threshold(\"load_skill\", Duration::from_millis(100));\n    fixture.checkpoint(\"load_benchmark_complete\");\n    \n    fixture.generate_report();\n}\n```\n\n### 2. E2E Fixture Implementation\n\n```rust\npub struct E2EFixture {\n    inner: TestFixture,\n    steps: Vec\u003cTestStep\u003e,\n    checkpoints: Vec\u003cCheckpoint\u003e,\n    benchmarks: HashMap\u003cString, Duration\u003e,\n}\n\nstruct TestStep {\n    number: usize,\n    name: String,\n    timestamp: DateTime\u003cUtc\u003e,\n}\n\nstruct Checkpoint {\n    name: String,\n    timestamp: DateTime\u003cUtc\u003e,\n    db_state: String,\n    file_count: usize,\n}\n\nimpl E2EFixture {\n    pub fn log_step(\u0026mut self, name: \u0026str) {\n        let step = TestStep {\n            number: self.steps.len() + 1,\n            name: name.to_string(),\n            timestamp: Utc::now(),\n        };\n        println!(\"\\n[STEP {}] {} @ {}\", step.number, step.name, step.timestamp);\n        self.steps.push(step);\n    }\n    \n    pub fn checkpoint(\u0026mut self, name: \u0026str) {\n        let checkpoint = Checkpoint {\n            name: name.to_string(),\n            timestamp: Utc::now(),\n            db_state: self.capture_db_state(),\n            file_count: self.count_files(),\n        };\n        println!(\"[CHECKPOINT] {} @ {}\", checkpoint.name, checkpoint.timestamp);\n        println!(\"[CHECKPOINT] DB: {}\", checkpoint.db_state);\n        println!(\"[CHECKPOINT] Files: {}\", checkpoint.file_count);\n        self.checkpoints.push(checkpoint);\n    }\n    \n    pub fn generate_report(\u0026self) {\n        // Generate JUnit XML\n        self.generate_junit_xml();\n        \n        // Generate HTML report\n        self.generate_html_report();\n        \n        // Print summary\n        self.print_summary();\n    }\n    \n    fn generate_junit_xml(\u0026self) {\n        let xml = format!(r#\"\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\n\u003ctestsuite name=\"{}\" tests=\"{}\" failures=\"0\" time=\"{}\"\u003e\n{}\n\u003c/testsuite\u003e\"#,\n            self.inner.test_name,\n            self.steps.len(),\n            self.total_time().as_secs_f64(),\n            self.steps.iter().map(|s| format!(\n                r#\"  \u003ctestcase name=\"{}\" time=\"0.0\"/\u003e\"#, s.name\n            )).collect::\u003cVec\u003c_\u003e\u003e().join(\"\\n\")\n        );\n        \n        let report_path = self.inner.temp_dir.path().join(\"junit-report.xml\");\n        std::fs::write(\u0026report_path, xml).expect(\"Failed to write JUnit report\");\n        println!(\"[REPORT] JUnit XML: {:?}\", report_path);\n    }\n    \n    fn generate_html_report(\u0026self) {\n        let html = format!(r#\"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003ctitle\u003eE2E Test Report: {}\u003c/title\u003e\n    \u003cstyle\u003e\n        body {{ font-family: sans-serif; margin: 20px; }}\n        .step {{ margin: 10px 0; padding: 10px; background: #f5f5f5; }}\n        .checkpoint {{ margin: 10px 0; padding: 10px; background: #e0ffe0; }}\n        .expandable {{ cursor: pointer; }}\n        .details {{ display: none; margin-left: 20px; }}\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003ch1\u003eE2E Test Report: {}\u003c/h1\u003e\n    \u003ch2\u003eSteps\u003c/h2\u003e\n    {}\n    \u003ch2\u003eCheckpoints\u003c/h2\u003e\n    {}\n    \u003ch2\u003eBenchmarks\u003c/h2\u003e\n    {}\n\u003c/body\u003e\n\u003c/html\u003e\"#,\n            self.inner.test_name,\n            self.inner.test_name,\n            self.render_steps_html(),\n            self.render_checkpoints_html(),\n            self.render_benchmarks_html()\n        );\n        \n        let report_path = self.inner.temp_dir.path().join(\"report.html\");\n        std::fs::write(\u0026report_path, html).expect(\"Failed to write HTML report\");\n        println!(\"[REPORT] HTML: {:?}\", report_path);\n    }\n}\n```\n\n### 3. Logging Requirements\n\nEvery E2E test must log:\n- **Timestamp**: For every step and checkpoint\n- **Command invocations**: Full command with all arguments\n- **stdout/stderr**: Captured separately\n- **Timing**: Duration for each step\n- **Database state**: At each checkpoint\n- **JUnit XML**: Generated for CI integration\n- **HTML report**: With expandable details for debugging\n\n### 4. CI Integration\n\nAdd to CI pipeline:\n```yaml\ne2e-tests:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v4\n    - name: Run E2E tests\n      run: cargo test --test e2e -- --test-threads=1\n    - name: Upload test reports\n      uses: actions/upload-artifact@v4\n      with:\n        name: e2e-reports\n        path: |\n          target/e2e-reports/*.xml\n          target/e2e-reports/*.html\n    - name: Publish test results\n      uses: dorny/test-reporter@v1\n      with:\n        name: E2E Test Results\n        path: target/e2e-reports/*.xml\n        reporter: java-junit\n```\n\n## Acceptance Criteria\n\n1. [ ] Fresh install workflow test passing\n2. [ ] Skill creation workflow test passing\n3. [ ] Bundle workflow test passing\n4. [ ] Multi-machine sync test passing\n5. [ ] Error recovery test passing\n6. [ ] Performance regression test passing\n7. [ ] JUnit XML reports generated\n8. [ ] HTML reports with expandable details\n9. [ ] All tests log timestamps for every step\n10. [ ] All tests capture stdout/stderr separately\n11. [ ] Database state logged at checkpoints\n12. [ ] Performance thresholds enforced\n\n## Dependencies\n\n- meta_skill-9pr (Integration Test Framework) - provides TestFixture base","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T22:56:33.829543302-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.859151863-05:00","labels":["e2e","scripts","testing"],"dependencies":[{"issue_id":"meta_skill-2kd","depends_on_id":"meta_skill-9pr","type":"blocks","created_at":"2026-01-13T22:56:38.556953575-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-2kd","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.178879716-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-327","title":"RU (Repo Updater) Integration for Skill Sync","description":"# RU (Repo Updater) Integration for Skill Sync\n\n## Overview\n\nIntegrate ru (repo_updater) from /data/projects/repo_updater as the repository synchronization layer for ms. ru provides battle-tested GitHub repo syncing with parallel operations, conflict detection, and automation-friendly JSON output.\n\n**Location**: `/data/projects/repo_updater`\n**Documentation**: `/data/projects/repo_updater/README.md`\n\n## Why ru (not custom implementation)\n\n| Aspect | Custom Implementation | ru Integration |\n|--------|----------------------|----------------|\n| **Maturity** | New, untested | Production-ready |\n| **Parallel sync** | Must build | Built-in with work-stealing |\n| **Conflict detection** | Manual | Automatic with resolution commands |\n| **Git plumbing** | String parsing | Reliable rev-list/porcelain |\n| **Exit codes** | Define own | Semantic (0-5) |\n| **Resume** | Build from scratch | --resume supported |\n\n## Use Cases for ms\n\n### 1. Skill Repository Sync\nSkills can be distributed as GitHub repositories:\n- User maintains skill repos in repos.d/skills.txt\n- `ms sync` calls ru to sync all skill repos\n- ru handles clone, pull, conflict detection\n- ms re-indexes after sync\n\n### 2. Skill Source Discovery\nru provides a list of all user's repositories:\n- `ru list --paths` outputs repo paths\n- ms can scan these for skills\n- Integration with ms index --discover\n\n### 3. Multi-Machine Skill Sync\nWhen skills are stored in Git repos:\n- Changes pushed to GitHub from machine A\n- ru sync on machine B pulls updates\n- ms automatically re-indexes changed skills\n\n### 4. Bundle Distribution via GitHub\nGitHub-hosted skill bundles:\n- `ms bundle publish` creates GitHub release\n- Other users add repo to ru config\n- `ru sync` + `ms bundle install` updates skills\n\n## Architecture\n\n```rust\n/// ru client for repository sync\nstruct RuClient {\n    /// Path to ru binary\n    ru_path: PathBuf,\n    /// Default flags for automation\n    default_flags: Vec\u003cString\u003e,\n}\n\nimpl RuClient {\n    /// Sync all configured repos\n    async fn sync(\u0026self, opts: SyncOptions) -\u003e Result\u003cSyncResult\u003e {\n        // Call: ru sync --non-interactive --json\n    }\n    \n    /// Get list of all repo paths\n    async fn list_paths(\u0026self) -\u003e Result\u003cVec\u003cPathBuf\u003e\u003e {\n        // Call: ru list --paths\n    }\n    \n    /// Check sync status without changes\n    async fn status(\u0026self) -\u003e Result\u003cRepoStatus\u003e {\n        // Call: ru status --no-fetch --json\n    }\n    \n    /// Sync specific repo\n    async fn sync_repo(\u0026self, repo: \u0026str) -\u003e Result\u003cSyncResult\u003e {\n        // Call: ru sync --filter \u003crepo\u003e --json\n    }\n}\n\nstruct SyncOptions {\n    parallel: Option\u003cu32\u003e,     // -j4\n    dry_run: bool,             // --dry-run\n    autostash: bool,           // --autostash\n}\n\n#[derive(Deserialize)]\nstruct SyncResult {\n    cloned: Vec\u003cString\u003e,\n    updated: Vec\u003cString\u003e,\n    current: Vec\u003cString\u003e,\n    conflicts: Vec\u003cConflictInfo\u003e,\n    exit_code: u8,\n}\n\n#[derive(Deserialize)]\nstruct ConflictInfo {\n    repo: String,\n    status: String,  // diverged, dirty, auth_failed\n    resolution: String,  // Copy-paste command\n}\n```\n\n## CLI Commands\n\n```bash\n# Sync skill repositories (wraps ru)\nms sync                     # Sync all skill repos\nms sync --parallel 4        # Parallel sync\nms sync --dry-run           # Preview changes\nms sync --status            # Status without sync\n\n# Discover skills in synced repos\nms index --discover         # Scan ru repos for skills\nms index --from-ru          # Index only ru-managed repos\n\n# Repo management integration\nms repo list               # List skill repos\nms repo add \u003crepo\u003e         # Add to ru config + skill sources\nms repo remove \u003crepo\u003e      # Remove from both\n\n# Status and health\nms sync status             # Sync status summary\nms sync health             # Repo health check\n```\n\n## Exit Code Mapping\n\nru exit codes (from ru docs):\n- 0 = All repos synced successfully\n- 1 = Partial success (some repos had issues)\n- 2 = Conflicts detected (need attention)\n- 3 = System error (git not found, etc.)\n- 4 = Bad arguments\n- 5 = Interrupted (can --resume)\n\nms maps these for user feedback:\n```rust\nfn handle_sync_result(exit_code: u8) -\u003e MsResult {\n    match exit_code {\n        0 =\u003e Ok(SyncSuccess),\n        1 =\u003e Ok(PartialSuccess { warning: \"Some repos skipped\" }),\n        2 =\u003e Err(ConflictsNeedAttention),\n        3 =\u003e Err(SystemError(\"Git/ru issue\")),\n        4 =\u003e Err(InvalidConfig),\n        5 =\u003e Ok(Interrupted { can_resume: true }),\n    }\n}\n```\n\n## Configuration\n\n```yaml\n# ~/.ms/config.yaml\nsync:\n  # Use ru for repo sync\n  backend: ru\n  \n  # ru-managed skill repos\n  skill_repos:\n    - \"Dicklesworthstone/claude-code-skills\"\n    - \"myorg/internal-skills@main\"\n  \n  # Auto-reindex after sync\n  auto_reindex: true\n  \n  # Parallel workers\n  parallel: 4\n  \n  # Autostash on conflict\n  autostash: true\n```\n\n## Tasks\n\n1. [ ] Detect ru installation and version\n2. [ ] Implement RuClient wrapper\n3. [ ] Parse ru JSON output format\n4. [ ] Build ms sync CLI commands\n5. [ ] Integrate with skill discovery (ms index --from-ru)\n6. [ ] Add repo management commands\n7. [ ] Handle exit codes with user feedback\n8. [ ] Auto-reindex after successful sync\n9. [ ] Document ru configuration for skills\n10. [ ] Handle ru unavailable gracefully\n\n## Testing Requirements\n\n- ru integration tests (sync, list, status)\n- Exit code handling correctness\n- JSON output parsing\n- Conflict scenario handling\n- Auto-reindex triggering\n- Graceful degradation without ru\n\n## Acceptance Criteria\n\n- ru detected and integrated\n- ms sync works with ru backend\n- Conflicts reported with resolutions\n- Auto-reindex after sync\n- Skill repos configurable\n- Works without ru (manual mode)\n\n## Dependencies\n\n- Phase 5 foundation (bundle distribution)\n- Multi-machine sync bead (meta_skill-ujr)\n- Skill discovery/indexing infrastructure\n\n## References\n\n- ru repository: /data/projects/repo_updater\n- ru README: /data/projects/repo_updater/README.md\n- AGENTS.md ru section\n- Plan Section 5.x (multi-machine sync)\n\nLabels: [phase-5 integration sync ru multi-machine]","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:18:04.189594404-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:18:04.189594404-05:00","labels":["integration","multi-machine","phase-5","ru","sync"],"dependencies":[{"issue_id":"meta_skill-327","depends_on_id":"meta_skill-ujr","type":"blocks","created_at":"2026-01-13T23:18:20.003887715-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-327","depends_on_id":"meta_skill-yu1","type":"blocks","created_at":"2026-01-13T23:18:21.125093961-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-330","title":"[P4] Interactive Build TUI","description":"# Interactive Build TUI\n\nGuided skill generation with Ratatui.\n\n## Tasks\n1. TUI layout with Ratatui\n2. Pattern selection screen\n3. Draft preview pane\n4. Refinement iteration\n5. Quality feedback\n6. Session state persistence\n\n## TUI Screens (from Section 21)\n1. **Session Selection**: Choose source sessions\n2. **Pattern Review**: Toggle patterns on/off\n3. **Draft Preview**: Live skill preview\n4. **Refinement**: Edit and iterate\n5. **Finalization**: Name, tags, save\n\n## Layout\n```\n\n MS Build - \"git commit patterns\"                \n\n Patterns (3 selected)     Draft Preview       \n [x] Conventional commits  # Git Commit Guide  \n [x] Scope naming                              \n [ ] Body formatting       ## Rules            \n [x] Breaking changes      1. Use conventional \n                           2. Include scope    \n\n [Enter] Accept  [Space] Toggle  [q] Quit       \n\n```\n\n## Keyboard Navigation\n- Arrow keys: Navigate\n- Space: Toggle selection\n- Enter: Accept/advance\n- Tab: Switch panes\n- q: Quit/cancel\n\n## Acceptance Criteria\n- TUI is responsive\n- Live preview updates\n- State persists on interrupt","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:50.439572634-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:50.439572634-05:00","labels":["build","phase-4","tui"],"dependencies":[{"issue_id":"meta_skill-330","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:13.126510003-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-330","depends_on_id":"meta_skill-9r9","type":"blocks","created_at":"2026-01-13T22:26:13.151767172-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-36x","title":"CASS Mining: Debugging Workflows","description":"Deep dive into debugging patterns across projects, systematic bug hunting, root cause analysis methodologies.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:38.882245568-05:00","created_by":"ubuntu","updated_at":"2026-01-13T21:12:20.589569786-05:00","closed_at":"2026-01-13T21:12:20.589569786-05:00","close_reason":"Added Section 37: Debugging Workflows and Methodologies (~615 lines). CASS mined brenner_bot, cass, caam, mcp_agent_mail, fix_my_documents_backend, and agentic_coding_flywheel_setup for debugging patterns covering: systematic debugging philosophy, race condition hunting, error handling detection, performance profiling, N+1 query patterns, test failure analysis, investigation report formats, structured logging, concurrency debugging, timeout handling, and comprehensive checklists by bug type.","labels":["cass-mining"]}
{"id":"meta_skill-4d7","title":"CASS Mining: Inner Truth/Abstract Principles (brenner_bot)","description":"Deep dive into brenner_bot CASS sessions for inner truth extraction, abstract principles, multi-model synthesis triangulation, metaprompt refinement patterns. This is a gold mine of skill methodology.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-13T17:47:17.426402691-05:00","created_by":"ubuntu","updated_at":"2026-01-13T17:51:40.014892136-05:00","closed_at":"2026-01-13T17:51:40.014892136-05:00","close_reason":"Section 28 added to plan with Brenner methodology for skill extraction","labels":["cass-mining"]}
{"id":"meta_skill-4g1","title":"Uncertainty Queue (Active Learning)","description":"## Section Reference\nSection 5.15 - Uncertainty Queue and Active Learning\n\n## Overview\nWhen generalization confidence is too low, queue candidates for targeted evidence gathering. Generate 3-7 targeted CASS queries per uncertainty (positive, negative, boundary cases).\n\n## Core Concept\nActive learning loop: when the system cannot confidently generalize a pattern, it queues the uncertainty and generates targeted queries to gather more evidence. This closes the feedback loop between pattern mining and evidence collection.\n\n## Data Structures\n\n```rust\n/// An item in the uncertainty queue awaiting resolution\nstruct UncertaintyItem {\n    /// Unique identifier for this uncertainty\n    id: UncertaintyId,\n    /// The pattern candidate that triggered uncertainty\n    pattern_candidate: ExtractedPattern,\n    /// Why confidence is too low\n    reason: MissingSignal,\n    /// Current confidence score (0.0-1.0)\n    confidence: f32,\n    /// Minimum confidence threshold for acceptance\n    threshold: f32,\n    /// Generated queries to gather evidence\n    suggested_queries: Vec\u003cSuggestedQuery\u003e,\n    /// Current resolution status\n    status: UncertaintyStatus,\n    /// When this item was created\n    created_at: DateTime\u003cUtc\u003e,\n    /// When this was last updated\n    updated_at: DateTime\u003cUtc\u003e,\n    /// Resolution attempts history\n    attempts: Vec\u003cResolutionAttempt\u003e,\n}\n\n/// Why confidence is insufficient\nenum MissingSignal {\n    /// Not enough examples to generalize\n    InsufficientInstances { \n        have: u32, \n        need: u32,\n        variance: f32,\n    },\n    /// Examples show high variation\n    HighVariance {\n        variance_score: f32,\n        conflicting_aspects: Vec\u003cString\u003e,\n    },\n    /// Found examples that contradict the pattern\n    CounterExampleFound {\n        counter_example: SessionId,\n        contradiction: String,\n    },\n    /// Scope/applicability unclear\n    AmbiguousScope {\n        possible_scopes: Vec\u003cScopeCandidate\u003e,\n    },\n    /// Preconditions unclear\n    UnclearPreconditions {\n        candidates: Vec\u003cPredicate\u003e,\n    },\n    /// Effect boundaries unknown\n    UnknownBoundaries {\n        dimension: String,\n        observed_range: (f32, f32),\n    },\n}\n\n/// Status of uncertainty resolution\nenum UncertaintyStatus {\n    /// Waiting in queue\n    Pending,\n    /// Currently gathering evidence\n    InProgress { \n        started_at: DateTime\u003cUtc\u003e,\n        queries_completed: u32,\n    },\n    /// Resolved - pattern accepted\n    Resolved { \n        new_confidence: f32,\n        resolution: Resolution,\n    },\n    /// Resolved - pattern rejected\n    Rejected { \n        reason: String,\n    },\n    /// Stalled - needs human input\n    NeedsHuman { \n        reason: String,\n    },\n    /// Expired - aged out\n    Expired,\n}\n\nenum Resolution {\n    /// Gathered enough evidence to accept\n    EvidenceGathered { new_sessions: Vec\u003cSessionId\u003e },\n    /// Refined pattern to be more specific\n    PatternRefined { new_pattern: ExtractedPattern },\n    /// Split into multiple patterns\n    PatternSplit { patterns: Vec\u003cExtractedPattern\u003e },\n    /// Human provided clarification\n    HumanClarified { annotation: String },\n}\n\n/// A suggested query to gather evidence\nstruct SuggestedQuery {\n    /// Query type\n    query_type: QueryType,\n    /// Natural language query\n    query: String,\n    /// CASS-formatted query if applicable\n    cass_query: Option\u003cString\u003e,\n    /// What evidence this would provide\n    expected_evidence: String,\n    /// Priority (higher = more valuable)\n    priority: u32,\n    /// Whether this query has been executed\n    executed: bool,\n    /// Results if executed\n    results: Option\u003cQueryResults\u003e,\n}\n\nenum QueryType {\n    /// Looking for positive examples\n    Positive,\n    /// Looking for negative examples / counter-examples\n    Negative,\n    /// Looking for boundary cases\n    Boundary,\n    /// Looking for scope clarification\n    ScopeClarification,\n    /// Looking for precondition evidence\n    PreconditionEvidence,\n}\n\n/// The uncertainty queue\nstruct UncertaintyQueue {\n    /// All items in the queue\n    items: Vec\u003cUncertaintyItem\u003e,\n    /// Configuration\n    config: UncertaintyConfig,\n    /// Statistics\n    stats: QueueStats,\n}\n\nstruct UncertaintyConfig {\n    /// Minimum confidence to skip queue\n    min_confidence: f32,\n    /// Maximum items to hold before forcing resolution\n    max_queue_size: usize,\n    /// Number of queries to generate per uncertainty\n    queries_per_uncertainty: RangeInclusive\u003cu32\u003e, // 3..=7\n    /// How long before expiry\n    expiry_duration: Duration,\n    /// Auto-resolve if possible\n    auto_resolve: bool,\n}\n\nstruct QueueStats {\n    total_queued: u64,\n    total_resolved: u64,\n    total_rejected: u64,\n    total_expired: u64,\n    average_resolution_time: Duration,\n    average_queries_needed: f32,\n}\n```\n\n## Query Generation\n\n```rust\ntrait QueryGenerator {\n    /// Generate targeted queries for an uncertainty\n    fn generate_queries(\n        \u0026self,\n        uncertainty: \u0026UncertaintyItem,\n        existing_evidence: \u0026[Session],\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n    \n    /// Generate positive example queries\n    fn generate_positive_queries(\n        \u0026self,\n        pattern: \u0026ExtractedPattern,\n        count: u32,\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n    \n    /// Generate negative/counter-example queries  \n    fn generate_negative_queries(\n        \u0026self,\n        pattern: \u0026ExtractedPattern,\n        count: u32,\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n    \n    /// Generate boundary case queries\n    fn generate_boundary_queries(\n        \u0026self,\n        pattern: \u0026ExtractedPattern,\n        count: u32,\n    ) -\u003e Vec\u003cSuggestedQuery\u003e;\n}\n\n/// Example query generation for a file-handling pattern\nfn example_query_generation(pattern: \u0026ExtractedPattern) -\u003e Vec\u003cSuggestedQuery\u003e {\n    vec![\n        SuggestedQuery {\n            query_type: QueryType::Positive,\n            query: \"Show sessions where I successfully handled large files with streaming\".into(),\n            cass_query: Some(\"topic:file-handling AND outcome:success AND size:large\".into()),\n            expected_evidence: \"More positive examples of the pattern\".into(),\n            priority: 3,\n            executed: false,\n            results: None,\n        },\n        SuggestedQuery {\n            query_type: QueryType::Negative,\n            query: \"Show sessions where file handling failed or I had to retry\".into(),\n            cass_query: Some(\"topic:file-handling AND (outcome:failure OR action:retry)\".into()),\n            expected_evidence: \"Counter-examples or boundary failures\".into(),\n            priority: 2,\n            executed: false,\n            results: None,\n        },\n        SuggestedQuery {\n            query_type: QueryType::Boundary,\n            query: \"Show sessions with medium-sized files around 1GB\".into(),\n            cass_query: Some(\"topic:file-handling AND size:1gb..2gb\".into()),\n            expected_evidence: \"Evidence for where pattern boundaries lie\".into(),\n            priority: 1,\n            executed: false,\n            results: None,\n        },\n    ]\n}\n```\n\n## Resolution Engine\n\n```rust\ntrait UncertaintyResolver {\n    /// Attempt to resolve an uncertainty with new evidence\n    fn attempt_resolution(\n        \u0026self,\n        uncertainty: \u0026mut UncertaintyItem,\n        new_evidence: \u0026[Session],\n    ) -\u003e ResolutionResult;\n    \n    /// Check if uncertainty can be auto-resolved\n    fn can_auto_resolve(\u0026self, uncertainty: \u0026UncertaintyItem) -\u003e bool;\n    \n    /// Escalate to human if needed\n    fn escalate_to_human(\u0026self, uncertainty: \u0026mut UncertaintyItem, reason: \u0026str);\n}\n\nenum ResolutionResult {\n    /// Resolved successfully\n    Resolved(Resolution),\n    /// Need more evidence\n    NeedsMoreEvidence { remaining_queries: Vec\u003cSuggestedQuery\u003e },\n    /// Pattern should be rejected\n    Reject { reason: String },\n    /// Needs human intervention\n    Escalate { reason: String },\n}\n\nfn attempt_resolution(\n    uncertainty: \u0026mut UncertaintyItem,\n    new_sessions: \u0026[Session],\n) -\u003e ResolutionResult {\n    // Re-extract pattern with new evidence\n    let all_sessions = collect_all_sessions(uncertainty, new_sessions);\n    let refined_pattern = extract_pattern(\u0026all_sessions);\n    \n    // Calculate new confidence\n    let new_confidence = calculate_confidence(\u0026refined_pattern, \u0026all_sessions);\n    \n    if new_confidence \u003e= uncertainty.threshold {\n        return ResolutionResult::Resolved(Resolution::EvidenceGathered {\n            new_sessions: new_sessions.iter().map(|s| s.id).collect(),\n        });\n    }\n    \n    // Check for counter-examples\n    if let Some(counter) = find_counter_examples(\u0026refined_pattern, new_sessions) {\n        if counter.is_fundamental {\n            return ResolutionResult::Reject {\n                reason: format!(\"Counter-example invalidates pattern: {}\", counter.description),\n            };\n        }\n        // Maybe pattern needs refinement\n        return ResolutionResult::NeedsMoreEvidence {\n            remaining_queries: generate_refinement_queries(\u0026counter),\n        };\n    }\n    \n    // Check if more queries available\n    let remaining: Vec\u003c_\u003e = uncertainty.suggested_queries\n        .iter()\n        .filter(|q| !q.executed)\n        .cloned()\n        .collect();\n    \n    if remaining.is_empty() {\n        ResolutionResult::Escalate {\n            reason: \"All queries exhausted, still below threshold\".into(),\n        }\n    } else {\n        ResolutionResult::NeedsMoreEvidence { remaining_queries: remaining }\n    }\n}\n```\n\n## CLI Commands\n\n```bash\n# List uncertainties in queue\nms uncertainties list\nms uncertainties list --status pending\nms uncertainties list --reason insufficient-instances\n\n# Show details of specific uncertainty\nms uncertainties show \u003cuncertainty-id\u003e\n\n# Mine sessions to resolve uncertainties\nms uncertainties --mine\nms uncertainties --mine --limit 5\n\n# Execute suggested queries\nms uncertainties query \u003cuncertainty-id\u003e\nms uncertainties query \u003cuncertainty-id\u003e --query-index 0\n\n# Manually resolve uncertainty\nms uncertainties resolve \u003cuncertainty-id\u003e --accept\nms uncertainties resolve \u003cuncertainty-id\u003e --reject --reason \"Pattern too specific\"\n\n# Build with auto-resolution\nms build --auto-resolve-uncertainties\n\n# Queue statistics\nms uncertainties stats\n\n# Expire old uncertainties\nms uncertainties prune --older-than 30d\n```\n\n## Output Format\n\n```\n$ ms uncertainties list\n\nUNCERTAINTY QUEUE (3 items)\n================================================================================\n\n[U-7f3a] Pattern: error-handling-with-retry\n  Reason: InsufficientInstances (have: 2, need: 5)\n  Confidence: 0.42 (threshold: 0.70)\n  Queries: 5 suggested, 1 executed\n  Status: Pending\n  Age: 2 days\n\n[U-8b2c] Pattern: git-branch-naming\n  Reason: HighVariance (variance: 0.68)\n  Confidence: 0.55 (threshold: 0.70)  \n  Queries: 4 suggested, 3 executed\n  Status: InProgress\n  Age: 5 days\n\n[U-9d1e] Pattern: test-file-organization\n  Reason: AmbiguousScope (2 possible scopes)\n  Confidence: 0.38 (threshold: 0.70)\n  Queries: 6 suggested, 0 executed\n  Status: NeedsHuman\n  Age: 12 days\n```\n\n## Integration Points\n\n- **Specific-to-General Transformation** (meta_skill-9r9): Uncertainties arise during generalization\n- **Pattern Extraction Pipeline**: Queue candidates from extraction\n- **CASS Query Interface**: Execute generated queries\n- **Anti-Pattern Mining**: Counter-examples feed anti-pattern detection\n- **Build Pipeline**: --auto-resolve-uncertainties flag integration\n\n## Queue Management\n\n```rust\nimpl UncertaintyQueue {\n    /// Add new uncertainty to queue\n    fn enqueue(\u0026mut self, item: UncertaintyItem) -\u003e Result\u003cUncertaintyId, QueueError\u003e;\n    \n    /// Get next item to process\n    fn next(\u0026mut self) -\u003e Option\u003c\u0026mut UncertaintyItem\u003e;\n    \n    /// Process queue with available evidence\n    fn process(\u0026mut self, evidence_store: \u0026EvidenceStore) -\u003e ProcessResult;\n    \n    /// Prune expired items\n    fn prune_expired(\u0026mut self) -\u003e Vec\u003cUncertaintyItem\u003e;\n    \n    /// Get queue statistics\n    fn stats(\u0026self) -\u003e QueueStats;\n}\n```\n\n## Testing Requirements\n\n- Unit tests for each MissingSignal type\n- Query generation tests (verify 3-7 queries generated)\n- Resolution flow tests\n- Queue management tests (enqueue, dequeue, prune)\n- Integration test: full uncertainty lifecycle\n- Test auto-resolution with mock evidence\n- Test escalation to human","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:53:29.725303472-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:53:29.725303472-05:00","labels":["active-learning","phase-4","uncertainty"],"dependencies":[{"issue_id":"meta_skill-4g1","depends_on_id":"meta_skill-9r9","type":"blocks","created_at":"2026-01-13T22:57:36.081054335-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-4ih","title":"Phase 2: Search (Hybrid Search Engine)","description":"# Phase 2: Search\n\nImplement the hybrid search system combining BM25 + hash embeddings + RRF fusion.\n\n## Core Components\n1. **Tantivy integration** - BM25 full-text search on skill content\n2. **Hash embeddings** - FNV-1a based, 384 dimensions, no ML dependency\n3. **RRF fusion** - Reciprocal Rank Fusion to combine rankings\n4. **Alias resolution** - Handle skill renames and deprecations\n5. **Filter support** - By tech stack, tags, layer, type\n6. **Search CLI** - `ms search` command with robot mode\n\n## Key Design Decisions\n- Hash embeddings from xf: 80-90% ML quality, zero dependencies\n- Optional local ML embedder for higher fidelity (pluggable)\n- Alias system preserves backward compatibility\n\n## Success Criteria\n- `ms search \"query\"` returns ranked results\n- `ms search --robot \"query\"` returns JSON\n- Filters work correctly\n- Alias resolution works transparently\n- Sub-50ms p99 search latency","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:53.002811842-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:53.002811842-05:00","dependencies":[{"issue_id":"meta_skill-4ih","depends_on_id":"meta_skill-6hm","type":"blocks","created_at":"2026-01-13T22:21:01.826297143-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-4ki","title":"Phase 4: CASS Integration (Skill Mining)","description":"# Phase 4: CASS Integration\n\nThe killer feature: generating skills from CASS session history.\n\n## Core Components\n1. **CASS client** - Subprocess integration with robot mode\n2. **Pattern extraction** - Mine commands, solutions, pitfalls from sessions\n3. **Pattern IR** - Typed intermediate representation\n4. **Specific-to-general transformation** - Extract universal patterns\n5. **Session marking** - Mark sessions as exemplary/anti-pattern\n6. **Provenance graph** - Link rules to evidence\n7. **Redaction pipeline** - Strip secrets/PII before extraction\n8. **Taint tracking** - Ensure unsafe content never leaks\n9. **Interactive build TUI** - Guided skill generation\n10. **Autonomous generation** - Hours-long checkpointed builds\n11. **Session quality scoring** - Filter low-quality sessions\n12. **Anti-pattern mining** - Extract what NOT to do\n13. **Uncertainty queue** - Active learning for low-confidence patterns\n\n## Key Design Decisions\n- Brenner Method: extract generative grammar, not summaries\n- Counterexamples are first-class patterns\n- Steady-state detection for convergence\n- Provenance is compressed (pointer + fetch on demand)\n\n## Success Criteria\n- `ms build --from-cass \"topic\"` extracts patterns\n- `ms build --guided` runs interactive TUI\n- `ms build --autonomous --duration 4h` runs with checkpoints\n- Generated skills include provenance links\n- Redaction prevents secrets in output","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:54.26921675-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:54.26921675-05:00","dependencies":[{"issue_id":"meta_skill-4ki","depends_on_id":"meta_skill-y73","type":"blocks","created_at":"2026-01-13T22:21:01.877739821-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-4tx","title":"Phase 6: Polish \u0026 Auto-Update","description":"# Phase 6: Polish \u0026 Auto-Update\n\nFinal polish, TUI, MCP server, doctor command, and auto-update.\n\n## Core Components\n1. **Doctor command** - Comprehensive health checks with --fix\n2. **Shell integration** - Completions, aliases, environment setup\n3. **MCP server mode** - Native agent tool-use integration\n4. **Auto-update** - Self-updating binary via GitHub Releases\n5. **Interactive build TUI** - Ratatui-based interface\n6. **Skill effectiveness tracking** - A/B experiments, feedback loop\n7. **Quality scoring updates** - Learn from usage\n8. **Skill versioning** - Semantic versions with migrations\n9. **Error recovery** - Retry, rate limit handling, graceful degradation\n10. **Skill templates** - Pre-built patterns for rapid creation\n\n## Key Design Decisions\n- MCP eliminates subprocess overhead for agents\n- Doctor checks are categorized and auto-fixable\n- Update checks happen in background, apply on restart\n- Experiments can target individual slices\n\n## Success Criteria\n- `ms doctor` identifies and fixes issues\n- `ms mcp serve` provides tool definitions\n- `ms update` self-updates the binary\n- TUI is responsive and intuitive\n- Effectiveness data improves skill quality over time","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-13T22:20:56.361593099-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:56.361593099-05:00","dependencies":[{"issue_id":"meta_skill-4tx","depends_on_id":"meta_skill-yu1","type":"blocks","created_at":"2026-01-13T22:21:01.929265045-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-5e6","title":"[P2] Search Filters","description":"# Search Filters\n\nFilter search results by tech stack, layer, tags, type.\n\n## Tasks\n1. Parse filter syntax: --layer, --tag, --type, --tech\n2. Apply filters in Tantivy query\n3. Apply filters post-search for embeddings\n4. Support multiple values (--tag a --tag b)\n5. Support exclusion (--exclude-tag wip)\n\n## Filter Options\n- --layer: prompt, procedure, pipeline, policy, practice\n- --tag: arbitrary tags\n- --type: skill, workflow, checklist, debugging\n- --tech: language/framework filters\n- --status: active, deprecated, draft\n\n## Query Construction\nFilters are AND-ed together:\n```\nquery AND layer:policy AND tag:security\n```\n\n## Acceptance Criteria\n- All filters work correctly\n- Filters combine with full-text query\n- Exclusion filters work","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:23:05.345921069-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:23:05.345921069-05:00","labels":["filters","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-5e6","depends_on_id":"meta_skill-mh8","type":"blocks","created_at":"2026-01-13T22:23:13.595096509-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-5s0","title":"[P1] Rust Project Scaffolding","description":"## Overview\n\nInitialize the meta_skill (`ms`) CLI project following the xf architecture exactly. This is the root foundation upon which all other components are built. The project scaffolding establishes the module structure, dependencies, error handling patterns, and build configurations that will be used throughout the codebase.\n\n## Background \u0026 Rationale\n\n### Why Follow xf Exactly\n\nThe xf (X Archive Search) codebase represents a battle-tested Rust CLI architecture with:\n- ~23,000 lines of production Rust code\n- Same author as ms, ensuring familiar patterns\n- Same constraints (local-first CLI with search + indexing)\n- Proven patterns for SQLite, Tantivy, hash embeddings, robot mode\n\nBy following xf exactly, we:\n1. **Reduce decision fatigue**: Architecture decisions already made and validated\n2. **Enable code reuse**: Many modules can be adapted directly\n3. **Ensure consistency**: Same patterns across the agent flywheel ecosystem\n4. **Accelerate development**: No need to reinvent solutions\n\n### Module Structure Philosophy\n\nThe module structure separates concerns cleanly:\n- **cli/**: Command-line interface definitions (clap derive)\n- **db/**: SQLite persistence layer (rusqlite, migrations)\n- **git/**: Git archive layer (human-readable persistence)\n- **skill/**: Core skill data models and compilation\n- **search/**: Tantivy BM25 + hash embeddings + RRF fusion\n- **build/**: CASS integration for skill mining\n- **bundle/**: Skill packaging for sharing\n\n## Key Data Structures (from Plan Section 2.3)\n\n```rust\n// src/lib.rs - Central re-exports\npub mod cli;\npub mod db;\npub mod git;\npub mod skill;\npub mod search;\npub mod build;\npub mod bundle;\n\npub use cli::Cli;\npub use db::Database;\npub use git::GitArchive;\npub use skill::{Skill, SkillSpec};\npub use search::{SearchEngine, SearchResult};\n\n// src/error.rs - Unified error types\nuse thiserror::Error;\n\n#[derive(Error, Debug)]\npub enum MsError {\n    #[error(\"Database error: {0}\")]\n    Database(#[from] rusqlite::Error),\n    \n    #[error(\"Git error: {0}\")]\n    Git(#[from] git2::Error),\n    \n    #[error(\"Tantivy error: {0}\")]\n    Search(#[from] tantivy::TantivyError),\n    \n    #[error(\"IO error: {0}\")]\n    Io(#[from] std::io::Error),\n    \n    #[error(\"Serialization error: {0}\")]\n    Serde(#[from] serde_json::Error),\n    \n    #[error(\"CASS client error: {0}\")]\n    Cass(String),\n    \n    #[error(\"Skill not found: {0}\")]\n    SkillNotFound(String),\n    \n    #[error(\"Validation error: {0}\")]\n    Validation(String),\n    \n    #[error(\"Configuration error: {0}\")]\n    Config(String),\n    \n    #[error(\"Lock error: {0}\")]\n    Lock(String),\n}\n\npub type Result\u003cT\u003e = std::result::Result\u003cT, MsError\u003e;\n\n// src/main.rs - Entry point\nuse clap::Parser;\nuse tracing_subscriber::EnvFilter;\n\n#[tokio::main]\nasync fn main() -\u003e anyhow::Result\u003c()\u003e {\n    // Initialize tracing\n    tracing_subscriber::fmt()\n        .with_env_filter(\n            EnvFilter::try_from_default_env()\n                .unwrap_or_else(|_| EnvFilter::new(\"ms=info\"))\n        )\n        .with_target(false)\n        .init();\n    \n    let cli = ms::cli::Cli::parse();\n    cli.run().await\n}\n```\n\n## Tasks\n\n### Task 1: Create Cargo.toml with Complete Dependencies\n- [ ] Initialize new Rust project: `cargo new ms --edition 2024`\n- [ ] Add clap with derive feature for CLI parsing\n- [ ] Add tokio with full runtime for async operations\n- [ ] Add rusqlite with bundled-sqlcipher feature for SQLite\n- [ ] Add tantivy for full-text search\n- [ ] Add serde and serde_json for serialization\n- [ ] Add thiserror and anyhow for error handling\n- [ ] Add tracing and tracing-subscriber for logging\n- [ ] Add directories crate for XDG path handling\n- [ ] Add git2 for Git operations\n- [ ] Add sha2 and hex for hashing\n- [ ] Configure Cargo profiles (dev, release, profiling)\n\n### Task 2: Set Up Module Structure\n- [ ] Create src/lib.rs with module declarations\n- [ ] Create src/cli/mod.rs for CLI command structure\n- [ ] Create src/db/mod.rs for database module\n- [ ] Create src/git/mod.rs for Git archive module\n- [ ] Create src/skill/mod.rs for skill data models\n- [ ] Create src/search/mod.rs for search engine\n- [ ] Create src/build/mod.rs for CASS integration\n- [ ] Create src/bundle/mod.rs for skill bundling\n- [ ] Create src/error.rs with unified error types\n\n### Task 3: Configure main.rs Entry Point\n- [ ] Set up tokio async runtime\n- [ ] Initialize tracing subscriber with env filter\n- [ ] Parse CLI arguments with clap\n- [ ] Route to appropriate command handlers\n- [ ] Handle graceful shutdown on Ctrl+C\n\n### Task 4: Configure Error Handling\n- [ ] Define MsError enum with all variant types\n- [ ] Implement From conversions for underlying errors\n- [ ] Create Result\u003cT\u003e type alias\n- [ ] Add context helpers with anyhow\n- [ ] Ensure errors are human-readable and actionable\n\n### Task 5: Configure Cargo Profiles\n- [ ] Set dev profile: debug symbols, fast compile\n- [ ] Set release profile: LTO thin, codegen-units 1, strip\n- [ ] Set profiling profile: release + debug symbols\n- [ ] Configure incremental compilation settings\n\n### Task 6: Add CI/CD Configuration Files\n- [ ] Create .github/workflows/ci.yml for GitHub Actions\n- [ ] Add rustfmt.toml with project formatting rules\n- [ ] Add clippy.toml with lint configuration\n- [ ] Create .cargo/config.toml for build settings\n\n## Cargo.toml Reference\n\n```toml\n[package]\nname = \"ms\"\nversion = \"0.1.0\"\nedition = \"2024\"\nauthors = [\"Your Name \u003cyour.email@example.com\u003e\"]\ndescription = \"Meta Skill - Mine CASS sessions to generate production-quality Claude Code skills\"\nlicense = \"MIT\"\nrepository = \"https://github.com/youruser/ms\"\nkeywords = [\"cli\", \"ai\", \"skills\", \"claude\", \"agents\"]\ncategories = [\"command-line-utilities\", \"development-tools\"]\n\n[[bin]]\nname = \"ms\"\npath = \"src/main.rs\"\n\n[lib]\nname = \"ms\"\npath = \"src/lib.rs\"\n\n[dependencies]\n# CLI framework\nclap = { version = \"4.5\", features = [\"derive\", \"env\", \"wrap_help\"] }\n\n# Async runtime\ntokio = { version = \"1.35\", features = [\"full\"] }\n\n# Database\nrusqlite = { version = \"0.32\", features = [\"bundled-sqlcipher\", \"backup\", \"functions\"] }\n\n# Search\ntantivy = \"0.22\"\n\n# Serialization\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\nserde_yaml = \"0.9\"\ntoml = \"0.8\"\n\n# Error handling\nthiserror = \"2.0\"\nanyhow = \"1.0\"\n\n# Logging\ntracing = \"0.1\"\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\", \"json\"] }\n\n# File system\ndirectories = \"5.0\"\nwalkdir = \"2.4\"\nnotify = \"7.0\"\n\n# Git operations\ngit2 = \"0.19\"\n\n# Hashing\nsha2 = \"0.10\"\nhex = \"0.4\"\n\n# Time\nchrono = { version = \"0.4\", features = [\"serde\"] }\n\n# Misc\nuuid = { version = \"1.6\", features = [\"v4\", \"serde\"] }\nregex = \"1.10\"\nlazy_static = \"1.4\"\nonce_cell = \"1.19\"\nsemver = { version = \"1.0\", features = [\"serde\"] }\nurl = { version = \"2.5\", features = [\"serde\"] }\n\n[dev-dependencies]\ntempfile = \"3.10\"\nassert_cmd = \"2.0\"\npredicates = \"3.1\"\ninsta = { version = \"1.34\", features = [\"yaml\"] }\nproptest = \"1.4\"\ncriterion = { version = \"0.5\", features = [\"html_reports\"] }\ntest-case = \"3.3\"\n\n[[bench]]\nname = \"search_benchmark\"\nharness = false\n\n[profile.dev]\nopt-level = 0\ndebug = true\nincremental = true\n\n[profile.release]\nopt-level = 3\nlto = \"thin\"\ncodegen-units = 1\nstrip = true\npanic = \"abort\"\n\n[profile.profiling]\ninherits = \"release\"\ndebug = true\nstrip = false\n```\n\n## Acceptance Criteria\n\n1. **Build Success**: `cargo build` completes without errors\n2. **Lint Clean**: `cargo clippy -- -D warnings` passes\n3. **Format Check**: `cargo fmt --check` passes\n4. **Module Structure**: All module files exist and compile\n5. **Binary Output**: Binary named `ms` is produced\n6. **Help Output**: `ms --help` shows usage information\n7. **Error Types**: MsError covers all anticipated failure modes\n8. **Logging**: Tracing output visible with RUST_LOG=ms=debug\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_error_display() {\n        let err = MsError::SkillNotFound(\"test-skill\".into());\n        assert_eq!(err.to_string(), \"Skill not found: test-skill\");\n    }\n    \n    #[test]\n    fn test_error_from_io() {\n        let io_err = std::io::Error::new(\n            std::io::ErrorKind::NotFound, \n            \"file not found\"\n        );\n        let ms_err: MsError = io_err.into();\n        assert!(matches!(ms_err, MsError::Io(_)));\n    }\n    \n    #[test]\n    fn test_error_source_chain() {\n        use std::error::Error;\n        \n        let io_err = std::io::Error::new(\n            std::io::ErrorKind::PermissionDenied, \n            \"access denied\"\n        );\n        let ms_err: MsError = io_err.into();\n        assert!(ms_err.source().is_some());\n    }\n}\n```\n\n### Integration Test (tests/cli.rs)\n```rust\nuse assert_cmd::Command;\nuse predicates::prelude::*;\n\n#[test]\nfn test_help_output() {\n    let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n    cmd.arg(\"--help\")\n        .assert()\n        .success()\n        .stdout(predicate::str::contains(\"Meta Skill\"))\n        .stdout(predicate::str::contains(\"USAGE\"));\n}\n\n#[test]\nfn test_version_output() {\n    let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n    cmd.arg(\"--version\")\n        .assert()\n        .success()\n        .stdout(predicate::str::contains(\"ms\"));\n}\n\n#[test]\nfn test_unknown_command() {\n    let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n    cmd.arg(\"nonexistent\")\n        .assert()\n        .failure()\n        .stderr(predicate::str::contains(\"error\"));\n}\n```\n\n### Logging Requirements\nAll operations must log with appropriate levels:\n- `TRACE`: Low-level operation details\n- `DEBUG`: Module initialization, configuration loading\n- `INFO`: Application startup, major operations\n- `WARN`: Recoverable issues, deprecated features\n- `ERROR`: Failures that prevent operation completion\n\nExample log output:\n```\n[INFO] ms v0.1.0 starting\n[DEBUG] Loading configuration from /home/user/.config/ms/config.yaml\n[DEBUG] Initializing database at /home/user/.local/share/ms/ms.db\n[INFO] Database initialized (0.023s)\n[DEBUG] Loading skill registry...\n[INFO] Loaded 42 skills from 3 layers\n```\n\n## References\n\n- Plan Section 2.3: File Layout (Following xf Pattern)\n- Plan Section 0.4: What Is xf (X Archive Search)?\n- xf Codebase: /data/projects/xf\n- Blocks: All Phase 1 beads depend on this scaffolding\n\nLabels: [phase-1 rust setup]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:21:58.323525006-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:36:32.169657975-05:00","labels":["phase-1","rust","setup"]}
{"id":"meta_skill-628","title":"[Cross-Cutting] CI/CD Pipeline","description":"# CI/CD Pipeline\n\nContinuous integration and deployment (Section 35).\n\n## GitHub Actions Workflows\n1. **ci.yml**: lint, test, build on push/PR\n2. **release.yml**: Tag-triggered releases\n3. **e2e.yml**: Full E2E test suite\n\n## CI Jobs (from Section 35.1)\n- lint: cargo clippy, cargo fmt --check\n- test: cargo test --all-features\n- build: cargo build --release\n- e2e: Full workflow tests\n\n## Release Workflow\n1. Tag triggers release\n2. Build binaries for all platforms\n3. Generate checksums\n4. Create GitHub Release\n5. Upload assets\n\n## Matrix Testing\n- OS: ubuntu-latest, macos-latest\n- Rust: stable, beta\n\n## Quality Gates\n- All tests pass\n- No clippy warnings\n- Format check passes\n- Coverage threshold met\n\n## Acceptance Criteria\n- CI runs on every push\n- Releases automated\n- Multi-platform builds work","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:29:08.211299891-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:29:08.211299891-05:00","labels":["automation","ci-cd","cross-cutting"]}
{"id":"meta_skill-67m","title":"[P6] Shell Integration","description":"# Shell Integration\n\nCompletions, aliases, and environment setup.\n\n## Tasks\n1. Generate shell completions (bash, zsh, fish)\n2. Provide shell init snippets\n3. Environment variable documentation\n4. Alias suggestions\n\n## Completions\n- Clap derive generates completions\n- `ms completions bash \u003e ~/.local/share/bash-completion/completions/ms`\n- Complete command names, skill IDs, flags\n\n## Shell Init\n```bash\n# Add to .bashrc / .zshrc\neval \"$(ms shell-init)\"\n```\n\nProvides:\n- PATH setup (if not already)\n- Completions loading\n- Optional aliases\n\n## Environment Variables\n- MS_HOME: Override config directory\n- MS_ROBOT: Default to robot mode\n- MS_LOG: Log level\n- MS_CASS_PATH: Path to CASS binary\n\n## Aliases (Optional)\n- `mss`  `ms search`\n- `msl`  `ms load`\n- `msb`  `ms build`\n\n## Acceptance Criteria\n- Completions work in all major shells\n- Shell init is documented\n- Environment variables respected","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:21.298706295-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:28:21.298706295-05:00","labels":["completions","phase-6","shell"],"dependencies":[{"issue_id":"meta_skill-67m","depends_on_id":"meta_skill-14h","type":"blocks","created_at":"2026-01-13T22:28:36.922166232-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-6fi","title":"[P5] Bundle Format and Manifest","description":"# Bundle Format and Manifest\n\nDefine the skill bundle package format.\n\n## Tasks\n1. Define BundleManifest struct\n2. Define bundle directory structure\n3. Implement bundle serialization\n4. Implement bundle parsing\n5. Validate manifest on load\n\n## Bundle Structure (from Section 9.1)\n```\nmy-skills.bundle/\n manifest.yaml\n skills/\n    skill-a/\n       skill.spec.yaml\n       SKILL.md\n    skill-b/\n        ...\n checksums.sha256\n```\n\n## Manifest Format\n```yaml\nname: my-skills\nversion: 1.0.0\ndescription: My curated skill collection\nauthor: user@example.com\nskills:\n  - id: skill-a\n    path: skills/skill-a\n  - id: skill-b\n    path: skills/skill-b\ndependencies:\n  - bundle: other-skills\n    version: \"\u003e=1.0\"\n```\n\n## Checksum Verification\n- SHA256 for each file\n- Verification on install\n- Tamper detection\n\n## Acceptance Criteria\n- Bundles created correctly\n- Manifest validated\n- Checksums verified on install","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:03.231665909-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:27:03.231665909-05:00","labels":["bundles","format","phase-5"],"dependencies":[{"issue_id":"meta_skill-6fi","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:27:15.347650553-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-6hm","title":"Phase 1: Foundation (Core Infrastructure)","description":"# Phase 1: Foundation\n\nThe foundational layer that everything else builds on. Following the xf architecture pattern exactly.\n\n## Core Components\n1. **Project scaffolding** - Rust project with clap CLI, tokio async, proper module structure\n2. **SQLite storage** - WAL mode, FTS5, proper migrations, skill registry tables\n3. **Git archive** - Human-readable YAML skill storage, dual persistence with SQLite\n4. **Two-phase commit** - Crash-safe writes across SQLite + Git\n5. **Global file locking** - Prevent concurrent write corruption\n6. **SkillSpec data model** - Typed skill structure, deterministic compilation to SKILL.md\n7. **Basic CLI commands** - init, index, list, show\n\n## Key Design Decisions\n- Follow xf codebase exactly for patterns (same crates, same structure)\n- SQLite + Git dual persistence (from mcp_agent_mail pattern)\n- Robot mode JSON output on all commands\n- Offline-first, sync when available\n\n## Success Criteria\n- `ms init` creates config and database\n- `ms index` discovers skills from paths\n- `ms list` shows indexed skills\n- `ms show \u003cid\u003e` displays skill details\n- All commands support --robot JSON output","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:51.975096697-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:51.975096697-05:00"}
{"id":"meta_skill-6st","title":"CASS Mining: REST API Design Patterns","description":"Deep dive into command-to-endpoint mapping, OpenAPI specs, acceptance criteria patterns, API versioning strategies.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:42.136787849-05:00","created_by":"ubuntu","updated_at":"2026-01-13T21:53:49.449202641-05:00","closed_at":"2026-01-13T21:53:49.449202641-05:00","close_reason":"Section 39 added: REST API Design Patterns covering Zod schemas, OpenAPI generation, error taxonomies, auth patterns, cursor pagination, idempotency middleware","labels":["cass-mining"]}
{"id":"meta_skill-7b9","title":"[P5] One-URL Sharing","description":"# One-URL Sharing\n\nShare entire skill set via single URL.\n\n## Tasks\n1. Bundle serialization to URL-safe format\n2. Hosting options (GitHub, custom server)\n3. Import from URL\n4. QR code generation (optional)\n\n## URL Format\n```\nms://install/github.com/user/skills@v1.0.0\nms://install/custom.server.com/bundle/abc123\n```\n\n## Short URLs (Optional)\n- Integration with URL shortener\n- `ms.sh/abc123` style links\n\n## Import Flow\n1. `ms bundle install \u003curl\u003e`\n2. Parse URL to determine source\n3. Fetch and verify\n4. Install normally\n\n## Security\n- HTTPS required\n- Checksum verification mandatory\n- Warning for unknown sources\n\n## Acceptance Criteria\n- Single URL installs bundle\n- Multiple hosting backends\n- Security warnings appropriate","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:27:06.627988316-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:27:06.627988316-05:00","labels":["phase-5","sharing","url"],"dependencies":[{"issue_id":"meta_skill-7b9","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.484259335-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7b9","depends_on_id":"meta_skill-08m","type":"blocks","created_at":"2026-01-13T22:27:15.511781593-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7dg","title":"[P5] ms bundle Command","description":"# ms bundle Command\n\nBundle management CLI interface.\n\n## Subcommands\n- `ms bundle create \u003cname\u003e` - Create new bundle\n- `ms bundle add \u003cskill\u003e` - Add skill to bundle\n- `ms bundle publish` - Publish to GitHub\n- `ms bundle install \u003csource\u003e` - Install bundle\n- `ms bundle update` - Update installed bundles\n- `ms bundle list` - List installed bundles\n- `ms bundle remove \u003cname\u003e` - Uninstall bundle\n\n## Create Workflow\n```bash\nms bundle create my-skills\nms bundle add git/commit\nms bundle add rust/error-handling\nms bundle publish --repo user/my-skills\n```\n\n## Install Options\n- `--force` - Overwrite conflicts\n- `--no-deps` - Skip dependencies\n- `--dry-run` - Preview changes\n- `--robot` - JSON output\n\n## List Output\n```\nInstalled bundles:\n  my-skills@1.0.0 (local)\n    - git/commit\n    - rust/error-handling\n  team-skills@2.1.0 (github.com/team/skills)\n    - react/hooks\n    - typescript/strict\n```\n\n## Acceptance Criteria\n- All subcommands work\n- Robot mode supported\n- Error handling is clear","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:08.032768966-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:27:08.032768966-05:00","labels":["bundles","cli","phase-5"],"dependencies":[{"issue_id":"meta_skill-7dg","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.565726317-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7dg","depends_on_id":"meta_skill-08m","type":"blocks","created_at":"2026-01-13T22:27:15.594783428-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7dg","depends_on_id":"meta_skill-swe","type":"blocks","created_at":"2026-01-13T22:27:15.623131092-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7t2","title":"Unit Test Infrastructure","description":"## Overview\n\nEstablish comprehensive unit test infrastructure for the meta_skill CLI using table-driven tests and property-based tests with proptest. This bead implements Section 18.2 of the Testing Strategy with NO MOCKS - all tests use real implementations with real data fixtures.\n\n## Requirements\n\n### 1. Table-Driven Test Framework\n\nCreate a test utilities module at `src/test_utils/mod.rs`:\n\n```rust\n/// Table-driven test case structure\npub struct TestCase\u003cI, E\u003e {\n    pub name: \u0026'static str,\n    pub input: I,\n    pub expected: E,\n    pub should_panic: bool,\n}\n\n/// Run table-driven tests with detailed logging\npub fn run_table_tests\u003cI, E, F\u003e(cases: Vec\u003cTestCase\u003cI, E\u003e\u003e, test_fn: F)\nwhere\n    I: std::fmt::Debug + Clone,\n    E: std::fmt::Debug + PartialEq,\n    F: Fn(I) -\u003e E,\n{\n    for case in cases {\n        let start = std::time::Instant::now();\n        println!(\"[TEST] Running: {}\", case.name);\n        println!(\"[TEST] Input: {:?}\", case.input);\n        \n        let result = test_fn(case.input.clone());\n        let elapsed = start.elapsed();\n        \n        println!(\"[TEST] Expected: {:?}\", case.expected);\n        println!(\"[TEST] Actual: {:?}\", result);\n        println!(\"[TEST] Timing: {:?}\", elapsed);\n        \n        assert_eq!(result, case.expected, \"Test '{}' failed\", case.name);\n        println!(\"[TEST] PASSED: {} ({:?})\\n\", case.name, elapsed);\n    }\n}\n```\n\n### 2. Property-Based Tests with proptest\n\nAdd to Cargo.toml:\n```toml\n[dev-dependencies]\nproptest = \"1.4\"\nproptest-derive = \"0.4\"\n```\n\nProperty categories to test:\n\n#### 2.1 Idempotence (Serialize/Deserialize Roundtrip)\n```rust\nproptest! {\n    #[test]\n    fn test_skill_spec_roundtrip(skill in arb_skill_spec()) {\n        let serialized = serde_json::to_string(\u0026skill)?;\n        let deserialized: SkillSpec = serde_json::from_str(\u0026serialized)?;\n        prop_assert_eq!(skill, deserialized);\n    }\n    \n    #[test]\n    fn test_config_roundtrip(config in arb_config()) {\n        let toml_str = toml::to_string(\u0026config)?;\n        let parsed: Config = toml::from_str(\u0026toml_str)?;\n        prop_assert_eq!(config, parsed);\n    }\n}\n```\n\n#### 2.2 Determinism (Same Input = Same Output)\n```rust\n#[test]\nfn test_fnv1a_deterministic() {\n    let embeddings: Vec\u003c_\u003e = (0..100).map(|_| hash_embedding(\"test\")).collect();\n    assert!(embeddings.windows(2).all(|w| w[0] == w[1]));\n}\n\nproptest! {\n    #[test]\n    fn test_skill_id_generation_unique(name in \"[a-z]{3,20}\", desc in \".{0,100}\") {\n        let id1 = generate_skill_id(\u0026name, \u0026desc);\n        let id2 = generate_skill_id(\u0026name, \u0026desc);\n        prop_assert_eq!(id1, id2);\n    }\n    \n    #[test]\n    fn test_hash_embedding_deterministic(text in \".*\") {\n        let hash1 = hash_embedding(\u0026text);\n        let hash2 = hash_embedding(\u0026text);\n        prop_assert_eq!(hash1, hash2);\n    }\n}\n```\n\n#### 2.3 Safety (Never Panic on Arbitrary Input)\n```rust\nproptest! {\n    #[test]\n    fn test_parser_never_panics(input in \".*\") {\n        // Should not panic, may return error\n        let _ = parse_skill_content(\u0026input);\n    }\n    \n    #[test]\n    fn test_search_query_never_panics(query in \".*\") {\n        let _ = parse_search_query(\u0026query);\n    }\n    \n    #[test]\n    fn test_validator_never_panics(arbitrary in any::\u003cVec\u003cu8\u003e\u003e()) {\n        let input = String::from_utf8_lossy(\u0026arbitrary);\n        let _ = validate_skill_name(\u0026input);\n    }\n}\n```\n\n### 3. Test Fixture System\n\nCreate `src/test_utils/fixtures.rs`:\n\n```rust\nuse std::path::PathBuf;\nuse tempfile::TempDir;\n\n/// Test fixture providing isolated filesystem environment\npub struct UnitTestFixture {\n    pub temp_dir: TempDir,\n    pub data_path: PathBuf,\n}\n\nimpl UnitTestFixture {\n    pub fn new() -\u003e Self {\n        let temp_dir = TempDir::new().expect(\"Failed to create temp dir\");\n        let data_path = temp_dir.path().to_path_buf();\n        \n        println!(\"[FIXTURE] Created temp directory: {:?}\", data_path);\n        \n        Self { temp_dir, data_path }\n    }\n    \n    /// Create a test file with content\n    pub fn create_file(\u0026self, relative_path: \u0026str, content: \u0026str) -\u003e PathBuf {\n        let full_path = self.data_path.join(relative_path);\n        if let Some(parent) = full_path.parent() {\n            std::fs::create_dir_all(parent).expect(\"Failed to create parent dirs\");\n        }\n        std::fs::write(\u0026full_path, content).expect(\"Failed to write file\");\n        println!(\"[FIXTURE] Created file: {:?} ({} bytes)\", full_path, content.len());\n        full_path\n    }\n    \n    /// Create a test skill file\n    pub fn create_skill(\u0026self, name: \u0026str, content: \u0026str) -\u003e PathBuf {\n        self.create_file(\u0026format!(\"skills/{}/SKILL.md\", name), content)\n    }\n}\n\nimpl Drop for UnitTestFixture {\n    fn drop(\u0026mut self) {\n        println!(\"[FIXTURE] Cleaning up temp directory: {:?}\", self.data_path);\n    }\n}\n```\n\n### 4. Arbitrary Generators for proptest\n\nCreate `src/test_utils/arbitrary.rs`:\n\n```rust\nuse proptest::prelude::*;\n\n/// Generate arbitrary SkillSpec\npub fn arb_skill_spec() -\u003e impl Strategy\u003cValue = SkillSpec\u003e {\n    (\n        \"[a-z][a-z0-9_]{2,30}\",           // name\n        \".{10,200}\",                       // description\n        prop::collection::vec(\".{5,50}\", 0..5),  // tags\n        prop::option::of(\".{10,500}\"),    // content\n    ).prop_map(|(name, description, tags, content)| {\n        SkillSpec {\n            name,\n            description,\n            tags,\n            content,\n            ..Default::default()\n        }\n    })\n}\n\n/// Generate arbitrary Config\npub fn arb_config() -\u003e impl Strategy\u003cValue = Config\u003e {\n    (\n        any::\u003cbool\u003e(),                     // auto_index\n        1usize..100,                       // max_results\n        prop::option::of(\"[a-z]{3,10}\"),  // default_bundle\n    ).prop_map(|(auto_index, max_results, default_bundle)| {\n        Config {\n            auto_index,\n            max_results,\n            default_bundle,\n            ..Default::default()\n        }\n    })\n}\n\n/// Generate arbitrary search query\npub fn arb_search_query() -\u003e impl Strategy\u003cValue = String\u003e {\n    prop::string::string_regex(\"[a-zA-Z0-9 ]{1,100}\")\n        .unwrap()\n}\n```\n\n### 5. Detailed Logging Requirements\n\nEvery test must log:\n- **Test name**: Clear identifier\n- **Inputs**: All input values in debug format\n- **Expected output**: What the test expects\n- **Actual output**: What was actually produced\n- **Timing**: Duration of test execution\n- **Pass/Fail status**: Clear indication\n\nCreate logging helper at `src/test_utils/logging.rs`:\n\n```rust\nuse std::time::Instant;\n\npub struct TestLogger {\n    test_name: String,\n    start_time: Instant,\n}\n\nimpl TestLogger {\n    pub fn new(test_name: \u0026str) -\u003e Self {\n        println!(\"\\n{'='.repeat(60)}\");\n        println!(\"[TEST START] {}\", test_name);\n        println!(\"{'='.repeat(60)}\");\n        Self {\n            test_name: test_name.to_string(),\n            start_time: Instant::now(),\n        }\n    }\n    \n    pub fn log_input\u003cT: std::fmt::Debug\u003e(\u0026self, name: \u0026str, value: \u0026T) {\n        println!(\"[INPUT] {}: {:?}\", name, value);\n    }\n    \n    pub fn log_expected\u003cT: std::fmt::Debug\u003e(\u0026self, value: \u0026T) {\n        println!(\"[EXPECTED] {:?}\", value);\n    }\n    \n    pub fn log_actual\u003cT: std::fmt::Debug\u003e(\u0026self, value: \u0026T) {\n        println!(\"[ACTUAL] {:?}\", value);\n    }\n    \n    pub fn pass(\u0026self) {\n        let elapsed = self.start_time.elapsed();\n        println!(\"[RESULT] PASSED in {:?}\", elapsed);\n        println!(\"{'='.repeat(60)}\\n\");\n    }\n    \n    pub fn fail(\u0026self, reason: \u0026str) {\n        let elapsed = self.start_time.elapsed();\n        println!(\"[RESULT] FAILED in {:?}\", elapsed);\n        println!(\"[REASON] {}\", reason);\n        println!(\"{'='.repeat(60)}\\n\");\n    }\n}\n```\n\n### 6. Coverage Requirements\n\nTarget: \u003e80% coverage for core modules\n\nModules requiring full coverage:\n- `src/core/skill.rs` - SkillSpec parsing/validation\n- `src/core/config.rs` - Configuration loading/saving\n- `src/search/hash_embed.rs` - Hash embedding generation\n- `src/search/bm25.rs` - BM25 scoring\n- `src/search/rrf.rs` - RRF fusion\n- `src/db/sqlite.rs` - Database operations\n- `src/parser/skill_md.rs` - SKILL.md parsing\n\nAdd coverage configuration to `.cargo/config.toml`:\n```toml\n[env]\nCARGO_INCREMENTAL = \"0\"\nRUSTFLAGS = \"-Cinstrument-coverage\"\nLLVM_PROFILE_FILE = \"coverage/default-%p-%m.profraw\"\n```\n\n### 7. Test Organization\n\n```\ntests/\n unit/\n    mod.rs\n    skill_spec_tests.rs\n    config_tests.rs\n    parser_tests.rs\n    hash_embed_tests.rs\n    bm25_tests.rs\n    rrf_tests.rs\n properties/\n    mod.rs\n    roundtrip_tests.rs\n    determinism_tests.rs\n    safety_tests.rs\n fixtures/\n     skills/\n        valid_minimal.md\n        valid_full.md\n        invalid_*.md\n     configs/\n         default.toml\n         custom.toml\n```\n\n## Acceptance Criteria\n\n1. [ ] Table-driven test framework implemented with detailed logging\n2. [ ] Property-based tests for idempotence, determinism, safety\n3. [ ] Test fixture system with temp directory management\n4. [ ] Arbitrary generators for all core types\n5. [ ] All parsers have table-driven tests\n6. [ ] All serializers have roundtrip property tests\n7. [ ] All validators have safety property tests\n8. [ ] Coverage \u003e80% for core modules\n9. [ ] Tests run in CI with coverage reporting\n10. [ ] Test output includes timing for all tests\n\n## Dependencies\n\n- meta_skill-5s0 (Rust Project Scaffolding) - provides project structure","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T22:51:49.869175982-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.804171956-05:00","labels":["infrastructure","testing","unit-tests"],"dependencies":[{"issue_id":"meta_skill-7t2","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:54:23.059055914-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7t2","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.101334497-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7va","title":"[P3] ms load Command","description":"# ms load Command\n\nLoad skills with disclosure control.\n\n## Subcommands\n- `ms load \u003cskill\u003e` - Load at default level\n- `ms load \u003cskill\u003e --level \u003clevel\u003e` - Specific level\n- `ms load \u003cskill\u003e --pack \u003ctokens\u003e` - Token-constrained\n- `ms load \u003cskill\u003e --robot` - JSON output for agents\n\n## Options\n- --level: minimal, overview, operational, reference, full\n- --pack: Token budget (number or \"auto\")\n- --format: markdown, json, clipboard\n- --no-predicates: Skip predicate filtering\n- --explain: Show packing decisions\n\n## Output Format (Robot)\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"skill_id\": \"git/commit\",\n    \"level\": \"operational\",\n    \"tokens\": 800,\n    \"slices\": [\"rule-1\", \"command-1\", \"example-1\"],\n    \"content\": \"...\"\n  }\n}\n```\n\n## Acceptance Criteria\n- All disclosure levels work\n- Token packing respects budget\n- Robot mode outputs valid JSON\n- Predicate filtering works","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:17.798003642-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:24:17.798003642-05:00","labels":["cli","load","phase-3"],"dependencies":[{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-9ik","type":"blocks","created_at":"2026-01-13T22:24:26.008014903-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-1jl","type":"blocks","created_at":"2026-01-13T22:24:26.034413754-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-jka","type":"blocks","created_at":"2026-01-13T22:54:02.861277974-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-cn4","type":"blocks","created_at":"2026-01-13T22:54:05.781296512-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-7va","depends_on_id":"meta_skill-7ws","type":"blocks","created_at":"2026-01-13T23:00:21.792544553-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-7ws","title":"Meta-Skills (Composed Slice Bundles)","description":"# Meta-Skills: First-Class Composed Slice Bundles\n\n## Overview\n\nMeta-skills are a powerful abstraction that allows users to load curated combinations of slices from multiple skills as cohesive \"task kits.\" Rather than manually loading individual slices from various skills, a meta-skill bundles them together with intelligent defaults and optimal packing strategies.\n\n**Example Use Case:**\n```bash\nms load frontend-polish\n```\nThis single command loads slices from:\n- `nextjs-ui` (component patterns, routing, SSR)\n- `a11y` (accessibility guidelines, ARIA patterns)\n- `react-patterns` (hooks, state management, performance)\n\nAll optimally packed to fit within context budget while maximizing task relevance.\n\n## Background \u0026 Rationale\n\n### Problem Statement\n\nAs the skill ecosystem grows, users face cognitive overhead in:\n1. Discovering which skills contain relevant slices for their task\n2. Manually loading multiple slices from different skills\n3. Managing context budget when combining slices\n4. Ensuring slice compatibility and avoiding redundancy\n\n### Solution: Meta-Skills\n\nMeta-skills solve this by providing:\n- **Curated Bundles**: Expert-composed combinations for common workflows\n- **Automatic Packing**: Intelligent fitting within context constraints\n- **Version Coherence**: Ensuring slice versions work together\n- **Task Optimization**: Slices ordered/filtered by task relevance\n\n### Relationship to Plan Section 6.6\n\nThis implements the \"Meta-skills\" concept from Section 6.6, which describes:\n\u003e \"Meta-skills are first-class compositions that combine slices from multiple skills into task kits.\"\n\n## Core Data Structures\n\n### MetaSkill Struct\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n/// A meta-skill is a curated bundle of slices from one or more skills,\n/// designed for a specific workflow or task type.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MetaSkill {\n    /// Unique identifier (e.g., \"frontend-polish\", \"rust-safety\")\n    pub id: String,\n    \n    /// Human-readable name for display\n    pub name: String,\n    \n    /// Detailed description of what this meta-skill provides\n    pub description: String,\n    \n    /// Ordered list of slice references from various skills\n    /// Order matters: earlier slices have higher priority for packing\n    pub slices: Vec\u003cMetaSkillSliceRef\u003e,\n    \n    /// Strategy for resolving skill versions\n    pub pin_strategy: PinStrategy,\n    \n    /// Optional metadata for categorization and search\n    pub metadata: MetaSkillMetadata,\n    \n    /// Minimum context budget required (in tokens)\n    pub min_context_tokens: usize,\n    \n    /// Recommended context budget for full experience\n    pub recommended_context_tokens: usize,\n}\n\n/// Metadata for meta-skill discovery and categorization\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct MetaSkillMetadata {\n    /// Author or maintainer\n    pub author: Option\u003cString\u003e,\n    \n    /// Semantic version of this meta-skill definition\n    pub version: String,\n    \n    /// Tags for search/filtering\n    pub tags: Vec\u003cString\u003e,\n    \n    /// Tech stacks this meta-skill is designed for\n    pub tech_stacks: Vec\u003cString\u003e,\n    \n    /// When this meta-skill was last updated\n    pub updated_at: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n}\n```\n\n### MetaSkillSliceRef Struct\n\n```rust\n/// A reference to one or more slices within a specific skill\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MetaSkillSliceRef {\n    /// The skill ID to pull slices from (e.g., \"nextjs-ui\")\n    pub skill_id: String,\n    \n    /// Specific slice IDs to include from this skill\n    /// If empty, includes all slices (filtered by level)\n    pub slice_ids: Vec\u003cString\u003e,\n    \n    /// Override disclosure level for these slices\n    /// None means use the slice's default level\n    pub level: Option\u003cDisclosureLevel\u003e,\n    \n    /// Priority weight for packing decisions (higher = more important)\n    pub priority: u8,\n    \n    /// Whether this slice group is required or optional\n    pub required: bool,\n    \n    /// Conditions under which to include these slices\n    pub conditions: Vec\u003cSliceCondition\u003e,\n}\n\n/// Disclosure level for progressive disclosure\n#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]\npub enum DisclosureLevel {\n    /// Always visible, essential information\n    Core,\n    /// Shown when user asks for more detail\n    Extended,\n    /// Deep-dive information, rarely needed\n    Deep,\n}\n\n/// Conditions for conditional slice inclusion\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum SliceCondition {\n    /// Include only if tech stack matches\n    TechStack(String),\n    /// Include only if file pattern exists in project\n    FileExists(String),\n    /// Include only if environment variable is set\n    EnvVar(String),\n    /// Include only if another slice is included\n    DependsOn { skill_id: String, slice_id: String },\n}\n```\n\n### PinStrategy Enum\n\n```rust\n/// Strategy for resolving skill versions when loading meta-skills\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]\npub enum PinStrategy {\n    /// Always use the latest version compatible with other constraints\n    /// This is the default for most meta-skills\n    LatestCompatible,\n    \n    /// Pin to an exact version string (e.g., \"1.2.3\")\n    /// Use when reproducibility is critical\n    ExactVersion(String),\n    \n    /// Allow floating within a major version (e.g., \"1.x\")\n    /// Balances stability with updates\n    FloatingMajor,\n    \n    /// Use whatever version is currently installed locally\n    /// Fastest but least predictable\n    LocalInstalled,\n    \n    /// Custom version constraints per skill\n    PerSkill(HashMap\u003cString, String\u003e),\n}\n\nimpl Default for PinStrategy {\n    fn default() -\u003e Self {\n        PinStrategy::LatestCompatible\n    }\n}\n```\n\n## MetaSkill Manager Implementation\n\n```rust\nuse std::path::PathBuf;\nuse std::sync::Arc;\nuse tokio::sync::RwLock;\n\n/// Manages meta-skill definitions, resolution, and loading\npub struct MetaSkillManager {\n    /// Registry of available meta-skills\n    registry: Arc\u003cRwLock\u003cMetaSkillRegistry\u003e\u003e,\n    \n    /// Skill loader for resolving individual skills\n    skill_loader: Arc\u003cSkillLoader\u003e,\n    \n    /// Context budget manager\n    context_budget: Arc\u003cContextBudget\u003e,\n    \n    /// Cache for resolved meta-skills\n    resolution_cache: Arc\u003cRwLock\u003cResolutionCache\u003e\u003e,\n    \n    /// Logger for detailed tracing\n    logger: Arc\u003cdyn MetaSkillLogger\u003e,\n}\n\nimpl MetaSkillManager {\n    /// Load a meta-skill by ID, resolving all slice references\n    pub async fn load(\u0026self, meta_skill_id: \u0026str) -\u003e Result\u003cLoadedMetaSkill, MetaSkillError\u003e {\n        self.logger.log_load_start(meta_skill_id);\n        \n        // 1. Look up the meta-skill definition\n        let meta_skill = self.registry.read().await\n            .get(meta_skill_id)\n            .ok_or_else(|| MetaSkillError::NotFound(meta_skill_id.to_string()))?\n            .clone();\n        \n        self.logger.log_meta_skill_found(\u0026meta_skill);\n        \n        // 2. Resolve all skill versions according to pin strategy\n        let resolved_skills = self.resolve_skills(\u0026meta_skill).await?;\n        \n        // 3. Collect all referenced slices\n        let mut slices = Vec::new();\n        for slice_ref in \u0026meta_skill.slices {\n            let skill = resolved_skills.get(\u0026slice_ref.skill_id)\n                .ok_or_else(|| MetaSkillError::SkillNotResolved(slice_ref.skill_id.clone()))?;\n            \n            // Check conditions\n            if !self.evaluate_conditions(\u0026slice_ref.conditions).await? {\n                self.logger.log_slice_skipped(\u0026slice_ref, \"conditions not met\");\n                continue;\n            }\n            \n            // Resolve slice IDs (empty means all)\n            let slice_ids = if slice_ref.slice_ids.is_empty() {\n                skill.all_slice_ids()\n            } else {\n                slice_ref.slice_ids.clone()\n            };\n            \n            for slice_id in slice_ids {\n                if let Some(slice) = skill.get_slice(\u0026slice_id) {\n                    slices.push(ResolvedSlice {\n                        skill_id: slice_ref.skill_id.clone(),\n                        slice: slice.clone(),\n                        level: slice_ref.level,\n                        priority: slice_ref.priority,\n                        required: slice_ref.required,\n                    });\n                }\n            }\n        }\n        \n        self.logger.log_slices_collected(slices.len());\n        \n        // 4. Pack slices into context budget\n        let packed = self.pack_slices(slices, \u0026meta_skill).await?;\n        \n        self.logger.log_load_complete(meta_skill_id, \u0026packed);\n        \n        Ok(LoadedMetaSkill {\n            meta_skill,\n            resolved_skills,\n            packed_slices: packed,\n            loaded_at: chrono::Utc::now(),\n        })\n    }\n    \n    /// Resolve all skill versions according to the pin strategy\n    async fn resolve_skills(\n        \u0026self,\n        meta_skill: \u0026MetaSkill,\n    ) -\u003e Result\u003cHashMap\u003cString, Arc\u003cSkill\u003e\u003e, MetaSkillError\u003e {\n        let mut resolved = HashMap::new();\n        \n        for slice_ref in \u0026meta_skill.slices {\n            if resolved.contains_key(\u0026slice_ref.skill_id) {\n                continue;\n            }\n            \n            let version = match \u0026meta_skill.pin_strategy {\n                PinStrategy::LatestCompatible =\u003e {\n                    self.skill_loader.resolve_latest(\u0026slice_ref.skill_id).await?\n                }\n                PinStrategy::ExactVersion(v) =\u003e v.clone(),\n                PinStrategy::FloatingMajor =\u003e {\n                    self.skill_loader.resolve_floating_major(\u0026slice_ref.skill_id).await?\n                }\n                PinStrategy::LocalInstalled =\u003e {\n                    self.skill_loader.get_local_version(\u0026slice_ref.skill_id).await?\n                }\n                PinStrategy::PerSkill(versions) =\u003e {\n                    versions.get(\u0026slice_ref.skill_id)\n                        .cloned()\n                        .unwrap_or_else(|| \"latest\".to_string())\n                }\n            };\n            \n            let skill = self.skill_loader.load(\u0026slice_ref.skill_id, \u0026version).await?;\n            resolved.insert(slice_ref.skill_id.clone(), skill);\n        }\n        \n        Ok(resolved)\n    }\n    \n    /// Pack slices into the available context budget\n    async fn pack_slices(\n        \u0026self,\n        slices: Vec\u003cResolvedSlice\u003e,\n        meta_skill: \u0026MetaSkill,\n    ) -\u003e Result\u003cPackedSlices, MetaSkillError\u003e {\n        let budget = self.context_budget.available().await;\n        \n        self.logger.log_packing_start(slices.len(), budget);\n        \n        // Sort by priority (required first, then by priority weight)\n        let mut sorted_slices = slices;\n        sorted_slices.sort_by(|a, b| {\n            match (a.required, b.required) {\n                (true, false) =\u003e std::cmp::Ordering::Less,\n                (false, true) =\u003e std::cmp::Ordering::Greater,\n                _ =\u003e b.priority.cmp(\u0026a.priority),\n            }\n        });\n        \n        let mut packed = PackedSlices::new(budget);\n        \n        for slice in sorted_slices {\n            let slice_tokens = slice.slice.estimate_tokens();\n            \n            if packed.can_fit(slice_tokens) {\n                packed.add(slice);\n                self.logger.log_slice_packed(\u0026slice.slice.id, slice_tokens);\n            } else if slice.required {\n                return Err(MetaSkillError::InsufficientBudget {\n                    required: slice_tokens,\n                    available: packed.remaining(),\n                    slice_id: slice.slice.id.clone(),\n                });\n            } else {\n                self.logger.log_slice_dropped(\u0026slice.slice.id, \"budget exceeded\");\n            }\n        }\n        \n        Ok(packed)\n    }\n}\n```\n\n## Meta-Skill Definition Format (TOML)\n\nMeta-skills are defined in TOML files for easy authoring:\n\n```toml\n# ~/.meta_skill/meta-skills/frontend-polish.toml\n\n[meta_skill]\nid = \"frontend-polish\"\nname = \"Frontend Polish Kit\"\ndescription = \"\"\"\nA comprehensive bundle for polishing frontend applications.\nIncludes UI component patterns, accessibility guidelines, and React best practices.\nOptimized for Next.js projects but works with any React setup.\n\"\"\"\n\n[meta_skill.metadata]\nauthor = \"meta_skill-community\"\nversion = \"1.0.0\"\ntags = [\"frontend\", \"react\", \"accessibility\", \"ui\"]\ntech_stacks = [\"nextjs\", \"react\"]\n\n[meta_skill.pin_strategy]\ntype = \"LatestCompatible\"\n\n# Context requirements\nmin_context_tokens = 4000\nrecommended_context_tokens = 12000\n\n# Slice references - order matters for packing priority\n\n[[slices]]\nskill_id = \"nextjs-ui\"\nslice_ids = [\"component-patterns\", \"routing-best-practices\", \"ssr-guidelines\"]\npriority = 100\nrequired = true\n\n[[slices]]\nskill_id = \"a11y\"\nslice_ids = [\"aria-patterns\", \"keyboard-navigation\", \"screen-reader-tips\"]\npriority = 90\nrequired = true\nlevel = \"Core\"\n\n[[slices]]\nskill_id = \"react-patterns\"\nslice_ids = [\"hooks-patterns\", \"state-management\", \"performance-optimization\"]\npriority = 80\nrequired = false\n\n[[slices]]\nskill_id = \"react-patterns\"\nslice_ids = [\"advanced-composition\", \"render-props\", \"hoc-patterns\"]\npriority = 50\nrequired = false\nlevel = \"Extended\"\n\n# Conditional slice - only included for TypeScript projects\n[[slices]]\nskill_id = \"typescript-react\"\nslice_ids = [\"type-safe-props\", \"generic-components\"]\npriority = 70\nrequired = false\n\n[[slices.conditions]]\ntype = \"FileExists\"\npattern = \"tsconfig.json\"\n```\n\n## Parsing and Validation\n\n```rust\nuse std::path::Path;\n\n/// Parser for meta-skill TOML definitions\npub struct MetaSkillParser;\n\nimpl MetaSkillParser {\n    /// Parse a meta-skill from a TOML file\n    pub fn parse_file(path: \u0026Path) -\u003e Result\u003cMetaSkill, ParseError\u003e {\n        let content = std::fs::read_to_string(path)\n            .map_err(|e| ParseError::IoError(path.to_path_buf(), e))?;\n        \n        Self::parse_str(\u0026content, path)\n    }\n    \n    /// Parse a meta-skill from a TOML string\n    pub fn parse_str(content: \u0026str, source: \u0026Path) -\u003e Result\u003cMetaSkill, ParseError\u003e {\n        let raw: RawMetaSkillToml = toml::from_str(content)\n            .map_err(|e| ParseError::TomlError(source.to_path_buf(), e))?;\n        \n        Self::validate_and_convert(raw, source)\n    }\n    \n    /// Validate the parsed structure and convert to domain type\n    fn validate_and_convert(\n        raw: RawMetaSkillToml,\n        source: \u0026Path,\n    ) -\u003e Result\u003cMetaSkill, ParseError\u003e {\n        // Validate required fields\n        if raw.meta_skill.id.is_empty() {\n            return Err(ParseError::MissingField(source.to_path_buf(), \"id\"));\n        }\n        \n        if raw.slices.is_empty() {\n            return Err(ParseError::MissingField(source.to_path_buf(), \"slices\"));\n        }\n        \n        // Validate slice references\n        for (i, slice) in raw.slices.iter().enumerate() {\n            if slice.skill_id.is_empty() {\n                return Err(ParseError::InvalidSlice(\n                    source.to_path_buf(),\n                    i,\n                    \"skill_id cannot be empty\",\n                ));\n            }\n        }\n        \n        // Convert to domain type\n        Ok(MetaSkill {\n            id: raw.meta_skill.id,\n            name: raw.meta_skill.name,\n            description: raw.meta_skill.description,\n            slices: raw.slices.into_iter().map(|s| s.into()).collect(),\n            pin_strategy: raw.meta_skill.pin_strategy.unwrap_or_default().into(),\n            metadata: raw.meta_skill.metadata.unwrap_or_default().into(),\n            min_context_tokens: raw.meta_skill.min_context_tokens.unwrap_or(2000),\n            recommended_context_tokens: raw.meta_skill.recommended_context_tokens.unwrap_or(8000),\n        })\n    }\n}\n```\n\n## Registry Implementation\n\n```rust\n/// Registry for discovering and managing meta-skills\npub struct MetaSkillRegistry {\n    /// Map of meta-skill ID to definition\n    meta_skills: HashMap\u003cString, MetaSkill\u003e,\n    \n    /// Index for tag-based search\n    tag_index: HashMap\u003cString, Vec\u003cString\u003e\u003e,\n    \n    /// Index for tech-stack-based search\n    tech_stack_index: HashMap\u003cString, Vec\u003cString\u003e\u003e,\n    \n    /// Paths to meta-skill directories\n    search_paths: Vec\u003cPathBuf\u003e,\n}\n\nimpl MetaSkillRegistry {\n    /// Create a new registry with default search paths\n    pub fn new() -\u003e Self {\n        let mut search_paths = vec![];\n        \n        // User meta-skills\n        if let Some(home) = dirs::home_dir() {\n            search_paths.push(home.join(\".meta_skill\").join(\"meta-skills\"));\n        }\n        \n        // Project-local meta-skills\n        search_paths.push(PathBuf::from(\".meta_skill\").join(\"meta-skills\"));\n        \n        // System meta-skills\n        search_paths.push(PathBuf::from(\"/usr/share/meta_skill/meta-skills\"));\n        \n        Self {\n            meta_skills: HashMap::new(),\n            tag_index: HashMap::new(),\n            tech_stack_index: HashMap::new(),\n            search_paths,\n        }\n    }\n    \n    /// Scan all search paths and load meta-skill definitions\n    pub fn scan(\u0026mut self) -\u003e Result\u003cScanResult, RegistryError\u003e {\n        let mut result = ScanResult::default();\n        \n        for path in \u0026self.search_paths {\n            if !path.exists() {\n                continue;\n            }\n            \n            for entry in std::fs::read_dir(path)? {\n                let entry = entry?;\n                let file_path = entry.path();\n                \n                if file_path.extension().map(|e| e == \"toml\").unwrap_or(false) {\n                    match MetaSkillParser::parse_file(\u0026file_path) {\n                        Ok(meta_skill) =\u003e {\n                            self.index_meta_skill(\u0026meta_skill);\n                            self.meta_skills.insert(meta_skill.id.clone(), meta_skill);\n                            result.loaded += 1;\n                        }\n                        Err(e) =\u003e {\n                            result.errors.push((file_path, e));\n                        }\n                    }\n                }\n            }\n        }\n        \n        Ok(result)\n    }\n    \n    /// Index a meta-skill for search\n    fn index_meta_skill(\u0026mut self, meta_skill: \u0026MetaSkill) {\n        // Index by tags\n        for tag in \u0026meta_skill.metadata.tags {\n            self.tag_index\n                .entry(tag.clone())\n                .or_default()\n                .push(meta_skill.id.clone());\n        }\n        \n        // Index by tech stack\n        for stack in \u0026meta_skill.metadata.tech_stacks {\n            self.tech_stack_index\n                .entry(stack.clone())\n                .or_default()\n                .push(meta_skill.id.clone());\n        }\n    }\n    \n    /// Search meta-skills by various criteria\n    pub fn search(\u0026self, query: \u0026MetaSkillQuery) -\u003e Vec\u003c\u0026MetaSkill\u003e {\n        let mut results: Vec\u003c\u0026MetaSkill\u003e = self.meta_skills.values().collect();\n        \n        // Filter by text search\n        if let Some(text) = \u0026query.text {\n            let text_lower = text.to_lowercase();\n            results.retain(|ms| {\n                ms.name.to_lowercase().contains(\u0026text_lower)\n                    || ms.description.to_lowercase().contains(\u0026text_lower)\n                    || ms.id.to_lowercase().contains(\u0026text_lower)\n            });\n        }\n        \n        // Filter by tags\n        if !query.tags.is_empty() {\n            results.retain(|ms| {\n                query.tags.iter().any(|t| ms.metadata.tags.contains(t))\n            });\n        }\n        \n        // Filter by tech stack\n        if let Some(stack) = \u0026query.tech_stack {\n            results.retain(|ms| ms.metadata.tech_stacks.contains(stack));\n        }\n        \n        results\n    }\n}\n```\n\n## CLI Integration\n\n```rust\n/// CLI subcommand for meta-skill operations\npub enum MetaSkillCommand {\n    /// Load a meta-skill into the current context\n    Load {\n        /// Meta-skill ID to load\n        id: String,\n        /// Override context budget\n        #[arg(long)]\n        budget: Option\u003cusize\u003e,\n        /// Force reload even if already loaded\n        #[arg(long)]\n        force: bool,\n    },\n    /// List available meta-skills\n    List {\n        /// Filter by tag\n        #[arg(long)]\n        tag: Option\u003cString\u003e,\n        /// Filter by tech stack\n        #[arg(long)]\n        stack: Option\u003cString\u003e,\n    },\n    /// Show details of a specific meta-skill\n    Show {\n        /// Meta-skill ID\n        id: String,\n    },\n    /// Create a new meta-skill definition\n    Create {\n        /// Meta-skill ID\n        id: String,\n        /// Interactive mode\n        #[arg(long)]\n        interactive: bool,\n    },\n}\n```\n\n## Tasks\n\n### Task 1: Define Core Data Structures\n- [ ] Create `src/meta_skills/types.rs` with all struct definitions\n- [ ] Implement `Serialize`/`Deserialize` for all types\n- [ ] Add validation methods to each struct\n- [ ] Write unit tests for serialization round-trips\n\n### Task 2: Implement MetaSkillParser\n- [ ] Create `src/meta_skills/parser.rs`\n- [ ] Implement TOML parsing with proper error handling\n- [ ] Add validation for required fields\n- [ ] Support all condition types in slice references\n- [ ] Write tests with valid and invalid TOML inputs\n\n### Task 3: Implement MetaSkillRegistry\n- [ ] Create `src/meta_skills/registry.rs`\n- [ ] Implement search path discovery\n- [ ] Build tag and tech-stack indexes\n- [ ] Implement search functionality\n- [ ] Add caching for parsed definitions\n\n### Task 4: Implement MetaSkillManager\n- [ ] Create `src/meta_skills/manager.rs`\n- [ ] Implement skill version resolution per pin strategy\n- [ ] Implement condition evaluation\n- [ ] Implement slice packing algorithm\n- [ ] Add comprehensive logging throughout\n\n### Task 5: Implement CLI Commands\n- [ ] Add `ms meta-skill load` subcommand\n- [ ] Add `ms meta-skill list` subcommand\n- [ ] Add `ms meta-skill show` subcommand\n- [ ] Add `ms meta-skill create` subcommand\n\n### Task 6: Create Built-in Meta-Skills\n- [ ] Create `frontend-polish.toml` meta-skill\n- [ ] Create `rust-safety.toml` meta-skill\n- [ ] Create `api-design.toml` meta-skill\n- [ ] Document meta-skill authoring guide\n\n## Acceptance Criteria\n\n1. **Definition Loading**: Meta-skill TOML files parse correctly with full validation\n2. **Skill Resolution**: All pin strategies resolve skill versions correctly\n3. **Slice Packing**: Slices are packed optimally within context budget\n4. **Required Enforcement**: Required slices always included or error raised\n5. **Condition Evaluation**: All condition types evaluate correctly\n6. **Registry Search**: Search by text, tags, and tech stack works correctly\n7. **CLI Integration**: All commands work and produce helpful output\n8. **Logging**: All operations produce detailed trace logs\n\n## Testing Requirements\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_meta_skill_parse_valid() {\n        let toml = r#\"\n            [meta_skill]\n            id = \"test-meta\"\n            name = \"Test Meta-Skill\"\n            description = \"A test meta-skill\"\n            \n            [[slices]]\n            skill_id = \"skill-1\"\n            slice_ids = [\"slice-a\", \"slice-b\"]\n            priority = 100\n            required = true\n        \"#;\n        \n        let result = MetaSkillParser::parse_str(toml, Path::new(\"test.toml\"));\n        assert!(result.is_ok());\n        \n        let ms = result.unwrap();\n        assert_eq!(ms.id, \"test-meta\");\n        assert_eq!(ms.slices.len(), 1);\n        assert!(ms.slices[0].required);\n    }\n    \n    #[test]\n    fn test_pin_strategy_resolution() {\n        // Test each pin strategy type\n    }\n    \n    #[test]\n    fn test_slice_packing_respects_budget() {\n        // Test that packing stays within budget\n    }\n    \n    #[test]\n    fn test_required_slices_must_fit() {\n        // Test that required slices cause error if they don't fit\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n#[tokio::test]\nasync fn test_meta_skill_load_end_to_end() {\n    // Set up test registry with sample meta-skills\n    // Load a meta-skill\n    // Verify all slices resolved correctly\n}\n\n#[tokio::test]\nasync fn test_condition_evaluation() {\n    // Test file-exists condition\n    // Test tech-stack condition\n    // Test depends-on condition\n}\n```\n\n### Logging Requirements\n\nAll operations must log with the following detail levels:\n\n```rust\n// DEBUG level - development troubleshooting\nlog::debug!(\"Parsing meta-skill from {:?}\", path);\nlog::debug!(\"Resolved skill {} to version {}\", skill_id, version);\nlog::debug!(\"Evaluating condition: {:?}\", condition);\n\n// INFO level - normal operation visibility\nlog::info!(\"Loading meta-skill: {}\", meta_skill_id);\nlog::info!(\"Packed {} slices using {} tokens\", count, tokens);\n\n// WARN level - recoverable issues\nlog::warn!(\"Slice {} dropped due to budget constraints\", slice_id);\nlog::warn!(\"Condition evaluation failed, skipping slice: {}\", slice_id);\n\n// ERROR level - failures\nlog::error!(\"Failed to parse meta-skill {}: {}\", path, error);\nlog::error!(\"Required slice {} exceeds available budget\", slice_id);\n```\n\n## Dependencies\n\n- **Depends on**: `meta_skill-0an` (Micro-Slicing Engine) - Need slice infrastructure\n- **Blocks**: `meta_skill-7va` (ms load Command) - Load command uses meta-skills\n\n## References\n\n- Plan Section 6.6: Meta-skills concept\n- Plan Section 6.1: Micro-slicing engine (dependency)\n- Plan Section 4.1: Progressive disclosure levels","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:54:18.6537013-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:54:18.6537013-05:00","labels":["composition","meta-skills","phase-3"],"dependencies":[{"issue_id":"meta_skill-7ws","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T23:00:20.519752956-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-897","title":"CASS Mining: Optimization Patterns","description":"Deep dive into cost analytics optimization, O(n log k) vs O(n log n) patterns, topk heap collectors, performance optimization workflows. Extract actionable skill patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:15.413819545-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:25:14.416904398-05:00","closed_at":"2026-01-13T18:25:14.416904398-05:00","close_reason":"Section 31 added: Optimization Patterns and Methodology (~1,550 lines of comprehensive optimization patterns from CASS mining)","labels":["cass-mining"]}
{"id":"meta_skill-8df","title":"Context Fingerprints \u0026 Suggestion Cooldowns","description":"# Context Fingerprints \u0026 Suggestion Cooldowns\n\n## Overview\n\nThis feature prevents suggestion spam by computing a \"fingerprint\" of the current context and implementing cooldowns for skill suggestions. When a user dismisses or ignores a suggestion, and the context hasn't meaningfully changed, the system should not re-suggest the same skills.\n\n**Problem Statement:**\nWithout fingerprinting and cooldowns, the suggestion system would repeatedly suggest the same skills every time the user invokes any command, leading to:\n- User frustration from repetitive suggestions\n- Degraded trust in the suggestion system\n- Increased cognitive load filtering out noise\n\n## Background \u0026 Rationale\n\n### Section 7.2.1 Reference\n\nFrom the plan Section 7.2.1:\n\u003e \"Prevent suggestion spam when context hasn't meaningfully changed. Compute context fingerprint from repo root, git head, diff hash, open files, recent commands.\"\n\n### What Constitutes \"Meaningful Change\"?\n\nThe fingerprint captures signals that indicate the user's working context has shifted:\n\n1. **Repository Root**: Different project entirely\n2. **Git HEAD**: New commits, branch switches\n3. **Diff Hash**: Uncommitted changes (staged + unstaged)\n4. **Open Files Hash**: Files user is actively editing\n5. **Recent Commands Hash**: CLI activity patterns\n\nWhen ANY of these change significantly, the fingerprint changes, allowing fresh suggestions.\n\n### Cooldown Behavior\n\n- When a skill is suggested and dismissed/ignored, record the fingerprint\n- Do not re-suggest that skill until fingerprint changes\n- Cooldown entries expire after configurable TTL (default: 1 hour)\n- Per-skill cooldowns (dismissing skill A doesn't affect skill B)\n\n## Core Data Structures\n\n### ContextFingerprint Struct\n\n```rust\nuse std::path::PathBuf;\nuse std::hash::{Hash, Hasher};\nuse std::collections::hash_map::DefaultHasher;\n\n/// A fingerprint capturing the current working context.\n/// Used to detect meaningful changes that should reset suggestion cooldowns.\n#[derive(Debug, Clone, PartialEq, Eq, Hash)]\npub struct ContextFingerprint {\n    /// Absolute path to the repository root (or project root if not git)\n    pub repo_root: PathBuf,\n    \n    /// Current git HEAD commit hash (None if not a git repo or detached)\n    pub git_head: Option\u003cString\u003e,\n    \n    /// Hash of the current git diff (staged + unstaged changes)\n    /// Changes when user modifies files\n    pub diff_hash: u64,\n    \n    /// Hash of the set of currently open files (from editor integration)\n    /// Changes when user opens/closes files\n    pub open_files_hash: u64,\n    \n    /// Hash of recent command history (last N commands)\n    /// Captures workflow patterns\n    pub recent_commands_hash: u64,\n}\n\nimpl ContextFingerprint {\n    /// Create a new fingerprint from current context\n    pub fn capture(ctx: \u0026ContextCapture) -\u003e Self {\n        Self {\n            repo_root: ctx.repo_root.clone(),\n            git_head: ctx.git_head.clone(),\n            diff_hash: ctx.compute_diff_hash(),\n            open_files_hash: ctx.compute_open_files_hash(),\n            recent_commands_hash: ctx.compute_commands_hash(),\n        }\n    }\n    \n    /// Compute a single u64 hash of the entire fingerprint for storage\n    pub fn as_u64(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        self.hash(\u0026mut hasher);\n        hasher.finish()\n    }\n    \n    /// Check if this fingerprint differs meaningfully from another\n    /// Returns a ChangeSignificance indicating how different they are\n    pub fn compare(\u0026self, other: \u0026Self) -\u003e ChangeSignificance {\n        // Different repo is always a major change\n        if self.repo_root != other.repo_root {\n            return ChangeSignificance::Major;\n        }\n        \n        // Different git HEAD is a major change (new commits, branch switch)\n        if self.git_head != other.git_head {\n            return ChangeSignificance::Major;\n        }\n        \n        // Collect minor changes\n        let mut minor_changes = 0;\n        \n        if self.diff_hash != other.diff_hash {\n            minor_changes += 1;\n        }\n        \n        if self.open_files_hash != other.open_files_hash {\n            minor_changes += 1;\n        }\n        \n        if self.recent_commands_hash != other.recent_commands_hash {\n            minor_changes += 1;\n        }\n        \n        match minor_changes {\n            0 =\u003e ChangeSignificance::None,\n            1 =\u003e ChangeSignificance::Minor,\n            _ =\u003e ChangeSignificance::Moderate,\n        }\n    }\n}\n\n/// How significantly has the context changed?\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]\npub enum ChangeSignificance {\n    /// No change detected\n    None,\n    /// Small change (e.g., one file edited)\n    Minor,\n    /// Moderate change (e.g., multiple files, different commands)\n    Moderate,\n    /// Major change (different repo, different branch/commit)\n    Major,\n}\n```\n\n### ContextCapture Struct\n\n```rust\nuse std::path::PathBuf;\nuse std::process::Command;\n\n/// Captures raw context data for fingerprint computation\npub struct ContextCapture {\n    pub repo_root: PathBuf,\n    pub git_head: Option\u003cString\u003e,\n    pub diff_content: Option\u003cString\u003e,\n    pub open_files: Vec\u003cPathBuf\u003e,\n    pub recent_commands: Vec\u003cString\u003e,\n}\n\nimpl ContextCapture {\n    /// Capture current context from the environment\n    pub fn capture_current() -\u003e Result\u003cSelf, CaptureError\u003e {\n        let repo_root = Self::find_repo_root()?;\n        let git_head = Self::get_git_head(\u0026repo_root);\n        let diff_content = Self::get_git_diff(\u0026repo_root);\n        let open_files = Self::get_open_files()?;\n        let recent_commands = Self::get_recent_commands()?;\n        \n        Ok(Self {\n            repo_root,\n            git_head,\n            diff_content,\n            open_files,\n            recent_commands,\n        })\n    }\n    \n    /// Find the repository or project root\n    fn find_repo_root() -\u003e Result\u003cPathBuf, CaptureError\u003e {\n        // Try git first\n        let output = Command::new(\"git\")\n            .args([\"rev-parse\", \"--show-toplevel\"])\n            .output();\n        \n        if let Ok(output) = output {\n            if output.status.success() {\n                let path = String::from_utf8_lossy(\u0026output.stdout);\n                return Ok(PathBuf::from(path.trim()));\n            }\n        }\n        \n        // Fall back to current directory\n        std::env::current_dir().map_err(CaptureError::IoError)\n    }\n    \n    /// Get current git HEAD commit hash\n    fn get_git_head(repo_root: \u0026Path) -\u003e Option\u003cString\u003e {\n        let output = Command::new(\"git\")\n            .args([\"rev-parse\", \"HEAD\"])\n            .current_dir(repo_root)\n            .output()\n            .ok()?;\n        \n        if output.status.success() {\n            Some(String::from_utf8_lossy(\u0026output.stdout).trim().to_string())\n        } else {\n            None\n        }\n    }\n    \n    /// Get current git diff (staged + unstaged)\n    fn get_git_diff(repo_root: \u0026Path) -\u003e Option\u003cString\u003e {\n        let output = Command::new(\"git\")\n            .args([\"diff\", \"HEAD\"])\n            .current_dir(repo_root)\n            .output()\n            .ok()?;\n        \n        if output.status.success() {\n            Some(String::from_utf8_lossy(\u0026output.stdout).to_string())\n        } else {\n            None\n        }\n    }\n    \n    /// Get list of open files from editor integration\n    fn get_open_files() -\u003e Result\u003cVec\u003cPathBuf\u003e, CaptureError\u003e {\n        // Check for VS Code workspace state\n        if let Some(files) = Self::get_vscode_open_files() {\n            return Ok(files);\n        }\n        \n        // Check for Neovim RPC\n        if let Some(files) = Self::get_neovim_open_files() {\n            return Ok(files);\n        }\n        \n        // Fall back to recently modified files in repo\n        Self::get_recently_modified_files()\n    }\n    \n    /// Get recent commands from history\n    fn get_recent_commands() -\u003e Result\u003cVec\u003cString\u003e, CaptureError\u003e {\n        // Read from ms command history file\n        let history_path = dirs::data_dir()\n            .unwrap_or_default()\n            .join(\"meta_skill\")\n            .join(\"command_history\");\n        \n        if history_path.exists() {\n            let content = std::fs::read_to_string(\u0026history_path)?;\n            let commands: Vec\u003cString\u003e = content\n                .lines()\n                .rev()\n                .take(20) // Last 20 commands\n                .map(|s| s.to_string())\n                .collect();\n            Ok(commands)\n        } else {\n            Ok(vec![])\n        }\n    }\n    \n    /// Compute hash of diff content\n    pub fn compute_diff_hash(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        if let Some(diff) = \u0026self.diff_content {\n            diff.hash(\u0026mut hasher);\n        }\n        hasher.finish()\n    }\n    \n    /// Compute hash of open files set\n    pub fn compute_open_files_hash(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        let mut sorted_files: Vec\u003c_\u003e = self.open_files.iter().collect();\n        sorted_files.sort();\n        for file in sorted_files {\n            file.hash(\u0026mut hasher);\n        }\n        hasher.finish()\n    }\n    \n    /// Compute hash of recent commands\n    pub fn compute_commands_hash(\u0026self) -\u003e u64 {\n        let mut hasher = DefaultHasher::new();\n        for cmd in \u0026self.recent_commands {\n            cmd.hash(\u0026mut hasher);\n        }\n        hasher.finish()\n    }\n}\n```\n\n### SuggestionCooldownCache Struct\n\n```rust\nuse std::collections::HashMap;\nuse chrono::{DateTime, Utc, Duration};\n\n/// Type alias for skill identifiers\npub type SkillId = String;\n\n/// Cache for tracking suggestion cooldowns per skill\n#[derive(Debug, Clone)]\npub struct SuggestionCooldownCache {\n    /// Map from skill ID to cooldown entry\n    entries: HashMap\u003cSkillId, CooldownEntry\u003e,\n    \n    /// Maximum number of entries to store (LRU eviction)\n    max_entries: usize,\n    \n    /// Default cooldown duration\n    default_ttl: Duration,\n    \n    /// Minimum context change significance to reset cooldown\n    reset_threshold: ChangeSignificance,\n}\n\n/// A single cooldown entry for a skill\n#[derive(Debug, Clone)]\npub struct CooldownEntry {\n    /// The skill IDs this cooldown applies to\n    pub skill_ids: Vec\u003cSkillId\u003e,\n    \n    /// When this suggestion was made\n    pub suggested_at: DateTime\u003cUtc\u003e,\n    \n    /// Fingerprint when suggestion was made (as u64 for storage efficiency)\n    pub fingerprint: u64,\n    \n    /// How the user responded to the suggestion\n    pub user_response: SuggestionResponse,\n    \n    /// When this entry expires (None = never auto-expire)\n    pub expires_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\n/// How did the user respond to a suggestion?\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum SuggestionResponse {\n    /// User explicitly dismissed (\"no thanks\")\n    Dismissed,\n    /// User ignored (didn't interact)\n    Ignored,\n    /// User accepted (loaded the skill)\n    Accepted,\n    /// User snoozed (\"remind me later\")\n    Snoozed { until: DateTime\u003cUtc\u003e },\n}\n\nimpl SuggestionCooldownCache {\n    /// Create a new cooldown cache with default settings\n    pub fn new() -\u003e Self {\n        Self {\n            entries: HashMap::new(),\n            max_entries: 1000,\n            default_ttl: Duration::hours(1),\n            reset_threshold: ChangeSignificance::Moderate,\n        }\n    }\n    \n    /// Create with custom configuration\n    pub fn with_config(config: CooldownConfig) -\u003e Self {\n        Self {\n            entries: HashMap::new(),\n            max_entries: config.max_entries,\n            default_ttl: config.default_ttl,\n            reset_threshold: config.reset_threshold,\n        }\n    }\n    \n    /// Check if a skill is currently on cooldown\n    pub fn is_on_cooldown(\n        \u0026self,\n        skill_id: \u0026SkillId,\n        current_fingerprint: \u0026ContextFingerprint,\n    ) -\u003e CooldownStatus {\n        let entry = match self.entries.get(skill_id) {\n            Some(e) =\u003e e,\n            None =\u003e return CooldownStatus::NotOnCooldown,\n        };\n        \n        // Check expiration\n        if let Some(expires) = entry.expires_at {\n            if Utc::now() \u003e expires {\n                return CooldownStatus::Expired;\n            }\n        }\n        \n        // Check if context has changed enough\n        let current_fp_hash = current_fingerprint.as_u64();\n        if current_fp_hash != entry.fingerprint {\n            // Fingerprint changed - need to determine significance\n            // For now, any change resets (we store hash, not full fingerprint)\n            return CooldownStatus::ContextChanged;\n        }\n        \n        // Still on cooldown\n        CooldownStatus::OnCooldown {\n            since: entry.suggested_at,\n            response: entry.user_response,\n        }\n    }\n    \n    /// Record a suggestion and user response\n    pub fn record_suggestion(\n        \u0026mut self,\n        skill_id: SkillId,\n        fingerprint: \u0026ContextFingerprint,\n        response: SuggestionResponse,\n    ) {\n        // Evict oldest if at capacity\n        if self.entries.len() \u003e= self.max_entries {\n            self.evict_oldest();\n        }\n        \n        let expires_at = match response {\n            SuggestionResponse::Accepted =\u003e None, // Don't cooldown accepted\n            SuggestionResponse::Snoozed { until } =\u003e Some(until),\n            _ =\u003e Some(Utc::now() + self.default_ttl),\n        };\n        \n        let entry = CooldownEntry {\n            skill_ids: vec![skill_id.clone()],\n            suggested_at: Utc::now(),\n            fingerprint: fingerprint.as_u64(),\n            user_response: response,\n            expires_at,\n        };\n        \n        self.entries.insert(skill_id, entry);\n    }\n    \n    /// Record suggestion for multiple skills at once\n    pub fn record_batch_suggestion(\n        \u0026mut self,\n        skill_ids: Vec\u003cSkillId\u003e,\n        fingerprint: \u0026ContextFingerprint,\n        response: SuggestionResponse,\n    ) {\n        for skill_id in skill_ids {\n            self.record_suggestion(skill_id, fingerprint, response);\n        }\n    }\n    \n    /// Clear cooldown for a specific skill\n    pub fn clear_cooldown(\u0026mut self, skill_id: \u0026SkillId) {\n        self.entries.remove(skill_id);\n    }\n    \n    /// Clear all expired entries\n    pub fn cleanup_expired(\u0026mut self) {\n        let now = Utc::now();\n        self.entries.retain(|_, entry| {\n            entry.expires_at.map(|exp| exp \u003e now).unwrap_or(true)\n        });\n    }\n    \n    /// Evict oldest entry (LRU)\n    fn evict_oldest(\u0026mut self) {\n        if let Some(oldest_key) = self.entries\n            .iter()\n            .min_by_key(|(_, e)| e.suggested_at)\n            .map(|(k, _)| k.clone())\n        {\n            self.entries.remove(\u0026oldest_key);\n        }\n    }\n    \n    /// Get statistics about the cache\n    pub fn stats(\u0026self) -\u003e CooldownStats {\n        let now = Utc::now();\n        let mut active = 0;\n        let mut expired = 0;\n        let mut by_response = HashMap::new();\n        \n        for entry in self.entries.values() {\n            if entry.expires_at.map(|exp| exp \u003e now).unwrap_or(true) {\n                active += 1;\n            } else {\n                expired += 1;\n            }\n            \n            *by_response.entry(entry.user_response).or_insert(0) += 1;\n        }\n        \n        CooldownStats {\n            total_entries: self.entries.len(),\n            active_cooldowns: active,\n            expired_pending_cleanup: expired,\n            by_response,\n        }\n    }\n}\n\n/// Result of checking cooldown status\n#[derive(Debug, Clone)]\npub enum CooldownStatus {\n    /// Not on cooldown - safe to suggest\n    NotOnCooldown,\n    /// Was on cooldown but expired\n    Expired,\n    /// Context changed enough to reset cooldown\n    ContextChanged,\n    /// Still on cooldown\n    OnCooldown {\n        since: DateTime\u003cUtc\u003e,\n        response: SuggestionResponse,\n    },\n}\n\nimpl CooldownStatus {\n    /// Should we suggest this skill?\n    pub fn should_suggest(\u0026self) -\u003e bool {\n        !matches!(self, CooldownStatus::OnCooldown { .. })\n    }\n}\n```\n\n### Persistence Layer\n\n```rust\nuse std::path::Path;\nuse serde::{Deserialize, Serialize};\n\n/// Persistent storage for cooldown cache\n#[derive(Debug, Serialize, Deserialize)]\npub struct CooldownCacheStorage {\n    pub version: u32,\n    pub entries: Vec\u003cStoredCooldownEntry\u003e,\n    pub last_updated: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct StoredCooldownEntry {\n    pub skill_id: String,\n    pub suggested_at: DateTime\u003cUtc\u003e,\n    pub fingerprint: u64,\n    pub response: String, // Serialized SuggestionResponse\n    pub expires_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\nimpl SuggestionCooldownCache {\n    /// Load cache from disk\n    pub fn load(path: \u0026Path) -\u003e Result\u003cSelf, CacheError\u003e {\n        if !path.exists() {\n            return Ok(Self::new());\n        }\n        \n        let content = std::fs::read_to_string(path)?;\n        let storage: CooldownCacheStorage = serde_json::from_str(\u0026content)?;\n        \n        let mut cache = Self::new();\n        for entry in storage.entries {\n            cache.entries.insert(\n                entry.skill_id.clone(),\n                CooldownEntry {\n                    skill_ids: vec![entry.skill_id],\n                    suggested_at: entry.suggested_at,\n                    fingerprint: entry.fingerprint,\n                    user_response: serde_json::from_str(\u0026entry.response)?,\n                    expires_at: entry.expires_at,\n                },\n            );\n        }\n        \n        // Cleanup expired on load\n        cache.cleanup_expired();\n        \n        Ok(cache)\n    }\n    \n    /// Save cache to disk\n    pub fn save(\u0026self, path: \u0026Path) -\u003e Result\u003c(), CacheError\u003e {\n        // Ensure directory exists\n        if let Some(parent) = path.parent() {\n            std::fs::create_dir_all(parent)?;\n        }\n        \n        let entries: Vec\u003cStoredCooldownEntry\u003e = self.entries\n            .iter()\n            .filter(|(_, e)| e.expires_at.map(|exp| exp \u003e Utc::now()).unwrap_or(true))\n            .map(|(skill_id, entry)| StoredCooldownEntry {\n                skill_id: skill_id.clone(),\n                suggested_at: entry.suggested_at,\n                fingerprint: entry.fingerprint,\n                response: serde_json::to_string(\u0026entry.user_response).unwrap(),\n                expires_at: entry.expires_at,\n            })\n            .collect();\n        \n        let storage = CooldownCacheStorage {\n            version: 1,\n            entries,\n            last_updated: Utc::now(),\n        };\n        \n        let content = serde_json::to_string_pretty(\u0026storage)?;\n        std::fs::write(path, content)?;\n        \n        Ok(())\n    }\n}\n```\n\n## Integration with Suggestion Engine\n\n```rust\n/// Suggestion engine with cooldown support\npub struct SuggestionEngine {\n    /// Skill matcher for finding relevant skills\n    skill_matcher: SkillMatcher,\n    \n    /// Cooldown cache\n    cooldown_cache: SuggestionCooldownCache,\n    \n    /// Logger for detailed tracing\n    logger: Arc\u003cdyn SuggestionLogger\u003e,\n}\n\nimpl SuggestionEngine {\n    /// Get suggestions, respecting cooldowns\n    pub fn get_suggestions(\n        \u0026self,\n        context: \u0026ContextCapture,\n        max_suggestions: usize,\n    ) -\u003e Vec\u003cSkillSuggestion\u003e {\n        // Compute current fingerprint\n        let fingerprint = ContextFingerprint::capture(context);\n        \n        self.logger.log_fingerprint_computed(\u0026fingerprint);\n        \n        // Get all matching skills\n        let all_matches = self.skill_matcher.find_matches(context);\n        \n        self.logger.log_matches_found(all_matches.len());\n        \n        // Filter by cooldown\n        let mut suggestions = Vec::new();\n        for matched in all_matches {\n            let cooldown_status = self.cooldown_cache.is_on_cooldown(\n                \u0026matched.skill_id,\n                \u0026fingerprint,\n            );\n            \n            self.logger.log_cooldown_check(\u0026matched.skill_id, \u0026cooldown_status);\n            \n            if cooldown_status.should_suggest() {\n                suggestions.push(matched);\n            }\n        }\n        \n        self.logger.log_suggestions_after_filter(suggestions.len());\n        \n        // Take top N\n        suggestions.truncate(max_suggestions);\n        suggestions\n    }\n    \n    /// Record user response to suggestion\n    pub fn record_response(\n        \u0026mut self,\n        skill_id: \u0026SkillId,\n        response: SuggestionResponse,\n        context: \u0026ContextCapture,\n    ) {\n        let fingerprint = ContextFingerprint::capture(context);\n        \n        self.logger.log_response_recorded(skill_id, \u0026response);\n        \n        self.cooldown_cache.record_suggestion(\n            skill_id.clone(),\n            \u0026fingerprint,\n            response,\n        );\n    }\n}\n```\n\n## Configuration\n\n```rust\n/// Configuration for cooldown behavior\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CooldownConfig {\n    /// Maximum entries in cache\n    pub max_entries: usize,\n    \n    /// Default TTL for cooldowns\n    pub default_ttl: Duration,\n    \n    /// Minimum change significance to reset cooldown\n    pub reset_threshold: ChangeSignificance,\n    \n    /// Per-response-type TTL overrides\n    pub response_ttls: HashMap\u003cString, Duration\u003e,\n    \n    /// Whether to persist cooldowns across sessions\n    pub persist: bool,\n    \n    /// Path for persistence file\n    pub persistence_path: Option\u003cPathBuf\u003e,\n}\n\nimpl Default for CooldownConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_entries: 1000,\n            default_ttl: Duration::hours(1),\n            reset_threshold: ChangeSignificance::Moderate,\n            response_ttls: HashMap::new(),\n            persist: true,\n            persistence_path: None,\n        }\n    }\n}\n```\n\n## Tasks\n\n### Task 1: Implement ContextFingerprint\n- [ ] Create `src/context/fingerprint.rs`\n- [ ] Implement hashing for all fingerprint components\n- [ ] Add comparison with significance levels\n- [ ] Write tests for fingerprint stability\n\n### Task 2: Implement ContextCapture\n- [ ] Create `src/context/capture.rs`\n- [ ] Implement git HEAD detection\n- [ ] Implement git diff hashing\n- [ ] Implement open files detection (VS Code, Neovim)\n- [ ] Implement command history capture\n\n### Task 3: Implement SuggestionCooldownCache\n- [ ] Create `src/suggestions/cooldown.rs`\n- [ ] Implement cooldown checking logic\n- [ ] Implement LRU eviction\n- [ ] Implement expiration cleanup\n- [ ] Add cache statistics\n\n### Task 4: Implement Persistence\n- [ ] Create `src/suggestions/cooldown_storage.rs`\n- [ ] Implement JSON serialization\n- [ ] Implement load with migration support\n- [ ] Implement atomic save with backup\n\n### Task 5: Integrate with Suggestion Engine\n- [ ] Modify suggestion engine to check cooldowns\n- [ ] Add response recording API\n- [ ] Wire up configuration loading\n- [ ] Add CLI flags for cooldown bypass\n\n### Task 6: Add CLI Commands\n- [ ] Add `ms suggest --ignore-cooldowns` flag\n- [ ] Add `ms suggest --reset-cooldowns` command\n- [ ] Add `ms cooldown list` to show active cooldowns\n- [ ] Add `ms cooldown clear \u003cskill-id\u003e` command\n\n## Acceptance Criteria\n\n1. **Fingerprint Accuracy**: Fingerprints correctly detect context changes\n2. **Cooldown Enforcement**: Dismissed suggestions don't reappear until context changes\n3. **Expiration**: Cooldowns expire after TTL even without context change\n4. **Persistence**: Cooldowns survive session restarts\n5. **Performance**: Fingerprint computation \u003c 50ms\n6. **LRU Eviction**: Cache doesn't grow unbounded\n7. **Configuration**: All behaviors configurable via config file\n\n## Testing Requirements\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_fingerprint_detects_git_head_change() {\n        let fp1 = ContextFingerprint {\n            repo_root: PathBuf::from(\"/project\"),\n            git_head: Some(\"abc123\".to_string()),\n            diff_hash: 0,\n            open_files_hash: 0,\n            recent_commands_hash: 0,\n        };\n        \n        let fp2 = ContextFingerprint {\n            git_head: Some(\"def456\".to_string()),\n            ..fp1.clone()\n        };\n        \n        assert_eq!(fp1.compare(\u0026fp2), ChangeSignificance::Major);\n    }\n    \n    #[test]\n    fn test_cooldown_respects_fingerprint() {\n        let mut cache = SuggestionCooldownCache::new();\n        let fp = ContextFingerprint { /* ... */ };\n        \n        cache.record_suggestion(\"skill-1\".to_string(), \u0026fp, SuggestionResponse::Dismissed);\n        \n        // Same fingerprint should be on cooldown\n        assert!(!cache.is_on_cooldown(\"skill-1\", \u0026fp).should_suggest());\n        \n        // Different fingerprint should not be on cooldown\n        let fp2 = ContextFingerprint {\n            git_head: Some(\"different\".to_string()),\n            ..fp\n        };\n        assert!(cache.is_on_cooldown(\"skill-1\", \u0026fp2).should_suggest());\n    }\n    \n    #[test]\n    fn test_cooldown_expires() {\n        // Test TTL expiration\n    }\n    \n    #[test]\n    fn test_lru_eviction() {\n        // Test that oldest entries are evicted\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n#[tokio::test]\nasync fn test_suggestion_engine_with_cooldowns() {\n    // Set up engine\n    // Get suggestions\n    // Dismiss one\n    // Get suggestions again - dismissed should be filtered\n}\n\n#[tokio::test]\nasync fn test_cooldown_persistence() {\n    // Create cache, add entries\n    // Save to disk\n    // Load from disk\n    // Verify entries restored\n}\n```\n\n### Logging Requirements\n\n```rust\n// DEBUG level\nlog::debug!(\"Computing context fingerprint...\");\nlog::debug!(\"Git HEAD: {:?}, diff hash: {}\", git_head, diff_hash);\nlog::debug!(\"Checking cooldown for skill {}: {:?}\", skill_id, status);\n\n// INFO level\nlog::info!(\"Context fingerprint changed: {:?}\", significance);\nlog::info!(\"Filtered {} suggestions due to cooldowns\", filtered_count);\n\n// WARN level\nlog::warn!(\"Failed to detect git HEAD: {}\", error);\nlog::warn!(\"Cooldown cache at capacity, evicting oldest entries\");\n\n// ERROR level\nlog::error!(\"Failed to load cooldown cache: {}\", error);\nlog::error!(\"Failed to save cooldown cache: {}\", error);\n```\n\n## Dependencies\n\n- **Depends on**: `meta_skill-o8o` (Context-Aware Suggestions) - Core suggestion infrastructure\n\n## References\n\n- Plan Section 7.2.1: Cooldown rationale\n- Plan Section 7.2: Context-aware suggestion system","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:56:17.759595169-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:56:17.759595169-05:00","labels":["context","cooldowns","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-8df","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:00:22.578870842-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-8gl","title":"Skill Simulation Sandbox (ms simulate)","description":"# Skill Simulation Sandbox (ms simulate)\n\n**Phase 6 - Section 18.10**\n\nSimulate skill end-to-end in a controlled workspace before publishing. This catches broken commands, missing assumptions, and brittle steps that would fail in real usage.\n\n---\n\n## Overview\n\nSkills often contain commands, code snippets, and workflows that make assumptions about the environment. The simulation sandbox:\n\n1. **Creates Isolated Workspace**: Temporary environment with mock files and tools\n2. **Executes Skill Steps**: Runs commands and workflows from the skill\n3. **Captures Output**: Records stdout, stderr, exit codes, and file changes\n4. **Validates Results**: Compares against assertions and expected outcomes\n5. **Generates Reports**: Produces detailed simulation transcript\n\nThis catches issues like:\n- Commands that don't exist or have wrong syntax\n- Missing dependencies or tools\n- Incorrect file paths or permissions\n- Steps that only work on specific platforms\n- Race conditions or timing issues\n\n---\n\n## Core Behavior\n\n### Simulation Workflow\n\n```\n1. Parse skill content to extract executable elements\n2. Create temporary workspace with:\n   - Required directory structure\n   - Mock files (from fixtures or generated)\n   - Mock tools (stubs for dangerous commands)\n3. For each executable element:\n   - Set up isolated environment\n   - Execute in sandbox\n   - Capture all output\n   - Check assertions\n4. Generate simulation report\n5. Clean up workspace\n```\n\n### What Gets Simulated\n\n```rust\n/// Elements that can be simulated from a skill\n#[derive(Debug, Clone)]\npub enum SimulatableElement {\n    /// Shell command from code block\n    Command {\n        command: String,\n        language: String,\n        context: CommandContext,\n    },\n    \n    /// Code snippet that should compile/run\n    CodeSnippet {\n        code: String,\n        language: String,\n        should_compile: bool,\n        should_run: bool,\n    },\n    \n    /// File operations (create, modify, read)\n    FileOperation {\n        operation: FileOp,\n        path: String,\n    },\n    \n    /// Workflow with multiple steps\n    Workflow {\n        name: String,\n        steps: Vec\u003cWorkflowStep\u003e,\n    },\n    \n    /// API call or network request\n    ApiCall {\n        method: String,\n        url: String,\n        expected_response: Option\u003cString\u003e,\n    },\n}\n\n#[derive(Debug, Clone)]\npub struct CommandContext {\n    /// Working directory for command\n    pub cwd: Option\u003cString\u003e,\n    \n    /// Required environment variables\n    pub env: HashMap\u003cString, String\u003e,\n    \n    /// Expected exit code\n    pub expected_exit_code: Option\u003ci32\u003e,\n    \n    /// Expected stdout pattern\n    pub expected_stdout: Option\u003cString\u003e,\n    \n    /// Whether command is destructive\n    pub is_destructive: bool,\n}\n\n#[derive(Debug, Clone)]\npub enum FileOp {\n    Create { content: String },\n    Append { content: String },\n    Read,\n    Delete,\n    Move { to: String },\n}\n\n#[derive(Debug, Clone)]\npub struct WorkflowStep {\n    pub name: String,\n    pub element: SimulatableElement,\n    pub depends_on: Vec\u003cString\u003e,\n}\n```\n\n---\n\n## Core Data Structures\n\n### Simulation Sandbox\n\n```rust\nuse std::collections::HashMap;\nuse std::path::PathBuf;\nuse std::process::Output;\nuse tempfile::TempDir;\n\n/// Isolated sandbox for skill simulation\npub struct SimulationSandbox {\n    /// Temporary workspace directory\n    workspace: TempDir,\n    \n    /// Mock tool registry\n    mock_tools: HashMap\u003cString, MockTool\u003e,\n    \n    /// Environment variables\n    env: HashMap\u003cString, String\u003e,\n    \n    /// File system snapshot before simulation\n    initial_state: FileSystemState,\n    \n    /// Captured outputs\n    outputs: Vec\u003cCapturedOutput\u003e,\n    \n    /// Simulation configuration\n    config: SimulationConfig,\n}\n\n#[derive(Debug, Clone)]\npub struct SimulationConfig {\n    /// Allow network access\n    pub allow_network: bool,\n    \n    /// Allow file system access outside workspace\n    pub allow_external_fs: bool,\n    \n    /// Maximum execution time per command\n    pub command_timeout: Duration,\n    \n    /// Maximum total simulation time\n    pub total_timeout: Duration,\n    \n    /// Commands to mock (replace with stubs)\n    pub mock_commands: Vec\u003cString\u003e,\n    \n    /// Whether to use Docker for isolation\n    pub use_container: bool,\n    \n    /// Resource limits\n    pub limits: ResourceLimits,\n}\n\n#[derive(Debug, Clone)]\npub struct ResourceLimits {\n    pub max_memory_mb: usize,\n    pub max_cpu_percent: usize,\n    pub max_disk_mb: usize,\n    pub max_processes: usize,\n}\n\nimpl Default for SimulationConfig {\n    fn default() -\u003e Self {\n        Self {\n            allow_network: false,\n            allow_external_fs: false,\n            command_timeout: Duration::from_secs(30),\n            total_timeout: Duration::from_secs(300),\n            mock_commands: vec![\n                \"rm -rf /\".to_string(),\n                \"sudo\".to_string(),\n                \"docker\".to_string(),\n            ],\n            use_container: false,\n            limits: ResourceLimits {\n                max_memory_mb: 512,\n                max_cpu_percent: 50,\n                max_disk_mb: 100,\n                max_processes: 10,\n            },\n        }\n    }\n}\n\nimpl SimulationSandbox {\n    /// Create a new simulation sandbox\n    pub fn new(config: SimulationConfig) -\u003e Result\u003cSelf, SimulationError\u003e {\n        let workspace = TempDir::new()?;\n        \n        Ok(Self {\n            workspace,\n            mock_tools: HashMap::new(),\n            env: HashMap::new(),\n            initial_state: FileSystemState::empty(),\n            outputs: Vec::new(),\n            config,\n        })\n    }\n    \n    /// Set up workspace with fixtures\n    pub fn setup_fixtures(\u0026mut self, fixtures_path: \u0026Path) -\u003e Result\u003c(), SimulationError\u003e {\n        // Copy all files from fixtures to workspace\n        self.copy_dir_recursive(fixtures_path, self.workspace.path())?;\n        \n        // Record initial state\n        self.initial_state = self.capture_fs_state()?;\n        \n        Ok(())\n    }\n    \n    /// Add a mock tool\n    pub fn add_mock_tool(\u0026mut self, name: \u0026str, mock: MockTool) {\n        self.mock_tools.insert(name.to_string(), mock);\n    }\n    \n    /// Execute a command in the sandbox\n    pub fn execute_command(\u0026mut self, cmd: \u0026str, context: \u0026CommandContext) -\u003e Result\u003cCommandResult, SimulationError\u003e {\n        // Check if command should be mocked\n        let cmd_name = cmd.split_whitespace().next().unwrap_or(\"\");\n        if let Some(mock) = self.mock_tools.get(cmd_name) {\n            return self.execute_mock(cmd, mock);\n        }\n        \n        // Check if command is in blocked list\n        for blocked in \u0026self.config.mock_commands {\n            if cmd.contains(blocked) {\n                return Err(SimulationError::BlockedCommand(cmd.to_string()));\n            }\n        }\n        \n        // Set up command\n        let working_dir = context.cwd\n            .as_ref()\n            .map(|p| self.workspace.path().join(p))\n            .unwrap_or_else(|| self.workspace.path().to_path_buf());\n        \n        let mut command = std::process::Command::new(\"sh\");\n        command.arg(\"-c\").arg(cmd);\n        command.current_dir(\u0026working_dir);\n        \n        // Set environment\n        command.env_clear();\n        for (k, v) in \u0026self.env {\n            command.env(k, v);\n        }\n        for (k, v) in \u0026context.env {\n            command.env(k, v);\n        }\n        \n        // Restrict PATH if not allowing external access\n        if !self.config.allow_external_fs {\n            let safe_path = \"/usr/bin:/bin:/usr/local/bin\";\n            command.env(\"PATH\", safe_path);\n        }\n        \n        // Execute with timeout\n        let start = std::time::Instant::now();\n        let output = self.execute_with_timeout(\u0026mut command, self.config.command_timeout)?;\n        let duration = start.elapsed();\n        \n        let result = CommandResult {\n            command: cmd.to_string(),\n            exit_code: output.status.code().unwrap_or(-1),\n            stdout: String::from_utf8_lossy(\u0026output.stdout).to_string(),\n            stderr: String::from_utf8_lossy(\u0026output.stderr).to_string(),\n            duration,\n            working_dir: working_dir.to_string_lossy().to_string(),\n        };\n        \n        // Capture output\n        self.outputs.push(CapturedOutput::Command(result.clone()));\n        \n        Ok(result)\n    }\n    \n    fn execute_with_timeout(\n        \u0026self,\n        command: \u0026mut std::process::Command,\n        timeout: Duration,\n    ) -\u003e Result\u003cOutput, SimulationError\u003e {\n        use std::process::Stdio;\n        \n        command.stdout(Stdio::piped());\n        command.stderr(Stdio::piped());\n        \n        let mut child = command.spawn()?;\n        \n        let start = std::time::Instant::now();\n        loop {\n            match child.try_wait()? {\n                Some(_) =\u003e {\n                    return child.wait_with_output().map_err(SimulationError::from);\n                }\n                None =\u003e {\n                    if start.elapsed() \u003e timeout {\n                        child.kill()?;\n                        return Err(SimulationError::Timeout(timeout));\n                    }\n                    std::thread::sleep(Duration::from_millis(100));\n                }\n            }\n        }\n    }\n    \n    fn execute_mock(\u0026self, cmd: \u0026str, mock: \u0026MockTool) -\u003e Result\u003cCommandResult, SimulationError\u003e {\n        Ok(CommandResult {\n            command: cmd.to_string(),\n            exit_code: mock.exit_code,\n            stdout: mock.stdout.clone(),\n            stderr: mock.stderr.clone(),\n            duration: Duration::from_millis(1),\n            working_dir: self.workspace.path().to_string_lossy().to_string(),\n        })\n    }\n    \n    /// Execute a code snippet\n    pub fn execute_code(\u0026mut self, code: \u0026str, language: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let result = match language {\n            \"rust\" =\u003e self.execute_rust_code(code)?,\n            \"python\" =\u003e self.execute_python_code(code)?,\n            \"javascript\" | \"js\" =\u003e self.execute_js_code(code)?,\n            \"bash\" | \"sh\" =\u003e self.execute_bash_code(code)?,\n            _ =\u003e return Err(SimulationError::UnsupportedLanguage(language.to_string())),\n        };\n        \n        self.outputs.push(CapturedOutput::Code(result.clone()));\n        \n        Ok(result)\n    }\n    \n    fn execute_rust_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        // Create a temporary Cargo project\n        let project_dir = self.workspace.path().join(\"rust_sim\");\n        std::fs::create_dir_all(\u0026project_dir)?;\n        \n        // Write Cargo.toml\n        let cargo_toml = r#\"\n[package]\nname = \"simulation\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\"#;\n        std::fs::write(project_dir.join(\"Cargo.toml\"), cargo_toml)?;\n        \n        // Write source\n        let src_dir = project_dir.join(\"src\");\n        std::fs::create_dir_all(\u0026src_dir)?;\n        std::fs::write(src_dir.join(\"main.rs\"), code)?;\n        \n        // Try to compile\n        let compile_result = self.execute_command(\n            \"cargo build\",\n            \u0026CommandContext {\n                cwd: Some(\"rust_sim\".to_string()),\n                env: HashMap::new(),\n                expected_exit_code: Some(0),\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        let compiled = compile_result.exit_code == 0;\n        \n        // Try to run if compiled\n        let (ran, run_output) = if compiled {\n            let run_result = self.execute_command(\n                \"cargo run\",\n                \u0026CommandContext {\n                    cwd: Some(\"rust_sim\".to_string()),\n                    env: HashMap::new(),\n                    expected_exit_code: None,\n                    expected_stdout: None,\n                    is_destructive: false,\n                },\n            )?;\n            (run_result.exit_code == 0, Some(run_result))\n        } else {\n            (false, None)\n        };\n        \n        Ok(CodeResult {\n            language: \"rust\".to_string(),\n            code: code.to_string(),\n            compiled,\n            compile_output: Some(compile_result.stderr),\n            ran,\n            run_output: run_output.map(|r| r.stdout),\n            exit_code: run_output.map(|r| r.exit_code),\n        })\n    }\n    \n    fn execute_python_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let script_path = self.workspace.path().join(\"script.py\");\n        std::fs::write(\u0026script_path, code)?;\n        \n        let result = self.execute_command(\n            \"python3 script.py\",\n            \u0026CommandContext {\n                cwd: None,\n                env: HashMap::new(),\n                expected_exit_code: None,\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        Ok(CodeResult {\n            language: \"python\".to_string(),\n            code: code.to_string(),\n            compiled: true, // Python is interpreted\n            compile_output: None,\n            ran: result.exit_code == 0,\n            run_output: Some(result.stdout),\n            exit_code: Some(result.exit_code),\n        })\n    }\n    \n    fn execute_js_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let script_path = self.workspace.path().join(\"script.js\");\n        std::fs::write(\u0026script_path, code)?;\n        \n        let result = self.execute_command(\n            \"node script.js\",\n            \u0026CommandContext {\n                cwd: None,\n                env: HashMap::new(),\n                expected_exit_code: None,\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        Ok(CodeResult {\n            language: \"javascript\".to_string(),\n            code: code.to_string(),\n            compiled: true,\n            compile_output: None,\n            ran: result.exit_code == 0,\n            run_output: Some(result.stdout),\n            exit_code: Some(result.exit_code),\n        })\n    }\n    \n    fn execute_bash_code(\u0026mut self, code: \u0026str) -\u003e Result\u003cCodeResult, SimulationError\u003e {\n        let script_path = self.workspace.path().join(\"script.sh\");\n        std::fs::write(\u0026script_path, code)?;\n        \n        let result = self.execute_command(\n            \"bash script.sh\",\n            \u0026CommandContext {\n                cwd: None,\n                env: HashMap::new(),\n                expected_exit_code: None,\n                expected_stdout: None,\n                is_destructive: false,\n            },\n        )?;\n        \n        Ok(CodeResult {\n            language: \"bash\".to_string(),\n            code: code.to_string(),\n            compiled: true,\n            compile_output: None,\n            ran: result.exit_code == 0,\n            run_output: Some(result.stdout),\n            exit_code: Some(result.exit_code),\n        })\n    }\n    \n    /// Capture file system changes\n    pub fn get_fs_changes(\u0026self) -\u003e Result\u003cFileSystemChanges, SimulationError\u003e {\n        let current_state = self.capture_fs_state()?;\n        Ok(self.initial_state.diff(\u0026current_state))\n    }\n    \n    fn capture_fs_state(\u0026self) -\u003e Result\u003cFileSystemState, SimulationError\u003e {\n        let mut state = FileSystemState::empty();\n        self.walk_dir(self.workspace.path(), \u0026mut state)?;\n        Ok(state)\n    }\n    \n    fn walk_dir(\u0026self, path: \u0026Path, state: \u0026mut FileSystemState) -\u003e Result\u003c(), SimulationError\u003e {\n        for entry in std::fs::read_dir(path)? {\n            let entry = entry?;\n            let path = entry.path();\n            \n            if path.is_dir() {\n                self.walk_dir(\u0026path, state)?;\n            } else {\n                let relative = path.strip_prefix(self.workspace.path())\n                    .unwrap_or(\u0026path)\n                    .to_string_lossy()\n                    .to_string();\n                let content = std::fs::read_to_string(\u0026path).unwrap_or_default();\n                let hash = Self::hash_content(\u0026content);\n                \n                state.files.insert(relative, FileInfo {\n                    hash,\n                    size: content.len(),\n                });\n            }\n        }\n        Ok(())\n    }\n    \n    fn hash_content(content: \u0026str) -\u003e String {\n        use sha2::{Sha256, Digest};\n        let mut hasher = Sha256::new();\n        hasher.update(content.as_bytes());\n        format!(\"{:x}\", hasher.finalize())[..16].to_string()\n    }\n    \n    fn copy_dir_recursive(\u0026self, src: \u0026Path, dst: \u0026Path) -\u003e Result\u003c(), SimulationError\u003e {\n        std::fs::create_dir_all(dst)?;\n        \n        for entry in std::fs::read_dir(src)? {\n            let entry = entry?;\n            let src_path = entry.path();\n            let dst_path = dst.join(entry.file_name());\n            \n            if src_path.is_dir() {\n                self.copy_dir_recursive(\u0026src_path, \u0026dst_path)?;\n            } else {\n                std::fs::copy(\u0026src_path, \u0026dst_path)?;\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n/// Mock tool for dangerous commands\n#[derive(Debug, Clone)]\npub struct MockTool {\n    pub exit_code: i32,\n    pub stdout: String,\n    pub stderr: String,\n}\n\n/// Command execution result\n#[derive(Debug, Clone)]\npub struct CommandResult {\n    pub command: String,\n    pub exit_code: i32,\n    pub stdout: String,\n    pub stderr: String,\n    pub duration: Duration,\n    pub working_dir: String,\n}\n\n/// Code execution result\n#[derive(Debug, Clone)]\npub struct CodeResult {\n    pub language: String,\n    pub code: String,\n    pub compiled: bool,\n    pub compile_output: Option\u003cString\u003e,\n    pub ran: bool,\n    pub run_output: Option\u003cString\u003e,\n    pub exit_code: Option\u003ci32\u003e,\n}\n\n/// Captured output during simulation\n#[derive(Debug, Clone)]\npub enum CapturedOutput {\n    Command(CommandResult),\n    Code(CodeResult),\n    FileChange(FileChange),\n}\n\n/// File system state snapshot\n#[derive(Debug, Clone)]\npub struct FileSystemState {\n    pub files: HashMap\u003cString, FileInfo\u003e,\n}\n\nimpl FileSystemState {\n    pub fn empty() -\u003e Self {\n        Self { files: HashMap::new() }\n    }\n    \n    pub fn diff(\u0026self, other: \u0026FileSystemState) -\u003e FileSystemChanges {\n        let mut created = Vec::new();\n        let mut modified = Vec::new();\n        let mut deleted = Vec::new();\n        \n        // Find created and modified\n        for (path, info) in \u0026other.files {\n            match self.files.get(path) {\n                None =\u003e created.push(path.clone()),\n                Some(old_info) if old_info.hash != info.hash =\u003e modified.push(path.clone()),\n                _ =\u003e {}\n            }\n        }\n        \n        // Find deleted\n        for path in self.files.keys() {\n            if !other.files.contains_key(path) {\n                deleted.push(path.clone());\n            }\n        }\n        \n        FileSystemChanges { created, modified, deleted }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FileInfo {\n    pub hash: String,\n    pub size: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct FileSystemChanges {\n    pub created: Vec\u003cString\u003e,\n    pub modified: Vec\u003cString\u003e,\n    pub deleted: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone)]\npub struct FileChange {\n    pub path: String,\n    pub change_type: FileChangeType,\n}\n\n#[derive(Debug, Clone)]\npub enum FileChangeType {\n    Created,\n    Modified,\n    Deleted,\n}\n```\n\n---\n\n## Skill Content Parser\n\n```rust\n/// Parses skill content to extract simulatable elements\npub struct SkillContentParser {\n    /// Regex patterns for different content types\n    command_pattern: regex::Regex,\n    code_block_pattern: regex::Regex,\n}\n\nimpl SkillContentParser {\n    pub fn new() -\u003e Self {\n        Self {\n            command_pattern: regex::Regex::new(r\"```(?:bash|sh|shell)\\n([\\s\\S]*?)```\").unwrap(),\n            code_block_pattern: regex::Regex::new(r\"```(\\w+)\\n([\\s\\S]*?)```\").unwrap(),\n        }\n    }\n    \n    /// Extract all simulatable elements from skill content\n    pub fn parse(\u0026self, skill: \u0026Skill) -\u003e Vec\u003cSimulatableElement\u003e {\n        let mut elements = Vec::new();\n        \n        for (section_name, section) in \u0026skill.sections {\n            // Extract commands\n            for cap in self.command_pattern.captures_iter(\u0026section.content) {\n                let command = cap.get(1).unwrap().as_str().trim();\n                for line in command.lines() {\n                    let line = line.trim();\n                    if line.starts_with('$') || line.starts_with('#') {\n                        continue; // Skip prompts and comments\n                    }\n                    if !line.is_empty() {\n                        elements.push(SimulatableElement::Command {\n                            command: line.to_string(),\n                            language: \"bash\".to_string(),\n                            context: CommandContext {\n                                cwd: None,\n                                env: HashMap::new(),\n                                expected_exit_code: Some(0),\n                                expected_stdout: None,\n                                is_destructive: self.is_destructive(line),\n                            },\n                        });\n                    }\n                }\n            }\n            \n            // Extract code snippets\n            for cap in self.code_block_pattern.captures_iter(\u0026section.content) {\n                let language = cap.get(1).unwrap().as_str();\n                let code = cap.get(2).unwrap().as_str().trim();\n                \n                // Skip if this is a command block (already processed)\n                if language == \"bash\" || language == \"sh\" || language == \"shell\" {\n                    continue;\n                }\n                \n                elements.push(SimulatableElement::CodeSnippet {\n                    code: code.to_string(),\n                    language: language.to_string(),\n                    should_compile: self.should_compile(language),\n                    should_run: self.should_run(language, code),\n                });\n            }\n        }\n        \n        elements\n    }\n    \n    fn is_destructive(\u0026self, cmd: \u0026str) -\u003e bool {\n        let destructive_patterns = [\n            \"rm -rf\", \"rm -r\", \"rmdir\",\n            \"dd if=\", \"mkfs\",\n            \"\u003e /dev/\", \"| sudo\",\n        ];\n        \n        destructive_patterns.iter().any(|p| cmd.contains(p))\n    }\n    \n    fn should_compile(\u0026self, language: \u0026str) -\u003e bool {\n        matches!(language, \"rust\" | \"go\" | \"c\" | \"cpp\" | \"java\" | \"typescript\")\n    }\n    \n    fn should_run(\u0026self, language: \u0026str, code: \u0026str) -\u003e bool {\n        // Check if code has a main function or is a script\n        match language {\n            \"rust\" =\u003e code.contains(\"fn main()\"),\n            \"python\" =\u003e !code.contains(\"def \") || code.contains(\"if __name__\"),\n            \"javascript\" | \"js\" =\u003e true,\n            \"go\" =\u003e code.contains(\"func main()\"),\n            _ =\u003e false,\n        }\n    }\n}\n```\n\n---\n\n## Simulation Report\n\n```rust\n/// Complete simulation report\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SimulationReport {\n    /// Skill that was simulated\n    pub skill_id: String,\n    \n    /// When simulation started\n    pub started_at: DateTime\u003cUtc\u003e,\n    \n    /// Total simulation duration\n    pub duration: Duration,\n    \n    /// Overall result\n    pub result: SimulationResult,\n    \n    /// Individual element results\n    pub element_results: Vec\u003cElementResult\u003e,\n    \n    /// File system changes\n    pub fs_changes: FileSystemChanges,\n    \n    /// Issues found\n    pub issues: Vec\u003cSimulationIssue\u003e,\n    \n    /// Warnings\n    pub warnings: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum SimulationResult {\n    /// All elements simulated successfully\n    Success,\n    \n    /// Some elements failed\n    PartialSuccess { passed: usize, failed: usize },\n    \n    /// Critical failure\n    Failure { reason: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ElementResult {\n    /// Element that was simulated\n    pub element: String,\n    \n    /// Whether it succeeded\n    pub success: bool,\n    \n    /// Captured output\n    pub output: Option\u003cString\u003e,\n    \n    /// Error message if failed\n    pub error: Option\u003cString\u003e,\n    \n    /// Duration\n    pub duration: Duration,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SimulationIssue {\n    /// Issue severity\n    pub severity: IssueSeverity,\n    \n    /// Element that caused the issue\n    pub element: String,\n    \n    /// Issue description\n    pub description: String,\n    \n    /// Suggested fix\n    pub suggestion: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum IssueSeverity {\n    Error,\n    Warning,\n    Info,\n}\n\nimpl SimulationReport {\n    /// Generate human-readable report\n    pub fn to_text(\u0026self) -\u003e String {\n        let mut output = String::new();\n        \n        output.push_str(\u0026format!(\"Simulation Report: {}\\n\", self.skill_id));\n        output.push_str(\u0026\"=\".repeat(50));\n        output.push_str(\"\\n\\n\");\n        \n        // Summary\n        output.push_str(\u0026format!(\"Result: {:?}\\n\", self.result));\n        output.push_str(\u0026format!(\"Duration: {:.2}s\\n\\n\", self.duration.as_secs_f64()));\n        \n        // Element results\n        output.push_str(\"Element Results:\\n\");\n        output.push_str(\u0026\"-\".repeat(40));\n        output.push('\\n');\n        \n        for result in \u0026self.element_results {\n            let status = if result.success { \"[PASS]\" } else { \"[FAIL]\" };\n            output.push_str(\u0026format!(\"{} {} ({:.2}s)\\n\", status, result.element, result.duration.as_secs_f64()));\n            \n            if let Some(error) = \u0026result.error {\n                output.push_str(\u0026format!(\"       Error: {}\\n\", error));\n            }\n        }\n        \n        // Issues\n        if !self.issues.is_empty() {\n            output.push_str(\"\\nIssues Found:\\n\");\n            output.push_str(\u0026\"-\".repeat(40));\n            output.push('\\n');\n            \n            for issue in \u0026self.issues {\n                let severity = match issue.severity {\n                    IssueSeverity::Error =\u003e \"ERROR\",\n                    IssueSeverity::Warning =\u003e \"WARN\",\n                    IssueSeverity::Info =\u003e \"INFO\",\n                };\n                output.push_str(\u0026format!(\"[{}] {}: {}\\n\", severity, issue.element, issue.description));\n                if let Some(suggestion) = \u0026issue.suggestion {\n                    output.push_str(\u0026format!(\"       Suggestion: {}\\n\", suggestion));\n                }\n            }\n        }\n        \n        // File system changes\n        if !self.fs_changes.created.is_empty() || !self.fs_changes.modified.is_empty() {\n            output.push_str(\"\\nFile System Changes:\\n\");\n            output.push_str(\u0026\"-\".repeat(40));\n            output.push('\\n');\n            \n            for path in \u0026self.fs_changes.created {\n                output.push_str(\u0026format!(\"  + {}\\n\", path));\n            }\n            for path in \u0026self.fs_changes.modified {\n                output.push_str(\u0026format!(\"  ~ {}\\n\", path));\n            }\n            for path in \u0026self.fs_changes.deleted {\n                output.push_str(\u0026format!(\"  - {}\\n\", path));\n            }\n        }\n        \n        output\n    }\n    \n    /// Generate JSON transcript\n    pub fn to_json(\u0026self) -\u003e Result\u003cString, serde_json::Error\u003e {\n        serde_json::to_string_pretty(self)\n    }\n}\n```\n\n---\n\n## Simulation Engine\n\n```rust\n/// Main simulation engine\npub struct SimulationEngine {\n    /// Skill registry\n    registry: SkillRegistry,\n    \n    /// Content parser\n    parser: SkillContentParser,\n    \n    /// Default configuration\n    default_config: SimulationConfig,\n}\n\nimpl SimulationEngine {\n    pub fn new(registry: SkillRegistry) -\u003e Self {\n        Self {\n            registry,\n            parser: SkillContentParser::new(),\n            default_config: SimulationConfig::default(),\n        }\n    }\n    \n    /// Simulate a skill\n    pub fn simulate(\n        \u0026self,\n        skill_id: \u0026str,\n        fixtures_path: Option\u003c\u0026Path\u003e,\n        config: Option\u003cSimulationConfig\u003e,\n    ) -\u003e Result\u003cSimulationReport, SimulationError\u003e {\n        let config = config.unwrap_or_else(|| self.default_config.clone());\n        let skill = self.registry.get(\u0026SkillId(skill_id.to_string()))?;\n        \n        let started_at = Utc::now();\n        let start_time = std::time::Instant::now();\n        \n        // Create sandbox\n        let mut sandbox = SimulationSandbox::new(config)?;\n        \n        // Set up fixtures if provided\n        if let Some(fixtures) = fixtures_path {\n            sandbox.setup_fixtures(fixtures)?;\n        }\n        \n        // Parse skill content\n        let elements = self.parser.parse(\u0026skill);\n        \n        // Simulate each element\n        let mut element_results = Vec::new();\n        let mut issues = Vec::new();\n        \n        for element in elements {\n            let result = self.simulate_element(\u0026mut sandbox, \u0026element);\n            \n            match result {\n                Ok(elem_result) =\u003e {\n                    if !elem_result.success {\n                        issues.push(SimulationIssue {\n                            severity: IssueSeverity::Error,\n                            element: elem_result.element.clone(),\n                            description: elem_result.error.clone().unwrap_or_default(),\n                            suggestion: self.suggest_fix(\u0026element, \u0026elem_result),\n                        });\n                    }\n                    element_results.push(elem_result);\n                }\n                Err(e) =\u003e {\n                    element_results.push(ElementResult {\n                        element: format!(\"{:?}\", element),\n                        success: false,\n                        output: None,\n                        error: Some(e.to_string()),\n                        duration: Duration::ZERO,\n                    });\n                    issues.push(SimulationIssue {\n                        severity: IssueSeverity::Error,\n                        element: format!(\"{:?}\", element),\n                        description: e.to_string(),\n                        suggestion: None,\n                    });\n                }\n            }\n            \n            // Check total timeout\n            if start_time.elapsed() \u003e self.default_config.total_timeout {\n                issues.push(SimulationIssue {\n                    severity: IssueSeverity::Error,\n                    element: \"overall\".to_string(),\n                    description: \"Simulation timeout exceeded\".to_string(),\n                    suggestion: Some(\"Reduce number of elements or increase timeout\".to_string()),\n                });\n                break;\n            }\n        }\n        \n        // Get file system changes\n        let fs_changes = sandbox.get_fs_changes()?;\n        \n        // Determine overall result\n        let passed = element_results.iter().filter(|r| r.success).count();\n        let failed = element_results.iter().filter(|r| !r.success).count();\n        \n        let result = if failed == 0 {\n            SimulationResult::Success\n        } else if passed \u003e 0 {\n            SimulationResult::PartialSuccess { passed, failed }\n        } else {\n            SimulationResult::Failure { reason: \"All elements failed\".to_string() }\n        };\n        \n        Ok(SimulationReport {\n            skill_id: skill_id.to_string(),\n            started_at,\n            duration: start_time.elapsed(),\n            result,\n            element_results,\n            fs_changes,\n            issues,\n            warnings: Vec::new(),\n        })\n    }\n    \n    fn simulate_element(\n        \u0026self,\n        sandbox: \u0026mut SimulationSandbox,\n        element: \u0026SimulatableElement,\n    ) -\u003e Result\u003cElementResult, SimulationError\u003e {\n        let start = std::time::Instant::now();\n        \n        match element {\n            SimulatableElement::Command { command, context, .. } =\u003e {\n                let result = sandbox.execute_command(command, context)?;\n                \n                let success = result.exit_code == context.expected_exit_code.unwrap_or(0);\n                \n                Ok(ElementResult {\n                    element: format!(\"Command: {}\", command),\n                    success,\n                    output: Some(result.stdout),\n                    error: if success { None } else { Some(result.stderr) },\n                    duration: start.elapsed(),\n                })\n            }\n            \n            SimulatableElement::CodeSnippet { code, language, should_compile, should_run } =\u003e {\n                let result = sandbox.execute_code(code, language)?;\n                \n                let success = (!*should_compile || result.compiled) \n                    \u0026\u0026 (!*should_run || result.ran);\n                \n                Ok(ElementResult {\n                    element: format!(\"Code ({}): {}...\", language, \u0026code[..50.min(code.len())]),\n                    success,\n                    output: result.run_output.or(result.compile_output),\n                    error: if success { None } else { \n                        Some(result.compile_output.unwrap_or_else(|| \"Execution failed\".to_string()))\n                    },\n                    duration: start.elapsed(),\n                })\n            }\n            \n            SimulatableElement::FileOperation { operation, path } =\u003e {\n                // File operations are validated but not executed\n                // (they're handled by the sandbox automatically)\n                Ok(ElementResult {\n                    element: format!(\"File: {:?} on {}\", operation, path),\n                    success: true,\n                    output: None,\n                    error: None,\n                    duration: start.elapsed(),\n                })\n            }\n            \n            _ =\u003e {\n                Ok(ElementResult {\n                    element: format!(\"{:?}\", element),\n                    success: true,\n                    output: Some(\"Skipped (not implemented)\".to_string()),\n                    error: None,\n                    duration: start.elapsed(),\n                })\n            }\n        }\n    }\n    \n    fn suggest_fix(\u0026self, element: \u0026SimulatableElement, result: \u0026ElementResult) -\u003e Option\u003cString\u003e {\n        if result.success {\n            return None;\n        }\n        \n        match element {\n            SimulatableElement::Command { command, .. } =\u003e {\n                if result.error.as_ref().map(|e| e.contains(\"not found\")).unwrap_or(false) {\n                    Some(format!(\"Command '{}' not found. Add to prerequisites or use full path.\", \n                        command.split_whitespace().next().unwrap_or(command)))\n                } else if result.error.as_ref().map(|e| e.contains(\"permission denied\")).unwrap_or(false) {\n                    Some(\"Permission denied. Avoid commands requiring elevated privileges.\".to_string())\n                } else {\n                    None\n                }\n            }\n            SimulatableElement::CodeSnippet { language, .. } =\u003e {\n                if result.error.as_ref().map(|e| e.contains(\"error[E\")).unwrap_or(false) {\n                    Some(\"Rust compilation error. Ensure code snippet is complete and correct.\".to_string())\n                } else {\n                    Some(format!(\"Ensure {} is installed and code is syntactically correct.\", language))\n                }\n            }\n            _ =\u003e None,\n        }\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms simulate \u003cskill\u003e`\n\n```\nSimulate a skill in a controlled environment\n\nUSAGE:\n    ms simulate \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name to simulate\n\nOPTIONS:\n    --with-fixtures \u003cDIR\u003e    Use fixtures from directory\n    --config \u003cFILE\u003e          Use custom simulation config\n    --timeout \u003cSECS\u003e         Total simulation timeout [default: 300]\n    --allow-network          Allow network access during simulation\n    --verbose                Show detailed output\n    --record-transcript      Save detailed transcript\n\nOUTPUT EXAMPLE:\n    Simulating skill: rust-error-handling\n    \n    Setting up sandbox...\n    Using fixtures from: ./fixtures/\n    \n    Simulating elements:\n      [PASS] Command: cargo new example_project (0.23s)\n      [PASS] Command: cargo build (1.45s)\n      [PASS] Code (rust): fn main() { ... } (2.12s)\n      [FAIL] Command: cargo clippy (0.89s)\n             Error: clippy not installed\n             Suggestion: Add clippy to prerequisites\n      [PASS] Code (rust): use std::error::Error... (1.87s)\n    \n    File System Changes:\n      + example_project/\n      + example_project/Cargo.toml\n      + example_project/src/main.rs\n    \n    Result: PartialSuccess (4 passed, 1 failed)\n    Duration: 6.56s\n    \n    Issues Found:\n      [ERROR] Command: cargo clippy\n              clippy not installed\n              Suggestion: Add clippy to prerequisites\n```\n\n### `ms simulate --with-fixtures`\n\n```\nRun simulation with fixture files\n\nUSAGE:\n    ms simulate \u003cSKILL\u003e --with-fixtures \u003cDIR\u003e\n\nThe fixtures directory should contain files that will be copied\nto the simulation workspace before execution.\n\nEXAMPLE STRUCTURE:\n    fixtures/\n     Cargo.toml          # Project manifest\n     src/\n        main.rs         # Sample source file\n     test_data/\n         input.json      # Test data\n\nEXAMPLE:\n    ms simulate rust-error-handling --with-fixtures ./fixtures/\n```\n\n### `ms simulate --record-transcript`\n\n```\nRecord detailed simulation transcript\n\nUSAGE:\n    ms simulate \u003cSKILL\u003e --record-transcript [OPTIONS]\n\nOPTIONS:\n    --output \u003cFILE\u003e     Output file [default: simulation-transcript.json]\n    --format \u003cFMT\u003e      Format: json, yaml, markdown [default: json]\n\nThe transcript includes:\n- All executed commands and their output\n- All code executions and results\n- File system changes\n- Timing information\n- Environment state\n\nEXAMPLE:\n    ms simulate rust-error-handling --record-transcript --output report.json\n    \n    # View as markdown\n    ms simulate rust-error-handling --record-transcript --format markdown \u003e report.md\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum SimulationError {\n    #[error(\"Skill not found: {0}\")]\n    SkillNotFound(String),\n    \n    #[error(\"Command blocked: {0}\")]\n    BlockedCommand(String),\n    \n    #[error(\"Execution timeout after {0:?}\")]\n    Timeout(Duration),\n    \n    #[error(\"Unsupported language: {0}\")]\n    UnsupportedLanguage(String),\n    \n    #[error(\"Sandbox creation failed: {0}\")]\n    SandboxError(String),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Parse error: {0}\")]\n    ParseError(String),\n}\n```\n\n---\n\n## Configuration File\n\nSimulation configuration in `~/.config/meta_skill/simulation.toml`:\n\n```toml\n[sandbox]\nallow_network = false\nallow_external_fs = false\ncommand_timeout_secs = 30\ntotal_timeout_secs = 300\nuse_container = false\n\n[limits]\nmax_memory_mb = 512\nmax_cpu_percent = 50\nmax_disk_mb = 100\nmax_processes = 10\n\n[mock_commands]\n# Commands to mock (replace with stubs)\nblocked = [\n    \"rm -rf /\",\n    \"sudo\",\n    \"docker run\",\n    \"kubectl delete\",\n]\n\n# Mock responses for specific commands\n[mock_responses]\n\"git --version\" = { exit_code = 0, stdout = \"git version 2.40.0\" }\n\"docker --version\" = { exit_code = 0, stdout = \"Docker version 24.0.0\" }\n```\n\n---\n\n## Dependencies\n\n- **Skill Tests (ms test)** (meta_skill-x7k): Test infrastructure this builds upon\n- `tempfile`: Temporary workspace management\n- `regex`: Content parsing\n- `sha2`: File content hashing\n- `serde`, `serde_json`, `serde_yaml`: Configuration and report serialization\n- `chrono`: Timestamps","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T23:04:03.74720972-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:07.081061577-05:00","labels":["phase-6","sandbox","simulation","validation"],"dependencies":[{"issue_id":"meta_skill-8gl","depends_on_id":"meta_skill-x7k","type":"blocks","created_at":"2026-01-13T23:04:16.53606976-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-8ti","title":"Cross-Project Learning","description":"# Cross-Project Learning\n\n**Phase 6 - Section 23**\n\nLearn from sessions across multiple projects to build comprehensive skills. This feature enables coverage gap analysis, universal pattern extraction, and knowledge synthesis across diverse codebases.\n\n---\n\n## Overview\n\nSkills become more valuable when they incorporate learnings from multiple projects. A single project may not exercise all aspects of a topic, but patterns observed across many projects reveal universal best practices. Cross-project learning:\n\n1. **Aggregates Sessions**: Collect CASS sessions from multiple projects\n2. **Finds Coverage Gaps**: Identify topics with insufficient skill coverage\n3. **Extracts Universal Patterns**: Find patterns that recur across projects\n4. **Builds Knowledge Graphs**: Connect related concepts across domains\n\n---\n\n## Core Data Structures\n\n### Cross-Project Analyzer\n\n```rust\nuse std::collections::{HashMap, HashSet};\nuse std::path::PathBuf;\n\n/// Analyzes patterns across multiple projects\npub struct CrossProjectAnalyzer {\n    /// CASS client for session access\n    cass: CassClient,\n    \n    /// Registered projects\n    projects: Vec\u003cProjectInfo\u003e,\n    \n    /// Pattern extractor\n    pattern_extractor: PatternExtractor,\n    \n    /// Knowledge graph builder\n    graph_builder: KnowledgeGraphBuilder,\n}\n\n/// Information about a project for cross-project analysis\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProjectInfo {\n    /// Unique project identifier\n    pub id: String,\n    \n    /// Human-readable project name\n    pub name: String,\n    \n    /// Path to project root\n    pub path: PathBuf,\n    \n    /// Path to CASS database\n    pub cass_path: PathBuf,\n    \n    /// Project metadata\n    pub metadata: ProjectMetadata,\n    \n    /// When project was registered\n    pub registered_at: DateTime\u003cUtc\u003e,\n    \n    /// Last analysis timestamp\n    pub last_analyzed: Option\u003cDateTime\u003cUtc\u003e\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProjectMetadata {\n    /// Primary language(s)\n    pub languages: Vec\u003cString\u003e,\n    \n    /// Frameworks/libraries used\n    pub frameworks: Vec\u003cString\u003e,\n    \n    /// Project type (web, cli, library, etc.)\n    pub project_type: ProjectType,\n    \n    /// Size estimate\n    pub size_estimate: ProjectSize,\n    \n    /// Custom tags\n    pub tags: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ProjectType {\n    WebBackend,\n    WebFrontend,\n    FullStack,\n    Cli,\n    Library,\n    MobileApp,\n    DataPipeline,\n    Infrastructure,\n    Other(String),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ProjectSize {\n    Small,      // \u003c 10k lines\n    Medium,     // 10k - 100k lines\n    Large,      // 100k - 1M lines\n    VeryLarge,  // \u003e 1M lines\n}\n\nimpl CrossProjectAnalyzer {\n    pub fn new(cass: CassClient) -\u003e Self {\n        Self {\n            cass,\n            projects: Vec::new(),\n            pattern_extractor: PatternExtractor::new(),\n            graph_builder: KnowledgeGraphBuilder::new(),\n        }\n    }\n    \n    /// Register a project for cross-project analysis\n    pub fn register_project(\u0026mut self, project: ProjectInfo) -\u003e Result\u003c(), AnalyzerError\u003e {\n        // Validate CASS database exists\n        if !project.cass_path.exists() {\n            return Err(AnalyzerError::CassNotFound(project.cass_path.clone()));\n        }\n        \n        // Check for duplicates\n        if self.projects.iter().any(|p| p.id == project.id) {\n            return Err(AnalyzerError::DuplicateProject(project.id));\n        }\n        \n        self.projects.push(project);\n        Ok(())\n    }\n    \n    /// Analyze all registered projects\n    pub fn analyze_all(\u0026mut self) -\u003e Result\u003cCrossProjectReport, AnalyzerError\u003e {\n        let mut report = CrossProjectReport::new();\n        \n        for project in \u0026self.projects {\n            let project_analysis = self.analyze_project(project)?;\n            report.add_project_analysis(project.id.clone(), project_analysis);\n        }\n        \n        // Find cross-project patterns\n        report.universal_patterns = self.find_universal_patterns(\u0026report.project_analyses)?;\n        \n        // Build knowledge graph\n        report.knowledge_graph = self.graph_builder.build(\u0026report)?;\n        \n        // Identify coverage gaps\n        report.coverage_gaps = self.identify_coverage_gaps(\u0026report)?;\n        \n        Ok(report)\n    }\n    \n    /// Analyze a single project\n    fn analyze_project(\u0026self, project: \u0026ProjectInfo) -\u003e Result\u003cProjectAnalysis, AnalyzerError\u003e {\n        // Connect to project's CASS database\n        let cass = CassClient::connect(\u0026project.cass_path)?;\n        \n        // Get all sessions\n        let sessions = cass.list_sessions()?;\n        \n        let mut analysis = ProjectAnalysis {\n            project_id: project.id.clone(),\n            session_count: sessions.len(),\n            patterns: Vec::new(),\n            topics: Vec::new(),\n            tool_usage: HashMap::new(),\n            error_types: HashMap::new(),\n        };\n        \n        // Extract patterns from each session\n        for session in sessions {\n            let session_patterns = self.pattern_extractor.extract(\u0026session)?;\n            analysis.patterns.extend(session_patterns);\n            \n            // Track topics discussed\n            for topic in self.extract_topics(\u0026session) {\n                if !analysis.topics.contains(\u0026topic) {\n                    analysis.topics.push(topic);\n                }\n            }\n            \n            // Track tool usage\n            for tool in \u0026session.tools_used {\n                *analysis.tool_usage.entry(tool.clone()).or_insert(0) += 1;\n            }\n            \n            // Track error types encountered\n            for error in \u0026session.errors {\n                let error_type = self.categorize_error(error);\n                *analysis.error_types.entry(error_type).or_insert(0) += 1;\n            }\n        }\n        \n        Ok(analysis)\n    }\n    \n    /// Find patterns that appear across multiple projects\n    fn find_universal_patterns(\n        \u0026self,\n        analyses: \u0026HashMap\u003cString, ProjectAnalysis\u003e,\n    ) -\u003e Result\u003cVec\u003cUniversalPattern\u003e, AnalyzerError\u003e {\n        let mut pattern_occurrences: HashMap\u003cPatternSignature, Vec\u003c(String, ExtractedPattern)\u003e\u003e = HashMap::new();\n        \n        // Group patterns by signature\n        for (project_id, analysis) in analyses {\n            for pattern in \u0026analysis.patterns {\n                let signature = pattern.signature();\n                pattern_occurrences\n                    .entry(signature)\n                    .or_default()\n                    .push((project_id.clone(), pattern.clone()));\n            }\n        }\n        \n        // Filter to patterns appearing in multiple projects\n        let min_projects = 2;\n        let universal: Vec\u003cUniversalPattern\u003e = pattern_occurrences\n            .into_iter()\n            .filter(|(_, occurrences)| {\n                let unique_projects: HashSet\u003c_\u003e = occurrences.iter().map(|(p, _)| p).collect();\n                unique_projects.len() \u003e= min_projects\n            })\n            .map(|(signature, occurrences)| {\n                let projects: Vec\u003c_\u003e = occurrences.iter().map(|(p, _)| p.clone()).collect();\n                let examples: Vec\u003c_\u003e = occurrences.into_iter().map(|(_, p)| p).collect();\n                \n                UniversalPattern {\n                    signature,\n                    projects,\n                    occurrence_count: examples.len(),\n                    examples,\n                    confidence: self.calculate_pattern_confidence(\u0026signature, \u0026examples),\n                }\n            })\n            .collect();\n        \n        Ok(universal)\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProjectAnalysis {\n    pub project_id: String,\n    pub session_count: usize,\n    pub patterns: Vec\u003cExtractedPattern\u003e,\n    pub topics: Vec\u003cTopic\u003e,\n    pub tool_usage: HashMap\u003cString, u32\u003e,\n    pub error_types: HashMap\u003cErrorCategory, u32\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UniversalPattern {\n    /// Pattern signature (for deduplication)\n    pub signature: PatternSignature,\n    \n    /// Projects where this pattern was observed\n    pub projects: Vec\u003cString\u003e,\n    \n    /// Total occurrences across all projects\n    pub occurrence_count: usize,\n    \n    /// Example instances\n    pub examples: Vec\u003cExtractedPattern\u003e,\n    \n    /// Confidence in pattern universality\n    pub confidence: f64,\n}\n\n#[derive(Debug, Clone, Hash, Eq, PartialEq, Serialize, Deserialize)]\npub struct PatternSignature {\n    /// Pattern type\n    pub pattern_type: PatternType,\n    \n    /// Key concept or topic\n    pub concept: String,\n    \n    /// Language(s) involved\n    pub languages: Vec\u003cString\u003e,\n}\n```\n\n### Coverage Analyzer\n\n```rust\n/// Analyzes coverage gaps in the skill library\npub struct CoverageAnalyzer {\n    /// CASS client for session data\n    cass: CassClient,\n    \n    /// Skill registry for existing skills\n    skill_registry: Registry,\n    \n    /// Hybrid searcher for skill matching\n    search: HybridSearcher,\n}\n\nimpl CoverageAnalyzer {\n    pub fn new(cass: CassClient, skill_registry: Registry, search: HybridSearcher) -\u003e Self {\n        Self { cass, skill_registry, search }\n    }\n    \n    /// Analyze coverage across all sessions\n    pub fn analyze_coverage(\u0026self) -\u003e Result\u003cCoverageReport, CoverageError\u003e {\n        let sessions = self.cass.list_sessions()?;\n        let existing_skills = self.skill_registry.list_all()?;\n        \n        let mut report = CoverageReport::new();\n        let mut topic_occurrences: HashMap\u003cTopic, TopicStats\u003e = HashMap::new();\n        \n        for session in sessions {\n            // Extract topics from session\n            let topics = self.extract_session_topics(\u0026session)?;\n            \n            for topic in topics {\n                let stats = topic_occurrences.entry(topic.clone()).or_default();\n                stats.occurrence_count += 1;\n                stats.sessions.push(session.id.clone());\n                \n                // Check if any skill covers this topic\n                let coverage = self.find_skill_coverage(\u0026topic, \u0026existing_skills)?;\n                \n                match coverage {\n                    SkillCoverage::Full(skill_id) =\u003e {\n                        stats.covered_by.push(skill_id);\n                    }\n                    SkillCoverage::Partial { skill_id, gap } =\u003e {\n                        stats.partially_covered_by.push((skill_id, gap));\n                    }\n                    SkillCoverage::None =\u003e {\n                        stats.uncovered = true;\n                    }\n                }\n            }\n        }\n        \n        // Build gaps list\n        for (topic, stats) in topic_occurrences {\n            if stats.uncovered \u0026\u0026 stats.occurrence_count \u003e= 3 {\n                report.gaps.push(CoverageGap {\n                    topic,\n                    occurrence_count: stats.occurrence_count,\n                    example_sessions: stats.sessions.into_iter().take(5).collect(),\n                    suggested_skill: self.suggest_skill(\u0026stats)?,\n                });\n            } else if !stats.partially_covered_by.is_empty() {\n                report.partial_gaps.push(PartialCoverageGap {\n                    topic,\n                    existing_skill: stats.partially_covered_by[0].0.clone(),\n                    missing_aspects: stats.partially_covered_by.iter()\n                        .map(|(_, gap)| gap.clone())\n                        .collect(),\n                });\n            }\n        }\n        \n        // Sort by occurrence count (most important gaps first)\n        report.gaps.sort_by(|a, b| b.occurrence_count.cmp(\u0026a.occurrence_count));\n        \n        Ok(report)\n    }\n    \n    /// Find skill coverage for a topic\n    fn find_skill_coverage(\n        \u0026self,\n        topic: \u0026Topic,\n        skills: \u0026[Skill],\n    ) -\u003e Result\u003cSkillCoverage, CoverageError\u003e {\n        // Search for matching skills\n        let query = topic.to_search_query();\n        let results = self.search.search(\u0026query, 5)?;\n        \n        if results.is_empty() {\n            return Ok(SkillCoverage::None);\n        }\n        \n        let best_match = \u0026results[0];\n        \n        // Check coverage depth\n        let coverage_score = self.calculate_coverage_score(topic, \u0026best_match.skill)?;\n        \n        if coverage_score \u003e= 0.8 {\n            Ok(SkillCoverage::Full(best_match.skill.id.clone()))\n        } else if coverage_score \u003e= 0.4 {\n            let gap = self.identify_gap(topic, \u0026best_match.skill)?;\n            Ok(SkillCoverage::Partial {\n                skill_id: best_match.skill.id.clone(),\n                gap,\n            })\n        } else {\n            Ok(SkillCoverage::None)\n        }\n    }\n    \n    /// Calculate how well a skill covers a topic\n    fn calculate_coverage_score(\u0026self, topic: \u0026Topic, skill: \u0026Skill) -\u003e Result\u003cf64, CoverageError\u003e {\n        let mut score = 0.0;\n        let mut total_weight = 0.0;\n        \n        // Check concept coverage\n        for concept in \u0026topic.concepts {\n            let weight = concept.importance;\n            total_weight += weight;\n            \n            if skill.mentions_concept(\u0026concept.name) {\n                score += weight * 1.0;\n            } else if skill.mentions_related_concept(\u0026concept.name) {\n                score += weight * 0.5;\n            }\n        }\n        \n        if total_weight == 0.0 {\n            return Ok(0.0);\n        }\n        \n        Ok(score / total_weight)\n    }\n    \n    /// Identify what's missing in skill coverage\n    fn identify_gap(\u0026self, topic: \u0026Topic, skill: \u0026Skill) -\u003e Result\u003cString, CoverageError\u003e {\n        let mut missing = Vec::new();\n        \n        for concept in \u0026topic.concepts {\n            if !skill.mentions_concept(\u0026concept.name) \u0026\u0026 !skill.mentions_related_concept(\u0026concept.name) {\n                missing.push(concept.name.clone());\n            }\n        }\n        \n        Ok(missing.join(\", \"))\n    }\n    \n    /// Suggest a skill to fill the gap\n    fn suggest_skill(\u0026self, stats: \u0026TopicStats) -\u003e Result\u003cSkillSuggestion, CoverageError\u003e {\n        // TODO: Use LLM to generate skill suggestion based on session context\n        Ok(SkillSuggestion {\n            suggested_name: format!(\"{}-skill\", stats.topic.name.to_lowercase().replace(' ', \"-\")),\n            suggested_sections: vec![\"overview\", \"best-practices\", \"examples\"]\n                .into_iter()\n                .map(String::from)\n                .collect(),\n            source_sessions: stats.sessions.clone(),\n        })\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CoverageReport {\n    /// Topics with no skill coverage\n    pub gaps: Vec\u003cCoverageGap\u003e,\n    \n    /// Topics with partial skill coverage\n    pub partial_gaps: Vec\u003cPartialCoverageGap\u003e,\n    \n    /// Overall coverage statistics\n    pub stats: CoverageStats,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CoverageGap {\n    /// Topic not covered\n    pub topic: Topic,\n    \n    /// How often this topic appears\n    pub occurrence_count: u32,\n    \n    /// Example sessions where topic appeared\n    pub example_sessions: Vec\u003cSessionId\u003e,\n    \n    /// Suggested skill to fill gap\n    pub suggested_skill: SkillSuggestion,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PartialCoverageGap {\n    pub topic: Topic,\n    pub existing_skill: SkillId,\n    pub missing_aspects: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CoverageStats {\n    pub total_topics: u32,\n    pub fully_covered: u32,\n    pub partially_covered: u32,\n    pub uncovered: u32,\n    pub coverage_percentage: f64,\n}\n\n#[derive(Debug)]\npub enum SkillCoverage {\n    Full(SkillId),\n    Partial { skill_id: SkillId, gap: String },\n    None,\n}\n```\n\n### Knowledge Graph\n\n```rust\n/// Knowledge graph connecting concepts across projects and skills\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct KnowledgeGraph {\n    /// All nodes in the graph\n    pub nodes: Vec\u003cGraphNode\u003e,\n    \n    /// All edges in the graph\n    pub edges: Vec\u003cGraphEdge\u003e,\n    \n    /// Index for fast lookup\n    #[serde(skip)]\n    node_index: HashMap\u003cNodeId, usize\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GraphNode {\n    /// Unique node identifier\n    pub id: NodeId,\n    \n    /// Node type\n    pub node_type: NodeType,\n    \n    /// Node label/name\n    pub label: String,\n    \n    /// Node properties\n    pub properties: HashMap\u003cString, String\u003e,\n    \n    /// Embedding for semantic search\n    pub embedding: Option\u003cVec\u003cf32\u003e\u003e,\n}\n\n#[derive(Debug, Clone, Hash, Eq, PartialEq, Serialize, Deserialize)]\npub struct NodeId(pub String);\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum NodeType {\n    /// A concept (e.g., \"error handling\", \"async/await\")\n    Concept,\n    \n    /// A skill\n    Skill,\n    \n    /// A project\n    Project,\n    \n    /// A technology (language, framework, tool)\n    Technology,\n    \n    /// A pattern\n    Pattern,\n    \n    /// An error category\n    ErrorCategory,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GraphEdge {\n    /// Source node\n    pub from: NodeId,\n    \n    /// Target node\n    pub to: NodeId,\n    \n    /// Edge type\n    pub edge_type: EdgeType,\n    \n    /// Edge weight (strength of relationship)\n    pub weight: f64,\n    \n    /// Edge properties\n    pub properties: HashMap\u003cString, String\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum EdgeType {\n    /// Skill covers concept\n    Covers,\n    \n    /// Concept relates to concept\n    RelatedTo,\n    \n    /// Project uses technology\n    Uses,\n    \n    /// Pattern applies to concept\n    AppliesTo,\n    \n    /// Concept is prerequisite for another\n    PrerequisiteFor,\n    \n    /// Error relates to concept\n    ErrorRelatesTo,\n    \n    /// Concept is part of broader concept\n    PartOf,\n}\n\nimpl KnowledgeGraph {\n    pub fn new() -\u003e Self {\n        Self {\n            nodes: Vec::new(),\n            edges: Vec::new(),\n            node_index: HashMap::new(),\n        }\n    }\n    \n    /// Add a node to the graph\n    pub fn add_node(\u0026mut self, node: GraphNode) {\n        let index = self.nodes.len();\n        self.node_index.insert(node.id.clone(), index);\n        self.nodes.push(node);\n    }\n    \n    /// Add an edge to the graph\n    pub fn add_edge(\u0026mut self, edge: GraphEdge) {\n        self.edges.push(edge);\n    }\n    \n    /// Find nodes related to a concept\n    pub fn find_related(\u0026self, node_id: \u0026NodeId, max_hops: u32) -\u003e Vec\u003c\u0026GraphNode\u003e {\n        let mut visited = HashSet::new();\n        let mut result = Vec::new();\n        let mut queue = vec![(node_id.clone(), 0u32)];\n        \n        while let Some((current, hops)) = queue.pop() {\n            if visited.contains(\u0026current) || hops \u003e max_hops {\n                continue;\n            }\n            visited.insert(current.clone());\n            \n            if let Some(\u0026index) = self.node_index.get(\u0026current) {\n                result.push(\u0026self.nodes[index]);\n            }\n            \n            // Find connected nodes\n            for edge in \u0026self.edges {\n                if edge.from == current \u0026\u0026 !visited.contains(\u0026edge.to) {\n                    queue.push((edge.to.clone(), hops + 1));\n                }\n                if edge.to == current \u0026\u0026 !visited.contains(\u0026edge.from) {\n                    queue.push((edge.from.clone(), hops + 1));\n                }\n            }\n        }\n        \n        result\n    }\n    \n    /// Find skills that cover a concept\n    pub fn find_covering_skills(\u0026self, concept_id: \u0026NodeId) -\u003e Vec\u003c\u0026GraphNode\u003e {\n        self.edges\n            .iter()\n            .filter(|e| e.to == *concept_id \u0026\u0026 e.edge_type == EdgeType::Covers)\n            .filter_map(|e| self.node_index.get(\u0026e.from))\n            .map(|\u0026i| \u0026self.nodes[i])\n            .collect()\n    }\n    \n    /// Find concepts not covered by any skill\n    pub fn find_uncovered_concepts(\u0026self) -\u003e Vec\u003c\u0026GraphNode\u003e {\n        let covered: HashSet\u003c_\u003e = self.edges\n            .iter()\n            .filter(|e| matches!(e.edge_type, EdgeType::Covers))\n            .map(|e| \u0026e.to)\n            .collect();\n        \n        self.nodes\n            .iter()\n            .filter(|n| matches!(n.node_type, NodeType::Concept))\n            .filter(|n| !covered.contains(\u0026n.id))\n            .collect()\n    }\n    \n    /// Find shortest path between two nodes\n    pub fn shortest_path(\u0026self, from: \u0026NodeId, to: \u0026NodeId) -\u003e Option\u003cVec\u003cNodeId\u003e\u003e {\n        use std::collections::VecDeque;\n        \n        let mut visited = HashSet::new();\n        let mut queue = VecDeque::new();\n        let mut parent: HashMap\u003cNodeId, NodeId\u003e = HashMap::new();\n        \n        queue.push_back(from.clone());\n        visited.insert(from.clone());\n        \n        while let Some(current) = queue.pop_front() {\n            if current == *to {\n                // Reconstruct path\n                let mut path = vec![current.clone()];\n                let mut node = \u0026current;\n                while let Some(p) = parent.get(node) {\n                    path.push(p.clone());\n                    node = p;\n                }\n                path.reverse();\n                return Some(path);\n            }\n            \n            for edge in \u0026self.edges {\n                let neighbor = if edge.from == current {\n                    \u0026edge.to\n                } else if edge.to == current {\n                    \u0026edge.from\n                } else {\n                    continue\n                };\n                \n                if !visited.contains(neighbor) {\n                    visited.insert(neighbor.clone());\n                    parent.insert(neighbor.clone(), current.clone());\n                    queue.push_back(neighbor.clone());\n                }\n            }\n        }\n        \n        None\n    }\n    \n    /// Export to DOT format for visualization\n    pub fn to_dot(\u0026self) -\u003e String {\n        let mut dot = String::from(\"digraph KnowledgeGraph {\\n\");\n        dot.push_str(\"  rankdir=LR;\\n\");\n        dot.push_str(\"  node [shape=box];\\n\\n\");\n        \n        // Add nodes\n        for node in \u0026self.nodes {\n            let color = match node.node_type {\n                NodeType::Concept =\u003e \"lightblue\",\n                NodeType::Skill =\u003e \"lightgreen\",\n                NodeType::Project =\u003e \"lightyellow\",\n                NodeType::Technology =\u003e \"lightpink\",\n                NodeType::Pattern =\u003e \"lavender\",\n                NodeType::ErrorCategory =\u003e \"lightsalmon\",\n            };\n            dot.push_str(\u0026format!(\n                \"  \\\"{}\\\" [label=\\\"{}\\\" fillcolor={} style=filled];\\n\",\n                node.id.0, node.label, color\n            ));\n        }\n        \n        dot.push_str(\"\\n\");\n        \n        // Add edges\n        for edge in \u0026self.edges {\n            let label = match \u0026edge.edge_type {\n                EdgeType::Covers =\u003e \"covers\",\n                EdgeType::RelatedTo =\u003e \"related\",\n                EdgeType::Uses =\u003e \"uses\",\n                EdgeType::AppliesTo =\u003e \"applies\",\n                EdgeType::PrerequisiteFor =\u003e \"prereq\",\n                EdgeType::ErrorRelatesTo =\u003e \"error\",\n                EdgeType::PartOf =\u003e \"part_of\",\n            };\n            dot.push_str(\u0026format!(\n                \"  \\\"{}\\\" -\u003e \\\"{}\\\" [label=\\\"{}\\\"];\\n\",\n                edge.from.0, edge.to.0, label\n            ));\n        }\n        \n        dot.push_str(\"}\\n\");\n        dot\n    }\n}\n\n/// Builds knowledge graph from analysis results\npub struct KnowledgeGraphBuilder {\n    /// LLM client for concept extraction\n    llm: Option\u003cLlmClient\u003e,\n}\n\nimpl KnowledgeGraphBuilder {\n    pub fn new() -\u003e Self {\n        Self { llm: None }\n    }\n    \n    /// Build knowledge graph from cross-project report\n    pub fn build(\u0026self, report: \u0026CrossProjectReport) -\u003e Result\u003cKnowledgeGraph, GraphError\u003e {\n        let mut graph = KnowledgeGraph::new();\n        \n        // Add project nodes\n        for (project_id, analysis) in \u0026report.project_analyses {\n            graph.add_node(GraphNode {\n                id: NodeId(format!(\"project:{}\", project_id)),\n                node_type: NodeType::Project,\n                label: project_id.clone(),\n                properties: HashMap::new(),\n                embedding: None,\n            });\n            \n            // Add technology nodes and edges\n            for (tech, count) in \u0026analysis.tool_usage {\n                let tech_id = NodeId(format!(\"tech:{}\", tech));\n                if !graph.node_index.contains_key(\u0026tech_id) {\n                    graph.add_node(GraphNode {\n                        id: tech_id.clone(),\n                        node_type: NodeType::Technology,\n                        label: tech.clone(),\n                        properties: HashMap::new(),\n                        embedding: None,\n                    });\n                }\n                \n                graph.add_edge(GraphEdge {\n                    from: NodeId(format!(\"project:{}\", project_id)),\n                    to: tech_id,\n                    edge_type: EdgeType::Uses,\n                    weight: *count as f64,\n                    properties: HashMap::new(),\n                });\n            }\n            \n            // Add topic nodes\n            for topic in \u0026analysis.topics {\n                let topic_id = NodeId(format!(\"concept:{}\", topic.name.to_lowercase().replace(' ', \"_\")));\n                if !graph.node_index.contains_key(\u0026topic_id) {\n                    graph.add_node(GraphNode {\n                        id: topic_id.clone(),\n                        node_type: NodeType::Concept,\n                        label: topic.name.clone(),\n                        properties: HashMap::new(),\n                        embedding: None,\n                    });\n                }\n            }\n        }\n        \n        // Add universal pattern nodes\n        for pattern in \u0026report.universal_patterns {\n            let pattern_id = NodeId(format!(\"pattern:{}\", pattern.signature.concept));\n            graph.add_node(GraphNode {\n                id: pattern_id.clone(),\n                node_type: NodeType::Pattern,\n                label: pattern.signature.concept.clone(),\n                properties: HashMap::from([\n                    (\"occurrence_count\".to_string(), pattern.occurrence_count.to_string()),\n                    (\"confidence\".to_string(), pattern.confidence.to_string()),\n                ]),\n                embedding: None,\n            });\n            \n            // Connect pattern to concept\n            let concept_id = NodeId(format!(\"concept:{}\", pattern.signature.concept.to_lowercase().replace(' ', \"_\")));\n            graph.add_edge(GraphEdge {\n                from: pattern_id,\n                to: concept_id,\n                edge_type: EdgeType::AppliesTo,\n                weight: pattern.confidence,\n                properties: HashMap::new(),\n            });\n        }\n        \n        // Add concept relationships (using LLM if available)\n        self.add_concept_relationships(\u0026mut graph)?;\n        \n        Ok(graph)\n    }\n    \n    fn add_concept_relationships(\u0026self, graph: \u0026mut KnowledgeGraph) -\u003e Result\u003c(), GraphError\u003e {\n        // Get all concept nodes\n        let concepts: Vec\u003c_\u003e = graph.nodes\n            .iter()\n            .filter(|n| matches!(n.node_type, NodeType::Concept))\n            .map(|n| (n.id.clone(), n.label.clone()))\n            .collect();\n        \n        // Add known relationships (could be enhanced with LLM)\n        let known_relationships = vec![\n            (\"error handling\", \"result type\", EdgeType::PartOf),\n            (\"error handling\", \"panic\", EdgeType::PartOf),\n            (\"async\", \"futures\", EdgeType::PartOf),\n            (\"async\", \"tokio\", EdgeType::Uses),\n            (\"testing\", \"unit tests\", EdgeType::PartOf),\n            (\"testing\", \"integration tests\", EdgeType::PartOf),\n        ];\n        \n        for (from, to, edge_type) in known_relationships {\n            let from_id = NodeId(format!(\"concept:{}\", from.replace(' ', \"_\")));\n            let to_id = NodeId(format!(\"concept:{}\", to.replace(' ', \"_\")));\n            \n            if graph.node_index.contains_key(\u0026from_id) \u0026\u0026 graph.node_index.contains_key(\u0026to_id) {\n                graph.add_edge(GraphEdge {\n                    from: from_id,\n                    to: to_id,\n                    edge_type,\n                    weight: 1.0,\n                    properties: HashMap::new(),\n                });\n            }\n        }\n        \n        Ok(())\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms coverage`\n\n```\nAnalyze skill coverage across sessions\n\nUSAGE:\n    ms coverage [OPTIONS]\n\nOPTIONS:\n    --project \u003cPATH\u003e        Analyze specific project (can specify multiple)\n    --all-projects          Analyze all registered projects\n    --min-occurrences \u003cN\u003e   Minimum topic occurrences to report [default: 3]\n    --format \u003cFMT\u003e          Output format: text, json, markdown [default: text]\n    -v, --verbose           Show detailed analysis\n\nOUTPUT EXAMPLE:\n    Skill Coverage Analysis\n    =======================\n    \n    Overall Coverage: 73.2%\n      Fully Covered:     89 topics\n      Partially Covered: 23 topics  \n      Uncovered:         31 topics\n    \n    Top Coverage Gaps (uncovered topics):\n    \n    1. \"GraphQL Schema Design\" (12 occurrences)\n       Sessions: proj-a#12, proj-b#45, proj-c#23, ...\n       Suggested: Create skill \"graphql-schema-design\"\n       \n    2. \"Kubernetes Networking\" (9 occurrences)\n       Sessions: infra#5, infra#12, webapp#89, ...\n       Suggested: Create skill \"k8s-networking\"\n       \n    3. \"React Performance Optimization\" (8 occurrences)\n       Sessions: frontend#34, frontend#56, mobile#12, ...\n       Suggested: Create skill \"react-performance\"\n    \n    Partial Coverage Gaps:\n    \n    1. \"Error Handling\" - skill: rust-error-handling\n       Missing: async error handling, error chain patterns\n       \n    2. \"Database Migrations\" - skill: database-basics\n       Missing: rollback strategies, zero-downtime migrations\n```\n\n### `ms coverage --show-gaps`\n\n```\nShow detailed gap analysis\n\nUSAGE:\n    ms coverage --show-gaps [OPTIONS]\n\nOPTIONS:\n    --gap \u003cTOPIC\u003e           Show details for specific gap\n    --generate-skill        Generate suggested skill for gap\n    --export \u003cFILE\u003e         Export gaps to file\n\nOUTPUT EXAMPLE:\n    Coverage Gap: \"GraphQL Schema Design\"\n    =====================================\n    \n    Occurrence Count: 12\n    Projects: proj-a (5), proj-b (4), proj-c (3)\n    \n    Example Sessions:\n    \n    1. proj-a#12 (2024-01-15)\n       Context: \"How do I design a GraphQL schema for...\"\n       Duration: 45 minutes\n       Outcome: Partial success\n       \n    2. proj-b#45 (2024-01-18)\n       Context: \"Best practices for GraphQL mutations...\"\n       Duration: 30 minutes\n       Outcome: Success\n    \n    Key Concepts Extracted:\n      - Schema-first design\n      - Type relationships\n      - Custom scalars\n      - Input types vs output types\n      - Resolver patterns\n    \n    Related Existing Skills:\n      - api-design (partial match: 23%)\n      - database-schema (partial match: 15%)\n    \n    Suggested Skill Structure:\n      Name: graphql-schema-design\n      Sections:\n        - overview: GraphQL schema fundamentals\n        - types: Defining types and relationships\n        - mutations: Mutation design patterns\n        - best-practices: Schema design principles\n        - examples: Real-world schema examples\n```\n\n### `ms analyze --cross-project`\n\n```\nRun cross-project analysis\n\nUSAGE:\n    ms analyze --cross-project [OPTIONS]\n\nOPTIONS:\n    --register \u003cPATH\u003e       Register project for analysis\n    --list-projects         List registered projects\n    --unregister \u003cID\u003e       Unregister a project\n    --full-report           Generate comprehensive report\n    --graph                 Generate knowledge graph\n    --graph-format \u003cFMT\u003e    Graph format: dot, json [default: dot]\n    --patterns              Focus on universal patterns\n    --export \u003cDIR\u003e          Export analysis results\n\nEXAMPLES:\n    # Register projects\n    ms analyze --cross-project --register ~/projects/webapp\n    ms analyze --cross-project --register ~/projects/api-server\n    \n    # Run analysis\n    ms analyze --cross-project --full-report\n    \n    # Generate knowledge graph\n    ms analyze --cross-project --graph --graph-format dot \u003e knowledge.dot\n    dot -Tsvg knowledge.dot -o knowledge.svg\n    \n    # Extract universal patterns\n    ms analyze --cross-project --patterns\n\nOUTPUT EXAMPLE (patterns):\n    Universal Patterns Across Projects\n    ==================================\n    \n    1. \"Result-based Error Handling\" (Rust)\n       Projects: api-server, cli-tool, library\n       Occurrences: 47\n       Confidence: 0.92\n       \n       Pattern:\n         - Use Result\u003cT, E\u003e for recoverable errors\n         - Create custom error types with thiserror\n         - Use ? operator for propagation\n         - Map errors at boundaries\n    \n    2. \"Repository Pattern\" (Multiple)\n       Projects: webapp, api-server, data-pipeline\n       Occurrences: 23\n       Confidence: 0.85\n       \n       Pattern:\n         - Abstract data access behind trait/interface\n         - Separate domain logic from persistence\n         - Use dependency injection for testing\n```\n\n---\n\n## Project Registration\n\n```rust\nimpl CrossProjectAnalyzer {\n    /// Register a project from path\n    pub fn register_from_path(\u0026mut self, path: \u0026Path) -\u003e Result\u003cProjectInfo, AnalyzerError\u003e {\n        // Find CASS database\n        let cass_path = self.find_cass_db(path)?;\n        \n        // Detect project metadata\n        let metadata = self.detect_metadata(path)?;\n        \n        let project = ProjectInfo {\n            id: self.generate_project_id(path),\n            name: path.file_name()\n                .map(|n| n.to_string_lossy().to_string())\n                .unwrap_or_else(|| \"unknown\".to_string()),\n            path: path.to_path_buf(),\n            cass_path,\n            metadata,\n            registered_at: Utc::now(),\n            last_analyzed: None,\n        };\n        \n        self.register_project(project.clone())?;\n        \n        Ok(project)\n    }\n    \n    /// Find CASS database for a project\n    fn find_cass_db(\u0026self, path: \u0026Path) -\u003e Result\u003cPathBuf, AnalyzerError\u003e {\n        // Check common locations\n        let candidates = vec![\n            path.join(\".cass\").join(\"sessions.db\"),\n            path.join(\".claude\").join(\"cass.db\"),\n            dirs::data_local_dir()\n                .unwrap_or_default()\n                .join(\"cass\")\n                .join(\"projects\")\n                .join(path.file_name().unwrap_or_default())\n                .join(\"sessions.db\"),\n        ];\n        \n        for candidate in candidates {\n            if candidate.exists() {\n                return Ok(candidate);\n            }\n        }\n        \n        Err(AnalyzerError::CassNotFound(path.to_path_buf()))\n    }\n    \n    /// Detect project metadata from files\n    fn detect_metadata(\u0026self, path: \u0026Path) -\u003e Result\u003cProjectMetadata, AnalyzerError\u003e {\n        let mut languages = Vec::new();\n        let mut frameworks = Vec::new();\n        \n        // Check for language indicators\n        if path.join(\"Cargo.toml\").exists() {\n            languages.push(\"rust\".to_string());\n        }\n        if path.join(\"package.json\").exists() {\n            languages.push(\"javascript\".to_string());\n            languages.push(\"typescript\".to_string());\n            \n            // Check for frameworks\n            if let Ok(content) = std::fs::read_to_string(path.join(\"package.json\")) {\n                if content.contains(\"\\\"react\\\"\") {\n                    frameworks.push(\"react\".to_string());\n                }\n                if content.contains(\"\\\"vue\\\"\") {\n                    frameworks.push(\"vue\".to_string());\n                }\n                if content.contains(\"\\\"express\\\"\") {\n                    frameworks.push(\"express\".to_string());\n                }\n            }\n        }\n        if path.join(\"requirements.txt\").exists() || path.join(\"pyproject.toml\").exists() {\n            languages.push(\"python\".to_string());\n        }\n        if path.join(\"go.mod\").exists() {\n            languages.push(\"go\".to_string());\n        }\n        \n        // Detect project type\n        let project_type = self.detect_project_type(path, \u0026languages, \u0026frameworks)?;\n        \n        // Estimate size\n        let size_estimate = self.estimate_project_size(path)?;\n        \n        Ok(ProjectMetadata {\n            languages,\n            frameworks,\n            project_type,\n            size_estimate,\n            tags: Vec::new(),\n        })\n    }\n    \n    fn detect_project_type(\n        \u0026self,\n        path: \u0026Path,\n        languages: \u0026[String],\n        frameworks: \u0026[String],\n    ) -\u003e Result\u003cProjectType, AnalyzerError\u003e {\n        // Check for specific indicators\n        if frameworks.iter().any(|f| [\"react\", \"vue\", \"angular\"].contains(\u0026f.as_str())) {\n            if path.join(\"server\").exists() || path.join(\"api\").exists() {\n                return Ok(ProjectType::FullStack);\n            }\n            return Ok(ProjectType::WebFrontend);\n        }\n        \n        if frameworks.iter().any(|f| [\"express\", \"fastapi\", \"actix\", \"gin\"].contains(\u0026f.as_str())) {\n            return Ok(ProjectType::WebBackend);\n        }\n        \n        if path.join(\"src/main.rs\").exists() || path.join(\"src/main.go\").exists() {\n            // Check for web server indicators\n            if let Ok(content) = std::fs::read_to_string(path.join(\"Cargo.toml\")) {\n                if content.contains(\"actix-web\") || content.contains(\"axum\") || content.contains(\"rocket\") {\n                    return Ok(ProjectType::WebBackend);\n                }\n            }\n            return Ok(ProjectType::Cli);\n        }\n        \n        if path.join(\"lib.rs\").exists() || path.join(\"index.ts\").exists() {\n            return Ok(ProjectType::Library);\n        }\n        \n        Ok(ProjectType::Other(\"unknown\".to_string()))\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum AnalyzerError {\n    #[error(\"CASS database not found at {0}\")]\n    CassNotFound(PathBuf),\n    \n    #[error(\"Duplicate project: {0}\")]\n    DuplicateProject(String),\n    \n    #[error(\"Project not found: {0}\")]\n    ProjectNotFound(String),\n    \n    #[error(\"Database error: {0}\")]\n    DatabaseError(#[from] rusqlite::Error),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Analysis error: {0}\")]\n    AnalysisError(String),\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum CoverageError {\n    #[error(\"Search error: {0}\")]\n    SearchError(String),\n    \n    #[error(\"Registry error: {0}\")]\n    RegistryError(String),\n    \n    #[error(\"CASS error: {0}\")]\n    CassError(String),\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum GraphError {\n    #[error(\"Node not found: {0}\")]\n    NodeNotFound(String),\n    \n    #[error(\"Invalid edge: {0}\")]\n    InvalidEdge(String),\n    \n    #[error(\"Cycle detected\")]\n    CycleDetected,\n}\n```\n\n---\n\n## Dependencies\n\n- **CASS Client Integration** (meta_skill-hhu): Access to session data across projects\n- `rusqlite`: Database access\n- `serde`, `serde_json`: Serialization\n- `chrono`: Timestamps\n- `petgraph` (optional): Graph algorithms\n- `walkdir`: File system traversal","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:58:12.241581989-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:29:59.927233364-05:00","labels":["coverage","cross-project","learning","phase-6"],"dependencies":[{"issue_id":"meta_skill-8ti","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T23:04:14.72051745-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-93z","title":"[P2] RRF Score Fusion","description":"# RRF Score Fusion\n\nCombine BM25 and embedding rankings using Reciprocal Rank Fusion.\n\n## Tasks\n1. Implement RRF algorithm\n2. Configure rank constant k (default 60)\n3. Configurable weighting between BM25 and embedding\n4. Tie-breaking strategy\n5. Score explanation in verbose mode\n\n## Algorithm (from Section 6.3)\n```\nRRF(doc) =  1 / (k + rank_i(doc))\n```\n\nWhere:\n- k = smoothing constant (60 is standard)\n- rank_i = rank from ranking system i\n\n## Fusion Configuration\n```yaml\nsearch:\n  bm25_weight: 0.6\n  embedding_weight: 0.4\n  rrf_k: 60\n```\n\n## Debug Output\nFor --verbose, show:\n- BM25 rank for each result\n- Embedding rank for each result\n- Final fused score\n\n## Acceptance Criteria\n- Fusion improves result quality\n- Configuration is tunable\n- Verbose mode shows score breakdown","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:04.096281917-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:23:04.096281917-05:00","labels":["phase-2","ranking","search"],"dependencies":[{"issue_id":"meta_skill-93z","depends_on_id":"meta_skill-mh8","type":"blocks","created_at":"2026-01-13T22:23:13.518848392-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-93z","depends_on_id":"meta_skill-ch6","type":"blocks","created_at":"2026-01-13T22:23:13.544929945-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-9ik","title":"[P3] Token Packer (Constrained Optimization)","description":"# Token Packer (Constrained Optimization)\n\nFit slices into token budget optimally.\n\n## Tasks\n1. Implement packing algorithm (not greedy)\n2. Respect mandatory policy slices\n3. Maximize value within budget\n4. Handle slice dependencies\n5. Output packed markdown\n\n## Algorithm (from Section 5.3)\n1. Start with mandatory slices\n2. Calculate remaining budget\n3. Apply constrained optimization\n4. Respect dependencies (if A, must include B)\n5. Return optimal slice selection\n\n## Optimization Target\n```\nmaximize:  value(slice) * selected(slice)\nsubject to:  tokens(slice) * selected(slice)  budget\n            mandatory(slice)  selected(slice) = 1\n            depends(A, B)  selected(A)  selected(B)\n```\n\n## CLI Integration\n- `ms load skill --pack 2000` - fit in 2000 tokens\n- `ms load skill --pack auto` - auto-detect budget\n\n## Acceptance Criteria\n- Packer respects budget\n- Mandatory slices always included\n- Dependencies satisfied","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:13.899489889-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:24:13.899489889-05:00","labels":["optimization","packing","phase-3"],"dependencies":[{"issue_id":"meta_skill-9ik","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T22:24:25.873226602-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-9ik","depends_on_id":"meta_skill-sqh","type":"blocks","created_at":"2026-01-13T22:24:25.900919392-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-9ok","title":"[Cross-Cutting] Testing Strategy","description":"# Testing Strategy\n\nComprehensive testing approach from the plan (Section 18, 34).\n\n## Test Types\n1. **Unit Tests**: Table-driven, property-based with proptest\n2. **Integration Tests**: Real filesystem, temp directories\n3. **E2E Tests**: Full CLI workflow tests\n4. **Benchmark Tests**: Criterion for performance\n5. **Snapshot Tests**: Output verification\n6. **Skill Tests**: Validate skill correctness\n\n## Testing Philosophy (from Section 34.1)\n\u003e \"NO mocks - test real implementations with real data fixtures.\"\n\nWhen to mock:\n- Animations (timing-dependent)\n- External APIs (when testing HTTP client behavior)\n- Time-dependent operations (use fakeable clocks)\n\n## Key Patterns\n- Use t.TempDir() / mkdtempSync() for filesystem\n- Table-driven tests for edge cases\n- Property tests: idempotence, determinism, safety\n- Real clipboard testing (from jeffreysprompts)\n\n## CI Integration\n- JUnit/TAP output for reporting\n- GitHub Actions workflow\n- Coverage reporting\n\n## Acceptance Criteria\n- All test types implemented\n- CI pipeline green\n- Coverage \u003e 80% for core modules","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:29:07.186502294-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:29:07.186502294-05:00","labels":["cross-cutting","quality","testing"]}
{"id":"meta_skill-9pr","title":"Integration Test Framework","description":"## Overview\n\nImplement comprehensive integration test framework for the meta_skill CLI that tests real filesystem operations, uses temp directories, and exercises full CLI workflows. This bead implements Section 18.3 of the Testing Strategy with real database state verification.\n\n## Requirements\n\n### 1. TestFixture Struct\n\nCreate `tests/integration/fixture.rs`:\n\n```rust\nuse std::path::PathBuf;\nuse std::process::Command;\nuse tempfile::TempDir;\nuse rusqlite::Connection;\n\n/// Integration test fixture providing complete test environment\npub struct TestFixture {\n    /// Root temp directory - all other paths are relative to this\n    pub temp_dir: TempDir,\n    \n    /// Config directory (~/.config/ms equivalent)\n    pub config_dir: PathBuf,\n    \n    /// Skills directory (~/.local/share/ms/skills equivalent)\n    pub skills_dir: PathBuf,\n    \n    /// Database connection for state verification\n    pub db: Option\u003cConnection\u003e,\n    \n    /// Search index path\n    pub index_path: PathBuf,\n    \n    /// Test start time for timing\n    start_time: std::time::Instant,\n    \n    /// Test name for logging\n    test_name: String,\n}\n\nimpl TestFixture {\n    /// Create a fresh test fixture\n    pub async fn new(test_name: \u0026str) -\u003e Self {\n        let start_time = std::time::Instant::now();\n        let temp_dir = TempDir::new().expect(\"Failed to create temp dir\");\n        let root = temp_dir.path();\n        \n        let config_dir = root.join(\".config/ms\");\n        let skills_dir = root.join(\".local/share/ms/skills\");\n        let index_path = root.join(\".local/share/ms/index\");\n        \n        std::fs::create_dir_all(\u0026config_dir).expect(\"Failed to create config dir\");\n        std::fs::create_dir_all(\u0026skills_dir).expect(\"Failed to create skills dir\");\n        std::fs::create_dir_all(\u0026index_path).expect(\"Failed to create index dir\");\n        \n        println!(\"\\n{'='.repeat(70)}\");\n        println!(\"[FIXTURE] Test: {}\", test_name);\n        println!(\"[FIXTURE] Root: {:?}\", root);\n        println!(\"[FIXTURE] Config: {:?}\", config_dir);\n        println!(\"[FIXTURE] Skills: {:?}\", skills_dir);\n        println!(\"[FIXTURE] Index: {:?}\", index_path);\n        println!(\"{'='.repeat(70)}\");\n        \n        Self {\n            temp_dir,\n            config_dir,\n            skills_dir,\n            index_path,\n            db: None,\n            start_time,\n            test_name: test_name.to_string(),\n        }\n    }\n    \n    /// Create fixture with pre-indexed skills\n    pub async fn with_indexed_skills(test_name: \u0026str, skills: \u0026[TestSkill]) -\u003e Self {\n        let mut fixture = Self::new(test_name).await;\n        \n        for skill in skills {\n            fixture.add_skill(skill);\n        }\n        \n        // Run ms index\n        let output = fixture.run_ms(\u0026[\"index\"]).await;\n        assert!(output.success, \"Failed to index skills: {}\", output.stderr);\n        \n        // Open database connection for verification\n        let db_path = fixture.config_dir.join(\"ms.db\");\n        if db_path.exists() {\n            fixture.db = Some(Connection::open(\u0026db_path).expect(\"Failed to open db\"));\n            println!(\"[FIXTURE] Database opened: {:?}\", db_path);\n        }\n        \n        fixture\n    }\n    \n    /// Create fixture with mock CASS integration\n    pub async fn with_mock_cass(test_name: \u0026str) -\u003e Self {\n        let fixture = Self::new(test_name).await;\n        \n        // Create mock CASS response files\n        let cass_dir = fixture.temp_dir.path().join(\"mock_cass\");\n        std::fs::create_dir_all(\u0026cass_dir).expect(\"Failed to create mock CASS dir\");\n        \n        // Create mock extraction response\n        let extraction = r#\"{\n            \"skill_name\": \"test-skill\",\n            \"description\": \"A test skill for integration testing\",\n            \"patterns\": [\"pattern1\", \"pattern2\"],\n            \"confidence\": 0.85\n        }\"#;\n        std::fs::write(cass_dir.join(\"extraction.json\"), extraction)\n            .expect(\"Failed to write mock extraction\");\n        \n        println!(\"[FIXTURE] Mock CASS configured at: {:?}\", cass_dir);\n        \n        fixture\n    }\n    \n    /// Add a skill to the test environment\n    pub fn add_skill(\u0026self, skill: \u0026TestSkill) {\n        let skill_dir = self.skills_dir.join(\u0026skill.name);\n        std::fs::create_dir_all(\u0026skill_dir).expect(\"Failed to create skill dir\");\n        \n        let skill_file = skill_dir.join(\"SKILL.md\");\n        std::fs::write(\u0026skill_file, \u0026skill.content).expect(\"Failed to write skill\");\n        \n        println!(\"[FIXTURE] Added skill: {} ({} bytes)\", skill.name, skill.content.len());\n    }\n    \n    /// Run ms CLI command and capture output\n    pub async fn run_ms(\u0026self, args: \u0026[\u0026str]) -\u003e CommandOutput {\n        let start = std::time::Instant::now();\n        \n        println!(\"\\n[CMD] ms {}\", args.join(\" \"));\n        \n        let output = Command::new(env!(\"CARGO_BIN_EXE_ms\"))\n            .args(args)\n            .env(\"MS_CONFIG_DIR\", \u0026self.config_dir)\n            .env(\"MS_DATA_DIR\", self.temp_dir.path().join(\".local/share/ms\"))\n            .env(\"HOME\", self.temp_dir.path())\n            .current_dir(self.temp_dir.path())\n            .output()\n            .expect(\"Failed to execute ms command\");\n        \n        let elapsed = start.elapsed();\n        let stdout = String::from_utf8_lossy(\u0026output.stdout).to_string();\n        let stderr = String::from_utf8_lossy(\u0026output.stderr).to_string();\n        \n        println!(\"[CMD] Exit code: {}\", output.status.code().unwrap_or(-1));\n        println!(\"[CMD] Timing: {:?}\", elapsed);\n        if !stdout.is_empty() {\n            println!(\"[STDOUT]\\n{}\", stdout);\n        }\n        if !stderr.is_empty() {\n            println!(\"[STDERR]\\n{}\", stderr);\n        }\n        \n        CommandOutput {\n            success: output.status.success(),\n            exit_code: output.status.code().unwrap_or(-1),\n            stdout,\n            stderr,\n            elapsed,\n        }\n    }\n    \n    /// Verify database state\n    pub fn verify_db_state(\u0026self, check: impl FnOnce(\u0026Connection) -\u003e bool, description: \u0026str) {\n        if let Some(ref db) = self.db {\n            let db_state_before = self.dump_db_state(db);\n            println!(\"[DB STATE BEFORE] {}\", db_state_before);\n            \n            let result = check(db);\n            assert!(result, \"Database state check failed: {}\", description);\n            \n            println!(\"[DB CHECK] {} - PASSED\", description);\n        } else {\n            println!(\"[DB CHECK] Skipped (no database connection): {}\", description);\n        }\n    }\n    \n    /// Dump database state for logging\n    fn dump_db_state(\u0026self, db: \u0026Connection) -\u003e String {\n        let mut state = String::new();\n        \n        // Count skills\n        if let Ok(count) = db.query_row::\u003ci64, _, _\u003e(\n            \"SELECT COUNT(*) FROM skills\", [], |r| r.get(0)\n        ) {\n            state.push_str(\u0026format!(\"skills={} \", count));\n        }\n        \n        // Count indexes\n        if let Ok(count) = db.query_row::\u003ci64, _, _\u003e(\n            \"SELECT COUNT(*) FROM search_index\", [], |r| r.get(0)\n        ) {\n            state.push_str(\u0026format!(\"indexed={} \", count));\n        }\n        \n        state\n    }\n}\n\nimpl Drop for TestFixture {\n    fn drop(\u0026mut self) {\n        let elapsed = self.start_time.elapsed();\n        println!(\"\\n{'='.repeat(70)}\");\n        println!(\"[FIXTURE] Test complete: {}\", self.test_name);\n        println!(\"[FIXTURE] Total time: {:?}\", elapsed);\n        println!(\"[FIXTURE] Cleaning up: {:?}\", self.temp_dir.path());\n        println!(\"{'='.repeat(70)}\\n\");\n    }\n}\n\n/// Test skill definition\npub struct TestSkill {\n    pub name: String,\n    pub content: String,\n}\n\nimpl TestSkill {\n    pub fn new(name: \u0026str, description: \u0026str) -\u003e Self {\n        let content = format!(r#\"---\nname: {}\ndescription: {}\ntags: [test]\n---\n\n# {}\n\n{}\n\"#, name, description, name, description);\n        \n        Self {\n            name: name.to_string(),\n            content,\n        }\n    }\n    \n    pub fn with_content(name: \u0026str, content: \u0026str) -\u003e Self {\n        Self {\n            name: name.to_string(),\n            content: content.to_string(),\n        }\n    }\n}\n\n/// Command output structure\npub struct CommandOutput {\n    pub success: bool,\n    pub exit_code: i32,\n    pub stdout: String,\n    pub stderr: String,\n    pub elapsed: std::time::Duration,\n}\n```\n\n### 2. CLI Command Tests\n\nCreate `tests/integration/cli_tests.rs`:\n\n```rust\nuse crate::fixture::{TestFixture, TestSkill};\n\n#[tokio::test]\nasync fn test_init_creates_config() {\n    let fixture = TestFixture::new(\"test_init_creates_config\").await;\n    \n    let output = fixture.run_ms(\u0026[\"init\"]).await;\n    \n    assert!(output.success, \"init command failed\");\n    assert!(fixture.config_dir.join(\"config.toml\").exists(), \"config.toml not created\");\n    \n    // Verify config content\n    let config_content = std::fs::read_to_string(fixture.config_dir.join(\"config.toml\"))\n        .expect(\"Failed to read config\");\n    assert!(config_content.contains(\"[general]\"), \"config missing [general] section\");\n}\n\n#[tokio::test]\nasync fn test_init_idempotent() {\n    let fixture = TestFixture::new(\"test_init_idempotent\").await;\n    \n    // Run init twice\n    let output1 = fixture.run_ms(\u0026[\"init\"]).await;\n    let output2 = fixture.run_ms(\u0026[\"init\"]).await;\n    \n    assert!(output1.success, \"first init failed\");\n    assert!(output2.success, \"second init failed\");\n    \n    // Should not error, config should exist\n    assert!(fixture.config_dir.join(\"config.toml\").exists());\n}\n\n#[tokio::test]\nasync fn test_index_empty_directory() {\n    let fixture = TestFixture::new(\"test_index_empty_directory\").await;\n    \n    let output = fixture.run_ms(\u0026[\"index\"]).await;\n    \n    // Should succeed but report 0 skills\n    assert!(output.success, \"index command failed\");\n    assert!(output.stdout.contains(\"0\") || output.stdout.contains(\"no skills\"));\n}\n\n#[tokio::test]\nasync fn test_index_with_skills() {\n    let skills = vec![\n        TestSkill::new(\"rust-error-handling\", \"Best practices for error handling in Rust\"),\n        TestSkill::new(\"git-workflow\", \"Standard git branching and merging workflow\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_index_with_skills\", \u0026skills).await;\n    \n    // Verify database has skills\n    fixture.verify_db_state(|db| {\n        let count: i64 = db.query_row(\"SELECT COUNT(*) FROM skills\", [], |r| r.get(0))\n            .unwrap_or(0);\n        count == 2\n    }, \"Should have 2 skills indexed\");\n}\n\n#[tokio::test]\nasync fn test_list_shows_indexed_skills() {\n    let skills = vec![\n        TestSkill::new(\"test-skill-1\", \"First test skill\"),\n        TestSkill::new(\"test-skill-2\", \"Second test skill\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_list_shows_indexed_skills\", \u0026skills).await;\n    \n    let output = fixture.run_ms(\u0026[\"list\"]).await;\n    \n    assert!(output.success, \"list command failed\");\n    assert!(output.stdout.contains(\"test-skill-1\"), \"Missing skill-1 in output\");\n    assert!(output.stdout.contains(\"test-skill-2\"), \"Missing skill-2 in output\");\n}\n\n#[tokio::test]\nasync fn test_show_skill_details() {\n    let skills = vec![\n        TestSkill::new(\"detailed-skill\", \"A skill with detailed information\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_show_skill_details\", \u0026skills).await;\n    \n    let output = fixture.run_ms(\u0026[\"show\", \"detailed-skill\"]).await;\n    \n    assert!(output.success, \"show command failed\");\n    assert!(output.stdout.contains(\"detailed-skill\"));\n    assert!(output.stdout.contains(\"detailed information\"));\n}\n\n#[tokio::test]\nasync fn test_show_nonexistent_skill() {\n    let fixture = TestFixture::new(\"test_show_nonexistent_skill\").await;\n    \n    let output = fixture.run_ms(\u0026[\"show\", \"nonexistent-skill\"]).await;\n    \n    assert!(!output.success, \"show should fail for nonexistent skill\");\n    assert!(output.stderr.contains(\"not found\") || output.exit_code != 0);\n}\n\n#[tokio::test]\nasync fn test_search_finds_matching_skills() {\n    let skills = vec![\n        TestSkill::new(\"rust-async\", \"Asynchronous programming patterns in Rust\"),\n        TestSkill::new(\"python-async\", \"Async/await patterns in Python\"),\n        TestSkill::new(\"git-basics\", \"Basic git commands and workflow\"),\n    ];\n    \n    let fixture = TestFixture::with_indexed_skills(\"test_search_finds_matching_skills\", \u0026skills).await;\n    \n    let output = fixture.run_ms(\u0026[\"search\", \"async\"]).await;\n    \n    assert!(output.success, \"search command failed\");\n    assert!(output.stdout.contains(\"rust-async\"), \"Missing rust-async in results\");\n    assert!(output.stdout.contains(\"python-async\"), \"Missing python-async in results\");\n    assert!(!output.stdout.contains(\"git-basics\"), \"git-basics should not match 'async'\");\n}\n```\n\n### 3. Database State Verification\n\n```rust\n/// Detailed database state checker\npub struct DbStateChecker\u003c'a\u003e {\n    db: \u0026'a Connection,\n}\n\nimpl\u003c'a\u003e DbStateChecker\u003c'a\u003e {\n    pub fn new(db: \u0026'a Connection) -\u003e Self {\n        Self { db }\n    }\n    \n    pub fn skill_count(\u0026self) -\u003e i64 {\n        self.db.query_row(\"SELECT COUNT(*) FROM skills\", [], |r| r.get(0))\n            .unwrap_or(0)\n    }\n    \n    pub fn skill_exists(\u0026self, name: \u0026str) -\u003e bool {\n        self.db.query_row(\n            \"SELECT 1 FROM skills WHERE name = ?\",\n            [name],\n            |_| Ok(true)\n        ).unwrap_or(false)\n    }\n    \n    pub fn skill_indexed(\u0026self, name: \u0026str) -\u003e bool {\n        self.db.query_row(\n            \"SELECT 1 FROM search_index WHERE skill_name = ?\",\n            [name],\n            |_| Ok(true)\n        ).unwrap_or(false)\n    }\n    \n    pub fn log_full_state(\u0026self) {\n        println!(\"\\n[DB FULL STATE]\");\n        println!(\"  Skills: {}\", self.skill_count());\n        \n        // List all skills\n        if let Ok(mut stmt) = self.db.prepare(\"SELECT name, description FROM skills\") {\n            if let Ok(rows) = stmt.query_map([], |row| {\n                Ok((row.get::\u003c_, String\u003e(0)?, row.get::\u003c_, String\u003e(1)?))\n            }) {\n                for row in rows.flatten() {\n                    println!(\"    - {}: {}\", row.0, row.1);\n                }\n            }\n        }\n    }\n}\n```\n\n### 4. Logging Requirements\n\nEvery integration test must log:\n- Command executed with full arguments\n- Exit code\n- stdout (separate from stderr)\n- stderr (separate from stdout)\n- Timing for each command\n- Database state before operation\n- Database state after operation\n\n### 5. Test Organization\n\n```\ntests/\n integration/\n    mod.rs\n    fixture.rs\n    cli_tests.rs\n       init_tests\n       index_tests\n       list_tests\n       show_tests\n       search_tests\n    workflow_tests.rs\n       full_workflow_test\n       error_recovery_test\n    db_state_tests.rs\n```\n\n## Acceptance Criteria\n\n1. [ ] TestFixture struct implemented with all methods\n2. [ ] with_indexed_skills() creates pre-populated test environment\n3. [ ] with_mock_cass() configures CASS mock responses\n4. [ ] All CLI commands have integration tests (init, index, list, show, search)\n5. [ ] Database state verification after each operation\n6. [ ] Detailed logging for all commands and state changes\n7. [ ] Tests use real filesystem (no mocks)\n8. [ ] Tests properly clean up temp directories\n9. [ ] Tests are isolated and can run in parallel\n10. [ ] All tests pass in CI environment\n\n## Dependencies\n\n- meta_skill-14h (CLI Commands) - commands must exist to test","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T22:55:17.640206042-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.833121835-05:00","labels":["framework","integration-tests","testing"],"dependencies":[{"issue_id":"meta_skill-9pr","depends_on_id":"meta_skill-14h","type":"blocks","created_at":"2026-01-13T22:55:22.372589829-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-9pr","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.152207328-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-9r9","title":"[P4] Specific-to-General Transformation","description":"# Specific-to-General Transformation\n\nExtract universal patterns from specific instances.\n\n## Tasks\n1. Implement LLM-powered transformation\n2. Prompt engineering for generalization\n3. Variable extraction (project names  placeholders)\n4. Pattern validation against multiple instances\n5. Confidence scoring\n\n## The Brenner Method (from Section 28.1)\n\u003e \"Don't summarizeextract the generative grammar.\"\n\nKey insight: We're looking for repeatable cognitive moves, not summaries of what happened.\n\n## Transformation Steps\n1. Collect specific instances\n2. Identify invariants (what's always true)\n3. Identify variants (what changes)\n4. Extract the generative pattern\n5. Validate against held-out examples\n\n## Generalization Prompt (from Section 16)\n- Input: Specific session excerpts\n- Output: General pattern/rule\n- Guidelines: Replace specifics with placeholders\n\n## Acceptance Criteria\n- Specific patterns become general rules\n- Variables properly abstracted\n- Validation confirms generalization","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:46.382724536-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:46.382724536-05:00","labels":["llm","phase-4","transformation"],"dependencies":[{"issue_id":"meta_skill-9r9","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:12.965845876-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-aku","title":"CASS Mining: Security Vulnerability Assessment","description":"Deep dive into security vulnerability assessment patterns, API secret exposure, MFA implementation review, authentication patterns. Extract actionable skill patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:16.142464402-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:30:17.927653105-05:00","closed_at":"2026-01-13T18:30:17.927653105-05:00","close_reason":"Section 32 added: Security Vulnerability Assessment Patterns (~1,450 lines covering OWASP categories, crypto security, input validation, authentication, rate limiting, secret management)","labels":["cass-mining"]}
{"id":"meta_skill-ans","title":"[P4] Redaction Pipeline","description":"# Redaction Pipeline\n\nStrip secrets and PII before extraction.\n\n## Tasks\n1. Define RedactionRule patterns\n2. Implement secret detection (API keys, tokens, passwords)\n3. Implement PII detection (emails, names, paths)\n4. Apply redaction to session content\n5. Taint tracking through pipeline\n\n## Redaction Rules (from Section 8.5)\n- API keys: Various formats (sk-..., ghp_..., etc.)\n- Tokens: Bearer tokens, session cookies\n- Passwords: Common patterns\n- Emails: Standard regex\n- Paths: User home directories\n- IPs: Internal IP ranges\n\n## Taint Tracking\n- Content marked as potentially_unsafe\n- Unsafe content NEVER reaches output\n- Audit log of redactions\n\n## Implementation\n```rust\nstruct Redactor {\n    rules: Vec\u003cRedactionRule\u003e,\n}\n\nimpl Redactor {\n    fn redact(\u0026self, content: \u0026str) -\u003e (String, Vec\u003cRedaction\u003e) {\n        // Returns (clean_content, list_of_redactions)\n    }\n}\n```\n\n## Acceptance Criteria\n- Secrets never in output\n- Redactions logged for audit\n- Taint tracking prevents leaks","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:49.71630535-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:49.71630535-05:00","labels":["phase-4","redaction","security"],"dependencies":[{"issue_id":"meta_skill-ans","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:12.993555547-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ans","depends_on_id":"meta_skill-fma","type":"blocks","created_at":"2026-01-13T22:57:37.229150881-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-avs","title":"CASS Mining: Refactoring Patterns","description":"Deep dive into CLI refactoring patterns, clippy-driven improvements, code modernization workflows.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:39.674096363-05:00","created_by":"ubuntu","updated_at":"2026-01-13T21:39:59.283028347-05:00","closed_at":"2026-01-13T21:39:59.283028347-05:00","close_reason":"Completed Section 38: Refactoring Patterns. Covered clippy-driven refactoring, dead code removal, function extraction, code organization patterns, consistency improvements, defensive refactoring, and type system improvements. ~280 lines added to PLAN_TO_MAKE_METASKILL_CLI.md.","labels":["cass-mining"]}
{"id":"meta_skill-b98","title":"[P1] Git Archive Layer","description":"## Overview\n\nImplement the Git archive layer that provides human-readable persistence for skills. Combined with SQLite (dual persistence pattern from mcp_agent_mail), this enables version history, collaborative editing, and sync across machines. Skills are stored as YAML source-of-truth files with deterministically compiled SKILL.md outputs.\n\n## Background \u0026 Rationale\n\n### Why Dual Persistence (SQLite + Git)\n\nThe mcp_agent_mail pattern teaches us that different access patterns need different storage:\n\n| Access Pattern | Best Storage | Reason |\n|----------------|--------------|--------|\n| Fast queries | SQLite | Indexed, sub-ms response |\n| Full-text search | SQLite FTS5 | Built-in BM25 ranking |\n| Version history | Git | Native branching, blame, diff |\n| Human review | Git/YAML | Readable without tools |\n| Sync/backup | Git | Push/pull to remotes |\n| Collaboration | Git | Merge, conflict resolution |\n\n### Why YAML for Source-of-Truth\n\n1. **Human Readable**: Engineers can read/edit without ms tooling\n2. **Diff Friendly**: Clean diffs in code review\n3. **Comments**: Can annotate skill files inline\n4. **Widely Understood**: Every engineer knows YAML\n5. **Round-Trippable**: Parse  modify  serialize preserves structure\n\n### Directory Structure Philosophy\n\nEach skill gets its own directory containing:\n- `skill.spec.yaml` - The source-of-truth structured data\n- `SKILL.md` - Compiled output (deterministic, don't edit directly)\n- `scripts/` - Executable scripts referenced by the skill\n- `references/` - Additional reference documents\n\nThis structure enables:\n- Easy `git clone` to get everything\n- IDE tooling (syntax highlighting for YAML, MD)\n- Script execution without extraction\n- Atomic skill updates (all files committed together)\n\n## Key Data Structures (from Plan Section 3.3)\n\n```rust\nuse git2::{Repository, Signature, Commit, Oid};\nuse std::path::{Path, PathBuf};\n\n/// Git archive manager for skill persistence\npub struct GitArchive {\n    /// The git repository\n    repo: Repository,\n    /// Root path of the archive\n    root: PathBuf,\n    /// Default author signature\n    author: Signature\u003c'static\u003e,\n}\n\nimpl GitArchive {\n    /// Open or initialize git archive at path\n    pub fn open(path: impl AsRef\u003cPath\u003e) -\u003e Result\u003cSelf\u003e {\n        let root = path.as_ref().to_path_buf();\n        \n        let repo = if root.join(\".git\").exists() {\n            Repository::open(\u0026root)?\n        } else {\n            std::fs::create_dir_all(\u0026root)?;\n            let repo = Repository::init(\u0026root)?;\n            \n            // Create initial commit\n            let sig = Signature::now(\"ms\", \"ms@local\")?;\n            let tree_id = repo.index()?.write_tree()?;\n            let tree = repo.find_tree(tree_id)?;\n            repo.commit(Some(\"HEAD\"), \u0026sig, \u0026sig, \"Initial commit\", \u0026tree, \u0026[])?;\n            \n            repo\n        };\n        \n        let author = Signature::now(\"ms\", \"ms@local\")?;\n        \n        Ok(Self { repo, root, author })\n    }\n    \n    /// Get path to skill directory\n    pub fn skill_path(\u0026self, skill_id: \u0026str) -\u003e PathBuf {\n        self.root.join(\"skills\").join(skill_id)\n    }\n    \n    /// Write a skill to the archive\n    pub fn write_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003cOid\u003e {\n        let skill_dir = self.skill_path(\u0026skill.id);\n        std::fs::create_dir_all(\u0026skill_dir)?;\n        \n        // Write skill.spec.yaml\n        let spec_path = skill_dir.join(\"skill.spec.yaml\");\n        let spec_yaml = serde_yaml::to_string(\u0026skill.spec)?;\n        std::fs::write(\u0026spec_path, \u0026spec_yaml)?;\n        \n        // Compile and write SKILL.md\n        let md_path = skill_dir.join(\"SKILL.md\");\n        let compiled = skill.compile_markdown()?;\n        std::fs::write(\u0026md_path, \u0026compiled)?;\n        \n        // Write scripts if any\n        if !skill.assets.scripts.is_empty() {\n            let scripts_dir = skill_dir.join(\"scripts\");\n            std::fs::create_dir_all(\u0026scripts_dir)?;\n            for (name, content) in \u0026skill.assets.scripts {\n                let script_path = scripts_dir.join(name);\n                std::fs::write(\u0026script_path, content)?;\n            }\n        }\n        \n        // Write references if any\n        if !skill.assets.references.is_empty() {\n            let refs_dir = skill_dir.join(\"references\");\n            std::fs::create_dir_all(\u0026refs_dir)?;\n            for (name, content) in \u0026skill.assets.references {\n                let ref_path = refs_dir.join(name);\n                std::fs::write(\u0026ref_path, content)?;\n            }\n        }\n        \n        // Stage and commit\n        self.commit_skill(\u0026skill.id, \"update\")\n    }\n    \n    /// Stage and commit changes for a skill\n    fn commit_skill(\u0026self, skill_id: \u0026str, operation: \u0026str) -\u003e Result\u003cOid\u003e {\n        let mut index = self.repo.index()?;\n        \n        // Add all files in skill directory\n        let skill_dir = format!(\"skills/{}\", skill_id);\n        index.add_all([\u0026skill_dir], git2::IndexAddOption::DEFAULT, None)?;\n        index.write()?;\n        \n        let tree_id = index.write_tree()?;\n        let tree = self.repo.find_tree(tree_id)?;\n        \n        let parent = self.repo.head()?.peel_to_commit()?;\n        let message = format!(\"[ms] {} skill: {}\", operation, skill_id);\n        \n        let commit_id = self.repo.commit(\n            Some(\"HEAD\"),\n            \u0026self.author,\n            \u0026self.author,\n            \u0026message,\n            \u0026tree,\n            \u0026[\u0026parent],\n        )?;\n        \n        tracing::info!(\"Committed skill {} ({}) -\u003e {}\", skill_id, operation, commit_id);\n        Ok(commit_id)\n    }\n    \n    /// Read a skill from the archive\n    pub fn read_skill(\u0026self, skill_id: \u0026str) -\u003e Result\u003cSkill\u003e {\n        let skill_dir = self.skill_path(skill_id);\n        let spec_path = skill_dir.join(\"skill.spec.yaml\");\n        \n        if !spec_path.exists() {\n            return Err(MsError::SkillNotFound(skill_id.to_string()));\n        }\n        \n        let spec_yaml = std::fs::read_to_string(\u0026spec_path)?;\n        let spec: SkillSpec = serde_yaml::from_str(\u0026spec_yaml)?;\n        \n        // Load scripts\n        let mut scripts = HashMap::new();\n        let scripts_dir = skill_dir.join(\"scripts\");\n        if scripts_dir.exists() {\n            for entry in std::fs::read_dir(\u0026scripts_dir)? {\n                let entry = entry?;\n                let name = entry.file_name().to_string_lossy().to_string();\n                let content = std::fs::read_to_string(entry.path())?;\n                scripts.insert(name, content);\n            }\n        }\n        \n        // Load references\n        let mut references = HashMap::new();\n        let refs_dir = skill_dir.join(\"references\");\n        if refs_dir.exists() {\n            for entry in std::fs::read_dir(\u0026refs_dir)? {\n                let entry = entry?;\n                let name = entry.file_name().to_string_lossy().to_string();\n                let content = std::fs::read_to_string(entry.path())?;\n                references.insert(name, content);\n            }\n        }\n        \n        Ok(Skill {\n            id: skill_id.to_string(),\n            spec,\n            assets: SkillAssets { scripts, references },\n            ..Default::default()\n        })\n    }\n    \n    /// Delete a skill from the archive\n    pub fn delete_skill(\u0026self, skill_id: \u0026str) -\u003e Result\u003cOid\u003e {\n        let skill_dir = self.skill_path(skill_id);\n        if skill_dir.exists() {\n            std::fs::remove_dir_all(\u0026skill_dir)?;\n        }\n        self.commit_skill(skill_id, \"delete\")\n    }\n    \n    /// List all skills in the archive\n    pub fn list_skills(\u0026self) -\u003e Result\u003cVec\u003cString\u003e\u003e {\n        let skills_dir = self.root.join(\"skills\");\n        if !skills_dir.exists() {\n            return Ok(vec![]);\n        }\n        \n        let mut skills = vec![];\n        for entry in std::fs::read_dir(\u0026skills_dir)? {\n            let entry = entry?;\n            if entry.file_type()?.is_dir() {\n                let name = entry.file_name().to_string_lossy().to_string();\n                // Only include if skill.spec.yaml exists\n                if entry.path().join(\"skill.spec.yaml\").exists() {\n                    skills.push(name);\n                }\n            }\n        }\n        \n        skills.sort();\n        Ok(skills)\n    }\n    \n    /// Get commit history for a skill\n    pub fn skill_history(\u0026self, skill_id: \u0026str, limit: usize) -\u003e Result\u003cVec\u003cSkillCommit\u003e\u003e {\n        let mut revwalk = self.repo.revwalk()?;\n        revwalk.push_head()?;\n        revwalk.set_sorting(git2::Sort::TIME)?;\n        \n        let skill_path = format!(\"skills/{}\", skill_id);\n        let mut history = vec![];\n        \n        for oid in revwalk.take(limit * 10) {\n            // Check more commits than limit since not all touch this skill\n            let oid = oid?;\n            let commit = self.repo.find_commit(oid)?;\n            \n            // Check if commit touches this skill\n            if commit.message().map(|m| m.contains(skill_id)).unwrap_or(false) {\n                history.push(SkillCommit {\n                    oid: oid.to_string(),\n                    message: commit.message().unwrap_or(\"\").to_string(),\n                    author: commit.author().name().unwrap_or(\"\").to_string(),\n                    timestamp: commit.time().seconds(),\n                });\n                \n                if history.len() \u003e= limit {\n                    break;\n                }\n            }\n        }\n        \n        Ok(history)\n    }\n    \n    /// Sync with remote repository\n    pub fn sync(\u0026self, remote_name: \u0026str) -\u003e Result\u003cSyncResult\u003e {\n        let mut remote = self.repo.find_remote(remote_name)?;\n        \n        // Fetch\n        remote.fetch(\u0026[\"main\"], None, None)?;\n        \n        // Get local and remote refs\n        let local_ref = self.repo.head()?.resolve()?.target().unwrap();\n        let remote_ref = self.repo\n            .find_reference(\u0026format!(\"refs/remotes/{}/main\", remote_name))?\n            .target()\n            .unwrap();\n        \n        if local_ref == remote_ref {\n            return Ok(SyncResult::UpToDate);\n        }\n        \n        // Check for divergence\n        let base = self.repo.merge_base(local_ref, remote_ref)?;\n        \n        if base == remote_ref {\n            // Local is ahead, push\n            remote.push(\u0026[\"refs/heads/main:refs/heads/main\"], None)?;\n            Ok(SyncResult::Pushed)\n        } else if base == local_ref {\n            // Remote is ahead, pull\n            let remote_commit = self.repo.find_commit(remote_ref)?;\n            self.repo.reset(remote_commit.as_object(), git2::ResetType::Hard, None)?;\n            Ok(SyncResult::Pulled)\n        } else {\n            // Diverged, need merge\n            Ok(SyncResult::Diverged)\n        }\n    }\n}\n\n/// Result of a skill commit\n#[derive(Debug, Clone)]\npub struct SkillCommit {\n    pub oid: String,\n    pub message: String,\n    pub author: String,\n    pub timestamp: i64,\n}\n\n/// Result of sync operation\n#[derive(Debug, Clone)]\npub enum SyncResult {\n    UpToDate,\n    Pushed,\n    Pulled,\n    Diverged,\n}\n```\n\n## Directory Structure\n\n```\n.ms/\n skills/\n    \u003cskill-id\u003e/\n        skill.spec.yaml  # Source of truth (human-editable)\n        SKILL.md         # Compiled output (generated)\n        scripts/\n           setup.sh\n           validate.py\n        references/\n            api-spec.json\n            architecture.md\n bundles/\n    \u003cbundle-name\u003e/\n        bundle.yaml\n        skills.txt\n config.yaml\n .git/\n```\n\n## Tasks\n\n### Task 1: Git Repository Management\n- [ ] Create `src/git/mod.rs` module\n- [ ] Implement `GitArchive::open()` with init or open\n- [ ] Create initial commit on new repository\n- [ ] Configure default author signature\n- [ ] Handle .gitignore for temporary files\n\n### Task 2: Skill Serialization to YAML\n- [ ] Define YAML schema for skill.spec.yaml\n- [ ] Implement serialization with serde_yaml\n- [ ] Preserve field ordering for clean diffs\n- [ ] Handle multiline strings properly\n- [ ] Add YAML comments for self-documentation\n\n### Task 3: SKILL.md Compilation\n- [ ] Compile SkillSpec to Markdown deterministically\n- [ ] Add warning header about auto-generation\n- [ ] Support all block types (rules, examples, pitfalls, etc.)\n- [ ] Handle conditional blocks\n- [ ] Generate table of contents if requested\n\n### Task 4: Skill Directory Operations\n- [ ] Implement `write_skill()` with full directory creation\n- [ ] Implement `read_skill()` with all assets\n- [ ] Implement `delete_skill()` with cleanup\n- [ ] Handle atomic writes (temp file + rename)\n- [ ] Set appropriate file permissions\n\n### Task 5: Git Commit Operations\n- [ ] Stage skill directory changes\n- [ ] Create commits with descriptive messages\n- [ ] Include operation type (add, update, delete)\n- [ ] Return commit OID for tracking\n- [ ] Support bulk commits for efficiency\n\n### Task 6: Git History and Blame\n- [ ] Implement `skill_history()` for version history\n- [ ] Support filtering by date range\n- [ ] Parse commit messages for operation type\n- [ ] Implement diff between versions\n- [ ] Support rollback to previous version\n\n### Task 7: Git Remote Sync\n- [ ] Implement `sync()` for push/pull\n- [ ] Detect local ahead, remote ahead, or diverged\n- [ ] Handle merge conflicts gracefully\n- [ ] Support multiple remotes\n- [ ] Implement `add_remote()` and `remove_remote()`\n\n### Task 8: Asset Management\n- [ ] Store scripts in scripts/ directory\n- [ ] Store references in references/ directory\n- [ ] Make scripts executable on Unix\n- [ ] Validate script shebang lines\n- [ ] Support binary assets (images, etc.)\n\n## Acceptance Criteria\n\n1. **YAML Readable**: skill.spec.yaml is human-readable and editable\n2. **Deterministic Compile**: Same spec always produces same SKILL.md\n3. **Git History**: All changes tracked in git\n4. **Atomic Commits**: Each skill change is one commit\n5. **Round-Trip**: Read  modify  write preserves structure\n6. **Sync Works**: Push/pull with remote repositories\n7. **Assets Included**: Scripts and references stored alongside\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::tempdir;\n    \n    #[test]\n    fn test_archive_initialization() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        // Should have .git directory\n        assert!(dir.path().join(\".git\").exists());\n        \n        // Should have initial commit\n        let head = archive.repo.head().unwrap();\n        assert!(head.peel_to_commit().is_ok());\n    }\n    \n    #[test]\n    fn test_skill_write_read_roundtrip() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        let skill = Skill {\n            id: \"test-skill\".into(),\n            spec: SkillSpec {\n                name: \"Test Skill\".into(),\n                description: \"A test skill for unit testing\".into(),\n                ..Default::default()\n            },\n            ..Default::default()\n        };\n        \n        // Write\n        archive.write_skill(\u0026skill).unwrap();\n        \n        // Verify files exist\n        let skill_dir = dir.path().join(\"skills\").join(\"test-skill\");\n        assert!(skill_dir.join(\"skill.spec.yaml\").exists());\n        assert!(skill_dir.join(\"SKILL.md\").exists());\n        \n        // Read back\n        let loaded = archive.read_skill(\"test-skill\").unwrap();\n        assert_eq!(loaded.spec.name, \"Test Skill\");\n    }\n    \n    #[test]\n    fn test_skill_yaml_format() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        let skill = Skill {\n            id: \"yaml-test\".into(),\n            spec: SkillSpec {\n                name: \"YAML Test\".into(),\n                description: \"Testing YAML output format\".into(),\n                ..Default::default()\n            },\n            ..Default::default()\n        };\n        \n        archive.write_skill(\u0026skill).unwrap();\n        \n        let yaml_content = std::fs::read_to_string(\n            dir.path().join(\"skills/yaml-test/skill.spec.yaml\")\n        ).unwrap();\n        \n        // Should be readable YAML\n        assert!(yaml_content.contains(\"name: YAML Test\"));\n        assert!(yaml_content.contains(\"description:\"));\n    }\n    \n    #[test]\n    fn test_skill_deletion() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        let skill = Skill {\n            id: \"delete-me\".into(),\n            spec: SkillSpec::default(),\n            ..Default::default()\n        };\n        \n        archive.write_skill(\u0026skill).unwrap();\n        assert!(archive.skill_path(\"delete-me\").exists());\n        \n        archive.delete_skill(\"delete-me\").unwrap();\n        assert!(!archive.skill_path(\"delete-me\").exists());\n    }\n    \n    #[test]\n    fn test_skill_history() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        let mut skill = Skill {\n            id: \"history-test\".into(),\n            spec: SkillSpec {\n                name: \"Version 1\".into(),\n                ..Default::default()\n            },\n            ..Default::default()\n        };\n        \n        archive.write_skill(\u0026skill).unwrap();\n        \n        skill.spec.name = \"Version 2\".into();\n        archive.write_skill(\u0026skill).unwrap();\n        \n        let history = archive.skill_history(\"history-test\", 10).unwrap();\n        assert_eq!(history.len(), 2);\n        assert!(history[0].message.contains(\"history-test\"));\n    }\n    \n    #[test]\n    fn test_list_skills() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        for i in 0..3 {\n            let skill = Skill {\n                id: format!(\"skill-{}\", i),\n                spec: SkillSpec::default(),\n                ..Default::default()\n            };\n            archive.write_skill(\u0026skill).unwrap();\n        }\n        \n        let skills = archive.list_skills().unwrap();\n        assert_eq!(skills.len(), 3);\n        assert!(skills.contains(\u0026\"skill-0\".to_string()));\n        assert!(skills.contains(\u0026\"skill-1\".to_string()));\n        assert!(skills.contains(\u0026\"skill-2\".to_string()));\n    }\n    \n    #[test]\n    fn test_scripts_stored() {\n        let dir = tempdir().unwrap();\n        let archive = GitArchive::open(dir.path()).unwrap();\n        \n        let mut scripts = HashMap::new();\n        scripts.insert(\"setup.sh\".into(), \"#!/bin/bash\\necho hello\".into());\n        \n        let skill = Skill {\n            id: \"with-scripts\".into(),\n            spec: SkillSpec::default(),\n            assets: SkillAssets {\n                scripts,\n                references: HashMap::new(),\n            },\n            ..Default::default()\n        };\n        \n        archive.write_skill(\u0026skill).unwrap();\n        \n        let script_path = dir.path().join(\"skills/with-scripts/scripts/setup.sh\");\n        assert!(script_path.exists());\n        \n        let content = std::fs::read_to_string(\u0026script_path).unwrap();\n        assert!(content.contains(\"#!/bin/bash\"));\n    }\n}\n```\n\n### Logging Requirements\nAll git operations must log:\n- `DEBUG`: File operations, git staging, commit details\n- `INFO`: Commits created, sync operations, skill count changes\n- `WARN`: Merge conflicts, missing remotes, uncommitted changes\n- `ERROR`: Git operation failures, filesystem errors\n\nExample log output:\n```\n[INFO] Opening git archive at /home/user/.ms/\n[DEBUG] Git repository found, loading existing\n[INFO] Writing skill: rust-debugging\n[DEBUG] Creating directory: /home/user/.ms/skills/rust-debugging/\n[DEBUG] Writing skill.spec.yaml (2.3KB)\n[DEBUG] Compiling SKILL.md (4.1KB)\n[DEBUG] Staging: skills/rust-debugging/*\n[INFO] Committed skill rust-debugging (update) -\u003e 7a8b9c0\n```\n\n## YAML Schema Example\n\n```yaml\n# skill.spec.yaml\nname: Rust Debugging\ndescription: Best practices for debugging Rust applications\nversion: 1.2.0\nlayer: global\n\nmetadata:\n  author: Your Name\n  tags: [rust, debugging, development]\n  requires: [rust-basics]\n  provides: [rust-debugging]\n\nsections:\n  - id: critical-rules\n    type: rules\n    content:\n      - Always check compiler errors first\n      - Use RUST_BACKTRACE=1 for stack traces\n      - Enable debug symbols in Cargo.toml\n\n  - id: examples\n    type: examples\n    content:\n      - title: Using dbg! macro\n        code: |\n          let value = dbg!(compute_something());\n          // Prints: [src/main.rs:10] compute_something() = 42\n\n  - id: pitfalls\n    type: pitfalls\n    content:\n      - description: Forgetting to enable debug profile\n        fix: Add [profile.dev] debug = true to Cargo.toml\n```\n\n## References\n\n- Plan Section 3.3: Git Archive Structure\n- Plan Section 3.5: Layering and Conflict Resolution (overlay files)\n- Plan Section 3.6: Skill Spec and Deterministic Compilation\n- mcp_agent_mail dual persistence pattern\n- Depends on: meta_skill-5s0 (Rust Project Scaffolding)\n- Blocks: meta_skill-fus (Two-Phase Commit), meta_skill-6fi (Bundle Format)\n\nLabels: [git persistence phase-1]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:01.489461268-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:40:54.146300898-05:00","labels":["git","persistence","phase-1"],"dependencies":[{"issue_id":"meta_skill-b98","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.82314122-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-c98","title":"[P6] Skill Templates Library","description":"# Skill Templates Library\n\nPre-built patterns for rapid skill creation.\n\n## Tasks\n1. Define template format\n2. Built-in templates (workflow, checklist, debugging, etc.)\n3. Template instantiation engine\n4. Custom template support\n5. Template discovery from sessions\n\n## Built-in Templates (from Section 19)\n- Workflow: Step-by-step procedures\n- Checklist: Verification lists\n- Debugging: Diagnostic flows\n- Integration: API/tool setup\n- Pattern: Design pattern documentation\n\n## Template Format\n```yaml\nid: workflow\nname: Workflow Template\ndescription: Step-by-step procedure template\nvariables:\n  - name: workflow_name\n    description: Name of the workflow\n    required: true\n  - name: steps\n    description: List of steps\n    type: array\ncontent: |\n  # {{workflow_name}}\n  \n  ## Steps\n  {{#each steps}}\n  {{@index}}. {{this}}\n  {{/each}}\n```\n\n## CLI\n- `ms template list` - Show available templates\n- `ms template use \u003cid\u003e` - Interactive instantiation\n- `ms template create` - Create custom template\n- `ms template discover` - Find patterns in sessions\n\n## Acceptance Criteria\n- Templates instantiate correctly\n- Variables substituted\n- Custom templates supported","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:26.843243723-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:28:26.843243723-05:00","labels":["authoring","phase-6","templates"],"dependencies":[{"issue_id":"meta_skill-c98","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:28:37.174615516-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-cbx","title":"CASS Mining: Testing Patterns","description":"Deep dive into Vitest, Testing Library, unit test patterns, integration test methodologies, property-based testing.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:40.328802502-05:00","created_by":"ubuntu","updated_at":"2026-01-13T20:12:49.25841031-05:00","closed_at":"2026-01-13T20:12:49.25841031-05:00","close_reason":"Added Section 34: Testing Patterns and Methodology (~915 lines). Covers NO mocks philosophy, test organization patterns (JS/TS/Go), fixtures with temp directories, property-based testing with proptest, coverage analysis, snapshot testing, E2E with Playwright, BATS for shell testing, clipboard testing, test harness patterns, and CI integration.","labels":["cass-mining"]}
{"id":"meta_skill-ch6","title":"[P2] Hash Embeddings (xf-style)","description":"# Hash Embeddings (xf-style)\n\nZero-dependency semantic embeddings using FNV-1a hashing.\n\n## Tasks\n1. Port hash_embed algorithm from xf\n2. Generate 384-dimensional vectors\n3. Normalize vectors to unit length\n4. Store embeddings in SQLite (skill_embeddings table)\n5. Cosine similarity search\n6. Optional: pluggable ML embedder for higher fidelity\n\n## Algorithm (from Section 6.2)\n1. Tokenize text into words\n2. For each word, hash to seed random generator\n3. Generate sparse vector, add to accumulator\n4. Normalize final vector\n\n## Why Hash Embeddings (from Section 13.1)\n| ML Embeddings | Hash Embeddings |\n|---------------|-----------------|\n| 100MB+ model download | Zero dependencies |\n| GPU/CPU overhead | Pure CPU, instant |\n| Version lock-in | Always reproducible |\n| Network dependency | Fully offline |\n\n80-90% of ML embedding quality with none of the operational complexity.\n\n## Acceptance Criteria\n- Embeddings generated deterministically\n- Cosine similarity works correctly\n- No external dependencies","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:03.391012411-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:23:03.391012411-05:00","labels":["embeddings","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-ch6","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:23:13.492169072-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-cn4","title":"Block-Level Overlays","description":"## Overview\n\nImplement block-level overlays for meta_skill (Section 3.5 of PLAN_TO_MAKE_METASKILL_CLI.md). Overlay files patch specific block IDs without copying entire skills, enabling surgical policy additions and customizations.\n\n## Background \u0026 Rationale\n\n### The Problem with Full Skill Copies\nWhen users want to customize a skill, the naive approach is to copy the entire skill to a higher layer. This creates problems:\n- **Maintenance Burden**: When the base skill updates, the copy is outdated\n- **Merge Conflicts**: No clear way to incorporate upstream changes\n- **Bloat**: Duplicating large skills for small changes wastes space\n- **Unclear Intent**: Hard to see what actually changed\n\n### The Overlay Solution\nOverlays are small patch files that declare:\n- Which skill they modify\n- Which blocks to add, replace, remove, or append to\n- What the new content should be\n\nBenefits:\n- **Minimal Footprint**: Only specify what changes\n- **Clear Intent**: Overlay file shows exactly what's different\n- **Automatic Updates**: Base skill updates flow through automatically\n- **Composable**: Multiple overlays can stack\n\n### Example Use Case\nA company wants to add a security reminder to the \"api-design\" skill without maintaining a full copy:\n\n```toml\n# ~/.config/ms/overlays/api-design-security.overlay\nskill_id = \"api-design\"\nlayer = \"global\"\n\n[[operations]]\ntype = \"add\"\nblock_id = \"security-reminder\"\nafter = \"best-practices\"\ncontent = \"\"\"\n## Security Reminder\nAll API endpoints MUST:\n- Validate input parameters\n- Use authentication\n- Log access attempts\n\"\"\"\n```\n\n## Key Data Structures (from Plan Section 3.5)\n\n```rust\n/// An overlay that patches a specific skill\nstruct SkillOverlay {\n    /// The skill this overlay patches\n    skill_id: String,\n    /// The layer this overlay exists in\n    layer: SkillLayer,\n    /// Ordered list of patch operations\n    operations: Vec\u003cOverlayOp\u003e,\n    /// Optional: only apply if condition is met\n    condition: Option\u003cOverlayCondition\u003e,\n    /// Metadata about the overlay\n    metadata: OverlayMetadata,\n}\n\n/// A single overlay operation\nenum OverlayOp {\n    /// Add a new block to the skill\n    Add {\n        block_id: String,\n        content: String,\n        /// Where to insert: after this block ID\n        after: Option\u003cString\u003e,\n        /// Where to insert: before this block ID\n        before: Option\u003cString\u003e,\n        /// Block type (section, example, tip, etc.)\n        block_type: BlockType,\n    },\n    /// Replace an existing block entirely\n    Replace {\n        block_id: String,\n        content: String,\n    },\n    /// Remove a block from the skill\n    Remove {\n        block_id: String,\n    },\n    /// Append content to an existing block\n    AppendTo {\n        block_id: String,\n        /// Items to append (e.g., list items, paragraphs)\n        items: Vec\u003cString\u003e,\n        /// Separator between existing and new content\n        separator: Option\u003cString\u003e,\n    },\n    /// Prepend content to an existing block\n    PrependTo {\n        block_id: String,\n        items: Vec\u003cString\u003e,\n        separator: Option\u003cString\u003e,\n    },\n    /// Modify block metadata without changing content\n    UpdateMetadata {\n        block_id: String,\n        updates: HashMap\u003cString, String\u003e,\n    },\n}\n\n/// Conditions for when to apply an overlay\nenum OverlayCondition {\n    /// Only apply in certain environments\n    Environment(String),\n    /// Only apply if a feature flag is set\n    FeatureFlag(String),\n    /// Only apply if another skill is loaded\n    SkillLoaded(String),\n    /// Combine conditions with AND\n    All(Vec\u003cOverlayCondition\u003e),\n    /// Combine conditions with OR\n    Any(Vec\u003cOverlayCondition\u003e),\n}\n\n/// Metadata about an overlay\nstruct OverlayMetadata {\n    /// Human-readable description of what this overlay does\n    description: String,\n    /// Who created this overlay\n    author: Option\u003cString\u003e,\n    /// Version of the overlay\n    version: Option\u003cVersion\u003e,\n    /// Minimum skill version this is compatible with\n    min_skill_version: Option\u003cVersion\u003e,\n}\n\n/// Result of applying an overlay\nstruct OverlayApplicationResult {\n    /// The skill ID that was modified\n    skill_id: String,\n    /// Operations that succeeded\n    applied: Vec\u003cOverlayOpResult\u003e,\n    /// Operations that failed (with reasons)\n    failed: Vec\u003cOverlayOpFailure\u003e,\n    /// Warnings (e.g., deprecated block IDs)\n    warnings: Vec\u003cString\u003e,\n}\n\n/// Result of a single operation\nstruct OverlayOpResult {\n    /// The operation that was applied\n    operation: OverlayOp,\n    /// Block IDs affected\n    affected_blocks: Vec\u003cString\u003e,\n}\n\n/// Failure information for an operation\nstruct OverlayOpFailure {\n    /// The operation that failed\n    operation: OverlayOp,\n    /// Why it failed\n    reason: OverlayError,\n}\n\n/// Possible overlay errors\nenum OverlayError {\n    /// Target block doesn't exist\n    BlockNotFound(String),\n    /// Target skill doesn't exist\n    SkillNotFound(String),\n    /// Block ID already exists (for Add)\n    BlockAlreadyExists(String),\n    /// Invalid content format\n    InvalidContent(String),\n    /// Condition not met\n    ConditionNotMet(OverlayCondition),\n    /// Version incompatibility\n    VersionMismatch { required: Version, actual: Version },\n}\n```\n\n## Tasks\n\n### Task 1: Define Overlay Types\n- [ ] Create `src/overlays/mod.rs` module\n- [ ] Define `SkillOverlay` struct\n- [ ] Define `OverlayOp` enum with all variants\n- [ ] Define `OverlayCondition` enum\n- [ ] Define `OverlayMetadata` struct\n- [ ] Define result and error types\n- [ ] Add comprehensive documentation\n\n### Task 2: Implement Overlay Parsing\n- [ ] Create TOML parser for `.overlay` files\n- [ ] Parse overlay header (skill_id, layer, metadata)\n- [ ] Parse operations array\n- [ ] Parse conditions\n- [ ] Validate overlay structure on parse\n- [ ] Return helpful parse errors with line numbers\n\n### Task 3: Implement Add Operation\n- [ ] Insert new block at specified position\n- [ ] Support `after` positioning (after specific block)\n- [ ] Support `before` positioning (before specific block)\n- [ ] Default to end if no position specified\n- [ ] Validate block_id doesn't already exist\n- [ ] Log block addition details\n\n### Task 4: Implement Replace Operation\n- [ ] Find target block by ID\n- [ ] Replace content entirely\n- [ ] Preserve block type unless explicitly changed\n- [ ] Error if block doesn't exist\n- [ ] Log original vs new content hash\n\n### Task 5: Implement Remove Operation\n- [ ] Find target block by ID\n- [ ] Remove block from skill\n- [ ] Handle references to removed block\n- [ ] Error if block doesn't exist\n- [ ] Log removed block details\n\n### Task 6: Implement AppendTo/PrependTo Operations\n- [ ] Find target block by ID\n- [ ] Append/prepend items to existing content\n- [ ] Support configurable separator\n- [ ] Handle list blocks specially (add list items)\n- [ ] Log appended/prepended content\n\n### Task 7: Implement Condition Evaluation\n- [ ] Implement `Environment` condition check\n- [ ] Implement `FeatureFlag` condition check\n- [ ] Implement `SkillLoaded` condition check\n- [ ] Implement `All` combinator (AND logic)\n- [ ] Implement `Any` combinator (OR logic)\n- [ ] Log condition evaluation results\n\n### Task 8: Implement Overlay Application Engine\n- [ ] Create `apply_overlay(skill, overlay)` function\n- [ ] Execute operations in order\n- [ ] Collect results for each operation\n- [ ] Continue on recoverable errors\n- [ ] Return comprehensive `OverlayApplicationResult`\n- [ ] Support dry-run mode for preview\n\n### Task 9: Implement Overlay Discovery\n- [ ] Scan overlay directories for `.overlay` files\n- [ ] Group overlays by target skill\n- [ ] Order overlays by layer priority\n- [ ] Handle overlay conflicts (same block in multiple overlays)\n- [ ] Log discovered overlays\n\n### Task 10: Integration with LayeredRegistry\n- [ ] Apply overlays during skill resolution\n- [ ] Apply overlays in layer order (system -\u003e session)\n- [ ] Cache applied overlay results\n- [ ] Support `--no-overlays` flag\n- [ ] Track which overlays are applied to each skill\n\n## Acceptance Criteria\n\n1. **Parsing**: Can parse all overlay file formats\n2. **Add Operation**: Adds blocks at correct positions\n3. **Replace Operation**: Replaces blocks correctly\n4. **Remove Operation**: Removes blocks cleanly\n5. **Append/Prepend**: Modifies existing blocks correctly\n6. **Conditions**: Evaluates all condition types\n7. **Error Handling**: Clear errors for invalid operations\n8. **Layer Order**: Overlays apply in correct layer order\n9. **Dry Run**: Can preview overlay effects\n10. **Performance**: Overlay application adds \u003c5ms per skill\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[test]\nfn test_parse_overlay_file() {\n    let overlay_toml = r#\"\n        skill_id = \"api-design\"\n        layer = \"global\"\n        \n        [metadata]\n        description = \"Add security section\"\n        \n        [[operations]]\n        type = \"add\"\n        block_id = \"security\"\n        after = \"best-practices\"\n        content = \"## Security\\nAlways validate input.\"\n    \"#;\n    \n    let overlay = SkillOverlay::from_toml(overlay_toml).unwrap();\n    assert_eq!(overlay.skill_id, \"api-design\");\n    assert_eq!(overlay.layer, SkillLayer::Global);\n    assert_eq!(overlay.operations.len(), 1);\n}\n\n#[test]\nfn test_add_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"intro\", \"Introduction content\");\n    skill.add_block(\"conclusion\", \"Conclusion content\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Add {\n                block_id: \"middle\".into(),\n                content: \"Middle content\".into(),\n                after: Some(\"intro\".into()),\n                before: None,\n                block_type: BlockType::Section,\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let result = apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    assert_eq!(result.applied.len(), 1);\n    \n    let blocks: Vec\u003c_\u003e = skill.blocks().map(|b| b.id()).collect();\n    assert_eq!(blocks, vec![\"intro\", \"middle\", \"conclusion\"]);\n}\n\n#[test]\nfn test_replace_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"target\", \"Original content\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"target\".into(),\n                content: \"Replaced content\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    assert_eq!(skill.get_block(\"target\").unwrap().content(), \"Replaced content\");\n}\n\n#[test]\nfn test_remove_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"keep\", \"Keep this\");\n    skill.add_block(\"remove\", \"Remove this\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Remove { block_id: \"remove\".into() }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    assert!(skill.get_block(\"remove\").is_none());\n    assert!(skill.get_block(\"keep\").is_some());\n}\n\n#[test]\nfn test_append_to_operation() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"list\", \"- Item 1\\n- Item 2\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::AppendTo {\n                block_id: \"list\".into(),\n                items: vec![\"- Item 3\".into(), \"- Item 4\".into()],\n                separator: Some(\"\\n\".into()),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    apply_overlay(\u0026mut skill, \u0026overlay).unwrap();\n    let content = skill.get_block(\"list\").unwrap().content();\n    assert!(content.contains(\"Item 3\"));\n    assert!(content.contains(\"Item 4\"));\n}\n\n#[test]\nfn test_condition_evaluation() {\n    let condition = OverlayCondition::All(vec![\n        OverlayCondition::Environment(\"production\".into()),\n        OverlayCondition::SkillLoaded(\"base-skill\".into()),\n    ]);\n    \n    let mut ctx = EvalContext::new();\n    ctx.set_environment(\"production\");\n    ctx.set_loaded_skills(vec![\"base-skill\".into()]);\n    \n    assert!(condition.evaluate(\u0026ctx));\n    \n    ctx.set_environment(\"development\");\n    assert!(!condition.evaluate(\u0026ctx));\n}\n\n#[test]\nfn test_overlay_error_block_not_found() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"nonexistent\".into(),\n                content: \"New content\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let result = apply_overlay(\u0026mut skill, \u0026overlay);\n    assert!(result.is_err());\n    assert!(matches!(\n        result.unwrap_err(),\n        OverlayError::BlockNotFound(_)\n    ));\n}\n\n#[test]\nfn test_multiple_overlays_layer_order() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"content\", \"Original\");\n    \n    let system_overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::System,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"content\".into(),\n                content: \"System version\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let project_overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Project,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"content\".into(),\n                content: \"Project version\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    // Apply in layer order\n    apply_overlay(\u0026mut skill, \u0026system_overlay).unwrap();\n    apply_overlay(\u0026mut skill, \u0026project_overlay).unwrap();\n    \n    // Project (higher layer) should win\n    assert_eq!(skill.get_block(\"content\").unwrap().content(), \"Project version\");\n}\n\n#[test]\nfn test_dry_run_mode() {\n    let mut skill = SkillSpec::new(\"test-skill\", \"Test\");\n    skill.add_block(\"content\", \"Original\");\n    \n    let overlay = SkillOverlay {\n        skill_id: \"test-skill\".into(),\n        layer: SkillLayer::Global,\n        operations: vec![\n            OverlayOp::Replace {\n                block_id: \"content\".into(),\n                content: \"Modified\".into(),\n            }\n        ],\n        condition: None,\n        metadata: OverlayMetadata::default(),\n    };\n    \n    let result = preview_overlay(\u0026skill, \u0026overlay).unwrap();\n    \n    // Original skill unchanged\n    assert_eq!(skill.get_block(\"content\").unwrap().content(), \"Original\");\n    \n    // Preview shows what would change\n    assert_eq!(result.would_modify.len(), 1);\n    assert_eq!(result.would_modify[0].block_id, \"content\");\n    assert_eq!(result.would_modify[0].new_content, \"Modified\");\n}\n```\n\n### Logging Requirements\nAll operations must log with appropriate levels:\n- `DEBUG`: Parse steps, operation execution details\n- `INFO`: Overlay application results, blocks modified\n- `WARN`: Deprecated block IDs, version mismatches\n- `ERROR`: Failed operations, invalid overlays\n\nExample log output:\n```\n[INFO] Discovering overlays in /home/user/.config/ms/overlays/\n[DEBUG] Found overlay: api-design-security.overlay\n[DEBUG] Parsing overlay for skill 'api-design'\n[INFO] Applying overlay 'api-design-security' to skill 'api-design'\n[DEBUG] Executing Add operation: block_id='security', after='best-practices'\n[DEBUG] Block 'security' inserted at position 3\n[INFO] Overlay applied successfully: 1 operation, 1 block affected\n[DEBUG] Overlay application took 2ms\n```\n\n## File Format Reference\n\n### Basic Overlay File\n```toml\n# skill-name.overlay\nskill_id = \"skill-name\"\nlayer = \"global\"\n\n[metadata]\ndescription = \"What this overlay does\"\nauthor = \"Your Name\"\nversion = \"1.0.0\"\n\n[[operations]]\ntype = \"add\"\nblock_id = \"new-section\"\nafter = \"existing-section\"\ncontent = \"\"\"\n## New Section\nContent here.\n\"\"\"\n\n[[operations]]\ntype = \"replace\"\nblock_id = \"old-section\"\ncontent = \"\"\"\n## Updated Section\nNew content.\n\"\"\"\n```\n\n### Conditional Overlay\n```toml\nskill_id = \"production-skill\"\nlayer = \"project\"\n\n[condition]\ntype = \"all\"\nconditions = [\n    { type = \"environment\", value = \"production\" },\n    { type = \"skill_loaded\", value = \"security-base\" }\n]\n\n[[operations]]\ntype = \"add\"\nblock_id = \"prod-warning\"\nbefore = \"intro\"\ncontent = \"\"\"\n\u003e **Production Environment**\n\u003e Extra care required.\n\"\"\"\n```\n\n## References\n\n- Plan Section 3.5: Layering \u0026 Conflict Resolution (overlay subsection)\n- Plan Section 3.1: Block structure and IDs\n- Depends on: meta_skill-225 (Skill Layering \u0026 Conflict Resolution)\n- Blocks: meta_skill-7va (ms load Command)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:53:53.125727762-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:53:53.125727762-05:00","labels":["layers","overlays","phase-1"],"dependencies":[{"issue_id":"meta_skill-cn4","depends_on_id":"meta_skill-225","type":"blocks","created_at":"2026-01-13T22:54:05.254837937-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-dag","title":"CASS Mining: Error Handling Patterns (anyhow/thiserror)","description":"Deep dive into anyhow::Result patterns, custom error types, robot-friendly structured output formats, error propagation best practices.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:27.956747962-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:42:47.226031489-05:00","closed_at":"2026-01-13T18:42:47.226031489-05:00","close_reason":"Completed Section 33: Error Handling Patterns and Methodology (~860 lines). Covers thiserror/anyhow dichotomy, structured CLI errors, error taxonomy, context chaining, retry with backoff, circuit breakers, panic vs Result guidelines, error boundaries, and logging best practices.","labels":["cass-mining"]}
{"id":"meta_skill-e5e","title":"Skill Quality Scoring Algorithm","description":"# Skill Quality Scoring Algorithm\n\n## Section Reference\nSection 7.4 - Skill Quality Scoring Algorithm\n\n## Overview\n\nQuality scoring determines which skills are most worth surfacing to agents. This implements a multi-factor scoring algorithm that combines structure analysis, content quality, provenance (evidence coverage and confidence), usage metrics, and toolchain compatibility.\n\n## Why Quality Scoring Matters\n\nNot all skills are equally useful:\n- Some may be outdated or stale\n- Some may lack evidence/provenance\n- Some may have low usage/adoption\n- Some may not match the current tech stack\n\nQuality scoring enables:\n- Prioritizing high-quality skills in search results\n- Filtering out low-quality skills from suggestions\n- Identifying skills that need improvement\n- Automated quality gates for publishing\n\n## Core Data Structures (from Plan)\n\n```rust\n/// Quality scoring system\nstruct QualityScorer {\n    weights: QualityWeights,\n    usage_tracker: UsageTracker,\n    toolchain_detector: ToolchainDetector,\n    project_path: Option\u003cPathBuf\u003e,\n}\n\n/// Configurable weights for quality factors\nstruct QualityWeights {\n    structure_weight: f32,      // Well-formed sections\n    content_weight: f32,        // Completeness and clarity\n    evidence_weight: f32,       // Provenance coverage\n    usage_weight: f32,          // Recent usage frequency\n    toolchain_weight: f32,      // Tech stack match\n    freshness_weight: f32,      // Last update recency\n}\n\nimpl Default for QualityWeights {\n    fn default() -\u003e Self {\n        Self {\n            structure_weight: 0.15,\n            content_weight: 0.25,\n            evidence_weight: 0.20,\n            usage_weight: 0.20,\n            toolchain_weight: 0.10,\n            freshness_weight: 0.10,\n        }\n    }\n}\n\n/// Quality assessment result\nstruct QualityScore {\n    overall: f32,               // 0.0 to 1.0\n    breakdown: QualityBreakdown,\n    issues: Vec\u003cQualityIssue\u003e,\n    suggestions: Vec\u003cString\u003e,\n}\n\nstruct QualityBreakdown {\n    structure: f32,\n    content: f32,\n    evidence: f32,\n    usage: f32,\n    toolchain: f32,\n    freshness: f32,\n}\n```\n\n## Quality Issue Types\n\n```rust\nenum QualityIssue {\n    MissingSection(String),      // Required section absent\n    ShortContent(String, usize), // Section too brief\n    NoExamples,                  // No code examples\n    StaleContent(DateTime),      // Not updated recently\n    LowEvidence(f32),            // Insufficient provenance\n    LowUsage(u32),               // Rarely used\n    ToolchainMismatch(String),   // Tech stack doesn't match\n    NoTags,                      // Missing categorization\n    PoorFormatting,              // Markdown issues\n}\n```\n\n## CLI Integration\n\n```bash\n# Show quality score for a skill\nms quality rust-error-handling\n\n# Show quality scores for all skills\nms quality --all\n\n# Filter search by minimum quality\nms search \"async\" --min-quality 0.7\n\n# Quality report for publishing readiness\nms quality rust-error-handling --report\n```\n\n## Robot Mode Output\n\n```json\n{\n  \"skill_id\": \"rust-error-handling\",\n  \"quality_score\": 0.85,\n  \"breakdown\": {\n    \"structure\": 0.95,\n    \"content\": 0.90,\n    \"evidence\": 0.80,\n    \"usage\": 0.75,\n    \"toolchain\": 1.0,\n    \"freshness\": 0.85\n  },\n  \"issues\": [\n    {\"type\": \"low_evidence\", \"details\": \"Only 3 provenance links\"}\n  ],\n  \"suggestions\": [\n    \"Add more examples\",\n    \"Link to additional evidence\"\n  ]\n}\n```\n\n## Acceptance Criteria\n\n1. [ ] QualityScorer struct with configurable weights\n2. [ ] Structure analysis (required sections present)\n3. [ ] Content analysis (completeness, examples)\n4. [ ] Evidence analysis (provenance coverage)\n5. [ ] Usage analysis (frequency tracking)\n6. [ ] Toolchain compatibility check\n7. [ ] Freshness check (last update time)\n8. [ ] CLI command: ms quality \u003cskill\u003e\n9. [ ] Integration with search filtering\n10. [ ] Robot mode JSON output\n\n## Dependencies\n\n- Depends on: meta_skill-o8o (Context-Aware Suggestions)\n- Depends on: meta_skill-qs1 (SQLite for usage tracking)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:32:59.834298977-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:32:59.834298977-05:00","labels":["phase-3","quality","search"],"dependencies":[{"issue_id":"meta_skill-e5e","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:33:30.949403194-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-e5e","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:33:30.9782025-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-f5n","title":"[P3] Suggestion Cooldowns","description":"# Suggestion Cooldowns\n\nPrevent suggestion spam via context fingerprints.\n\n## Tasks\n1. Define ContextFingerprint (hash of recent context)\n2. Track recently suggested skills per fingerprint\n3. Implement cooldown periods\n4. Decay old fingerprints\n\n## Cooldown Logic (from Section 7.3)\n- Skill suggested  record (skill_id, context_hash, timestamp)\n- Same context + same skill  cooldown 30 minutes\n- Different context  suggest again\n- Explicit dismiss  extended cooldown\n\n## Storage\n```sql\nCREATE TABLE suggestion_cooldowns (\n    skill_id TEXT,\n    context_hash TEXT,\n    suggested_at TIMESTAMP,\n    dismissed BOOLEAN DEFAULT FALSE,\n    PRIMARY KEY (skill_id, context_hash)\n);\n```\n\n## Fingerprint Components\n- Hash of: cwd + sorted(files) + project_type\n- Exclude volatile data (timestamps, etc.)\n\n## Acceptance Criteria\n- Same context doesn't spam suggestions\n- Context change resets cooldown\n- Dismissed skills get longer cooldown","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:24:17.144555618-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:41:58.72973145-05:00","closed_at":"2026-01-13T23:41:58.72973145-05:00","close_reason":"Duplicate of meta_skill-8df (Context Fingerprints \u0026 Suggestion Cooldowns)","labels":["cooldown","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-f5n","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T22:24:25.980432592-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-f8s","title":"CASS Mining: CI/CD Automation Patterns","description":"Deep dive into GitHub Actions workflows (ci.yml, deploy.yml, e2e.yml, dependabot.yml), release automation, pipeline optimization.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:28.647404029-05:00","created_by":"ubuntu","updated_at":"2026-01-13T20:33:12.38309515-05:00","closed_at":"2026-01-13T20:33:12.38309515-05:00","close_reason":"Added Section 35: CI/CD Automation Patterns (~925 lines). CASS mined: repo_updater, apr, jeffreysprompts_premium, flywheel_gateway, destructive_command_guard. Covered: 35.1 GitHub Actions Workflow Architecture (ci.yml 5-job pattern), 35.2 Job Dependencies and Ordering, 35.3 Release Automation (tag-triggered with checksums), 35.4 Version Management (dual storage, semantic comparison), 35.5 Matrix Testing Strategies (OS+runtime matrices), 35.6 Container Image Pipelines (multi-stage Dockerfile, Trivy, SBOM), 35.7 Artifact Management (upload/download/cache patterns), 35.8 Dependabot Configuration, 35.9 Pre-Commit Hook Integration, 35.10 Deployment Workflows (Vercel + smoke tests), 35.11 Quality Gates (lint/type/format/test/build), 35.12 Self-Update Mechanisms (SHA256 verification), 35.13 Application to meta_skill table, 35.14 CI/CD Checklist.","labels":["cass-mining"]}
{"id":"meta_skill-f97","title":"[P4] Anti-Pattern Mining","description":"# Anti-Pattern Mining\n\nExtract \"what NOT to do\" from sessions.\n\n## Tasks\n1. Identify failure sequences\n2. Extract error patterns\n3. Link to eventual solutions\n4. Generate Pitfall slices\n5. Weight by frequency\n\n## Anti-Pattern Structure (from Section 8.9)\n```yaml\npitfall:\n  symptom: \"Module not found error after npm install\"\n  wrong_approach: \"Deleting node_modules and reinstalling\"\n  why_wrong: \"Often masks dependency version conflicts\"\n  correct_approach: \"Check package-lock.json for version mismatches\"\n  evidence:\n    - session: cass-abc123\n      quote: \"Tried deleting node_modules three times...\"\n```\n\n## Detection Heuristics\n- Multiple retries of same command\n- Error followed by different approach followed by success\n- Explicit \"that didn't work\" statements\n- Tool/command switches mid-task\n\n## Counterexample Value\n- Pitfalls are often more valuable than rules\n- Prevent common mistakes\n- Save debugging time\n\n## Acceptance Criteria\n- Anti-patterns extracted\n- Linked to correct approaches\n- Formatted as Pitfall slices","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:26:03.441956729-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:41:50.80848741-05:00","closed_at":"2026-01-13T23:41:50.80848741-05:00","close_reason":"Duplicate of meta_skill-tun (Anti-Pattern Mining)","labels":["anti-patterns","phase-4","pitfalls"],"dependencies":[{"issue_id":"meta_skill-f97","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:26:13.099831725-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-fma","title":"Prompt Injection Defense","description":"## Section Reference\nSection 5.17 - Prompt Injection Defense\n\n## Overview\n\n**CRITICAL**: This feature integrates ACIP (Advanced Cognitive Inoculation Prompt) v1.3 from `/data/projects/acip` as the primary prompt injection defense layer. ACIP is a battle-tested, comprehensive framework specifically designed to protect against sophisticated prompt injection attacks.\n\nRather than building custom detection from scratch, ms leverages ACIP's Cognitive Integrity Framework (CIF) and adapts it for CASS session mining contexts.\n\n## Why ACIP (not custom implementation)\n\n| Aspect | Custom Implementation | ACIP v1.3 |\n|--------|----------------------|-----------|\n| **Maturity** | New, untested | Battle-tested framework |\n| **Attack coverage** | Limited | Comprehensive (direct, indirect, exfiltration, bypass) |\n| **False positive rate** | Unknown | Tuned and documented |\n| **Maintenance** | Must track evolving attacks | Community-maintained |\n| **Audit mode** | Must build | Built-in operator observability |\n| **Domain coverage** | Generic | 6 balanced high-risk domains |\n\n## ACIP Integration Architecture\n\n```rust\n/// ACIP-based injection analyzer for session mining\nstruct AcipSessionAnalyzer {\n    /// ACIP v1.3 config (loaded from /data/projects/acip)\n    acip_config: AcipConfig,\n    /// Local quarantine store\n    quarantine: QuarantineStore,\n    /// Audit mode enabled (maps to ACIP_AUDIT_MODE)\n    audit_mode: bool,\n}\n\n/// Maps ACIP concepts to session mining\nstruct AcipConfig {\n    /// Path to ACIP prompt text\n    acip_prompt_path: PathBuf,\n    /// Version (should be \"1.3\")\n    version: String,\n    /// Trust boundaries for session content\n    trust_boundaries: TrustBoundaryConfig,\n    /// Decision discipline config\n    decision_config: DecisionConfig,\n}\n\n/// Trust boundary configuration per ACIP Section 3\nstruct TrustBoundaryConfig {\n    /// User messages: instructions or data?\n    user_messages: TrustLevel,\n    /// Assistant responses: trusted or verify?\n    assistant_messages: TrustLevel,\n    /// Tool outputs: always untrusted per ACIP\n    tool_outputs: TrustLevel,\n    /// File contents: always untrusted\n    file_contents: TrustLevel,\n}\n\nenum TrustLevel {\n    /// Can be instructions\n    Trusted,\n    /// Data only, never execute\n    Untrusted,\n    /// Verify before trusting\n    VerifyRequired,\n}\n```\n\n## ACIP Threat Model (from v1.3)\n\nACIP defends against:\n1. **Direct prompt injection**  malicious instructions from user\n2. **Indirect prompt injection**  instructions in untrusted content (tool outputs, webpages, documents)\n3. **Data exfiltration**  attempts to extract secrets/policies\n4. **Policy bypass**  encoding, transformation, aggregation attacks\n\n**Session mining specific threats:**\n- Poisoned sessions with embedded injection attempts\n- Payload smuggling in code snippets\n- Recursive injection (instructions to inject into outputs)\n- Multi-turn capability aggregation across session messages\n\n## ACIP Decision Discipline Integration\n\nPer ACIP v1.3 Section \"Decision Discipline\":\n\n```rust\n/// Classification result per ACIP decision framework\nenum AcipClassification {\n    /// Safe to extract patterns from\n    Safe,\n    /// Allowed but needs defensive framing\n    SensitiveAllowed { constraints: Vec\u003cString\u003e },\n    /// Must not extract patterns from\n    Disallowed { category: String, action: String },\n}\n\n/// Decision engine following ACIP discipline\nstruct DecisionEngine {\n    /// Classification logic\n    classifier: Box\u003cdyn Classifier\u003e,\n}\n\nimpl DecisionEngine {\n    /// Step 1: Classification (internal, never disclosed per ACIP)\n    fn classify(\u0026self, content: \u0026SessionContent) -\u003e AcipClassification {\n        // Check for:\n        // - Priority manipulation\n        // - Secret requests\n        // - Exfiltration vectors\n        // - High-risk domain escalation\n        // - Multi-turn drift\n        // - Capability aggregation\n        // - Contextual risk amplification\n        unimplemented!()\n    }\n    \n    /// Step 2: Response construction\n    fn respond(\u0026self, classification: AcipClassification) -\u003e FilterAction {\n        match classification {\n            AcipClassification::Safe =\u003e FilterAction::Extract,\n            AcipClassification::SensitiveAllowed { constraints } =\u003e {\n                FilterAction::ExtractWithConstraints(constraints)\n            }\n            AcipClassification::Disallowed { .. } =\u003e {\n                FilterAction::Quarantine\n            }\n        }\n    }\n}\n```\n\n## Audit Mode (per ACIP v1.3)\n\nACIP v1.3 includes operator audit mode for observability without oracle leakage:\n\n```rust\n/// Audit tag format per ACIP spec\nstruct AcipAuditTag {\n    action: AuditAction,\n    category: AuditCategory,\n    source: AuditSource,\n    turn: usize,\n}\n\nenum AuditAction {\n    Denied,\n    Filtered,\n    Escalated,\n}\n\nenum AuditCategory {\n    Injection,\n    Exfiltration,\n    Bypass,\n    HighRisk,\n    Aggregation,\n    Drift,\n    CovertChannel,\n}\n\nenum AuditSource {\n    Direct,\n    Indirect,\n    Tool,\n    MultiTurn,\n}\n\n/// Enable audit mode for ms operations\nfn enable_audit_mode() {\n    // Set ACIP_AUDIT_MODE=ENABLED in context\n    // Audit tags appended to filter reports\n}\n```\n\n## Forensic Quarantine (enhanced with ACIP)\n\n```rust\n/// Quarantine store with ACIP metadata\nstruct QuarantineStore {\n    items: Vec\u003cQuarantinedItem\u003e,\n    by_session: HashMap\u003cSessionId, Vec\u003cQuarantineId\u003e\u003e,\n    path: PathBuf,\n}\n\nstruct QuarantinedItem {\n    id: QuarantineId,\n    /// Hash of original content\n    content_hash: ContentHash,\n    /// Safe excerpt (heavily redacted per ACIP oracle prevention)\n    safe_excerpt: String,\n    /// ACIP classification\n    acip_classification: AcipClassification,\n    /// ACIP audit tag if audit mode was on\n    audit_tag: Option\u003cAcipAuditTag\u003e,\n    /// Session reference\n    session_id: SessionId,\n    message_index: usize,\n    quarantined_at: DateTime\u003cUtc\u003e,\n    /// Replay command (requires explicit invocation)\n    replay_command: String,\n}\n```\n\n## Detection Rules (leveraging ACIP categories)\n\nInstead of custom rules, leverage ACIP's CIF rules:\n\n```rust\n/// ACIP-derived detection categories\nenum AcipDetectionCategory {\n    /// Per CIF Section 2: Anticipatory Threat Recognition\n    SemanticReframing,\n    IndirectTasking,\n    HypotheticalExtraction,\n    AuthorityLaundering,\n    UrgencyFraming,\n    MoralCoercion,\n    IndirectInjection,\n    ExfiltrationAttempt,\n    /// Per CIF Section 3: Instruction-Source Separation\n    InstructionDataConfusion,\n    /// Per CIF Section 4: Semantic Isolation\n    SynonymSubstitution,\n    NegationReversal,\n    ImplicitAssumption,\n    PhraseReordering,\n    /// Per ACIP v1.3 additions\n    CapabilityAggregation,\n    CovertChannel,\n    MultiTurnDrift,\n}\n```\n\n## CLI Commands\n\n```bash\n# Scan sessions for injection attempts (uses ACIP)\nms security scan\nms security scan --session \u003csession-id\u003e\nms security scan --audit-mode  # Enable ACIP audit tags\n\n# View quarantine\nms security quarantine list\nms security quarantine show \u003cquarantine-id\u003e\n\n# Review quarantined items\nms security quarantine review \u003cquarantine-id\u003e --confirm-injection\nms security quarantine review \u003cquarantine-id\u003e --false-positive --reason \"...\"\n\n# Replay (requires explicit permission per ACIP trust boundaries)\nms security quarantine replay \u003cquarantine-id\u003e --i-understand-the-risks\n\n# View ACIP config\nms security acip status\nms security acip config\nms security acip version\n\n# Test detection\nms security test --input \"ignore previous instructions...\"\n```\n\n## Tasks\n\n1. [ ] Load ACIP v1.3 prompt from /data/projects/acip/ACIP_v_1.3_Full_Text.md\n2. [ ] Implement TrustBoundaryConfig for session content types\n3. [ ] Implement DecisionEngine following ACIP Decision Discipline\n4. [ ] Implement AcipAuditTag generation when audit mode enabled\n5. [ ] Integrate ACIP categories into detection rules\n6. [ ] Build QuarantineStore with ACIP metadata\n7. [ ] Implement session pre-filter using ACIP classification\n8. [ ] Build CLI commands for security scanning\n9. [ ] Add audit mode toggle and output formatting\n\n## Testing Requirements\n\n- ACIP integration tests (load config, classify content)\n- Decision discipline correctness tests\n- Trust boundary enforcement tests\n- Audit tag generation tests\n- Quarantine storage tests\n- Pipeline integration tests\n- False positive rate validation against ACIP benchmarks\n\n## Acceptance Criteria\n\n- ACIP v1.3 loaded and integrated\n- All session content classified per ACIP trust boundaries\n- Audit mode produces valid ACIP audit tags\n- Quarantine preserves ACIP classification\n- No oracle leakage (safe excerpts only)\n- CLI commands functional\n\n## References\n\n- ACIP repository: /data/projects/acip\n- ACIP v1.3 full text: /data/projects/acip/ACIP_v_1.3_Full_Text.md\n- Plan Section 5.17\n\nLabels: [defense injection phase-4 security acip]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:54:22.557041146-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:06:45.590145183-05:00","labels":["defense","injection","phase-4","security"],"dependencies":[{"issue_id":"meta_skill-fma","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:57:36.574085418-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ftb","title":"Benchmark Tests","description":"## Overview\n\nImplement Criterion benchmark tests for performance-critical paths in the meta_skill CLI. This bead implements Section 18.6 of the Testing Strategy with specific performance targets and CI integration for regression detection.\n\n## Requirements\n\n### 1. Benchmark Configuration\n\nAdd to `Cargo.toml`:\n```toml\n[dev-dependencies]\ncriterion = { version = \"0.5\", features = [\"html_reports\"] }\n\n[[bench]]\nname = \"benchmarks\"\nharness = false\n```\n\nCreate `benches/benchmarks.rs`:\n```rust\nuse criterion::{\n    black_box, criterion_group, criterion_main,\n    Criterion, BenchmarkId, Throughput,\n};\n\nmod hash_embedding;\nmod search;\nmod rrf_fusion;\nmod indexing;\nmod loading;\nmod packing;\n\ncriterion_group!(\n    benches,\n    hash_embedding::benches,\n    search::benches,\n    rrf_fusion::benches,\n    indexing::benches,\n    loading::benches,\n    packing::benches,\n);\n\ncriterion_main!(benches);\n```\n\n### 2. Hash Embedding Benchmarks\n\nTarget: **\u003c 1s per embedding**\n\nCreate `benches/hash_embedding.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId, Throughput};\nuse ms::search::hash_embed::{hash_embedding, HashEmbedding};\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"hash_embedding\");\n    \n    // Benchmark different input sizes\n    for size in [10, 100, 1000, 10000].iter() {\n        let input: String = \"a\".repeat(*size);\n        \n        group.throughput(Throughput::Bytes(*size as u64));\n        group.bench_with_input(\n            BenchmarkId::new(\"hash_embedding\", size),\n            \u0026input,\n            |b, input| {\n                b.iter(|| hash_embedding(black_box(input)))\n            },\n        );\n    }\n    \n    group.finish();\n    \n    // Benchmark batch processing\n    let mut batch_group = c.benchmark_group(\"hash_embedding_batch\");\n    let inputs: Vec\u003cString\u003e = (0..100).map(|i| format!(\"sample text {}\", i)).collect();\n    \n    batch_group.throughput(Throughput::Elements(100));\n    batch_group.bench_function(\"batch_100\", |b| {\n        b.iter(|| {\n            inputs.iter().map(|s| hash_embedding(black_box(s))).collect::\u003cVec\u003c_\u003e\u003e()\n        })\n    });\n    \n    batch_group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_hash_embedding_performance_target() {\n    use std::time::Instant;\n    \n    let iterations = 10000;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = hash_embedding(black_box(\"sample text for embedding\"));\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] hash_embedding: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_micros(1),\n        \"hash_embedding exceeded 1s target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 3. Search Benchmarks\n\nTarget: **\u003c 50ms p99 for 1000 skills**\n\nCreate `benches/search.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId, Throughput};\nuse ms::search::{SearchEngine, SearchQuery};\nuse tempfile::TempDir;\n\npub fn benches(c: \u0026mut Criterion) {\n    // Setup: Create index with skills\n    let temp_dir = TempDir::new().unwrap();\n    let engine = setup_search_engine(\u0026temp_dir, 1000);\n    \n    let mut group = c.benchmark_group(\"search\");\n    group.sample_size(100);\n    \n    // Benchmark different query types\n    let queries = vec![\n        (\"simple\", \"rust\"),\n        (\"two_words\", \"error handling\"),\n        (\"phrase\", \"async await patterns\"),\n        (\"complex\", \"rust error handling async\"),\n    ];\n    \n    for (name, query) in queries {\n        group.bench_with_input(\n            BenchmarkId::new(\"query\", name),\n            \u0026query,\n            |b, query| {\n                b.iter(|| engine.search(black_box(*query), 10))\n            },\n        );\n    }\n    \n    group.finish();\n    \n    // Benchmark scaling\n    let mut scaling_group = c.benchmark_group(\"search_scaling\");\n    for skill_count in [100, 500, 1000, 5000].iter() {\n        let temp = TempDir::new().unwrap();\n        let engine = setup_search_engine(\u0026temp, *skill_count);\n        \n        scaling_group.throughput(Throughput::Elements(*skill_count as u64));\n        scaling_group.bench_with_input(\n            BenchmarkId::new(\"skills\", skill_count),\n            skill_count,\n            |b, _| {\n                b.iter(|| engine.search(black_box(\"test query\"), 10))\n            },\n        );\n    }\n    \n    scaling_group.finish();\n}\n\nfn setup_search_engine(temp_dir: \u0026TempDir, skill_count: usize) -\u003e SearchEngine {\n    let mut engine = SearchEngine::new(temp_dir.path()).unwrap();\n    \n    for i in 0..skill_count {\n        engine.index_skill(\u0026format!(\"skill-{}\", i), \u0026format!(\n            \"Description for skill {} with various keywords like rust, async, error handling\",\n            i\n        )).unwrap();\n    }\n    \n    engine\n}\n\n// Target assertion for CI\n#[test]\nfn test_search_performance_target() {\n    use std::time::Instant;\n    \n    let temp_dir = TempDir::new().unwrap();\n    let engine = setup_search_engine(\u0026temp_dir, 1000);\n    \n    let mut times = Vec::new();\n    let queries = [\"rust\", \"error\", \"async\", \"handling\", \"patterns\"];\n    \n    for _ in 0..100 {\n        for query in \u0026queries {\n            let start = Instant::now();\n            let _ = engine.search(black_box(*query), 10);\n            times.push(start.elapsed());\n        }\n    }\n    \n    times.sort();\n    let p99 = times[times.len() * 99 / 100];\n    \n    println!(\"[PERF] search p99: {:?}\", p99);\n    assert!(\n        p99 \u003c std::time::Duration::from_millis(50),\n        \"search p99 exceeded 50ms target: {:?}\",\n        p99\n    );\n}\n```\n\n### 4. RRF Fusion Benchmarks\n\nTarget: **\u003c 10ms for combining rankings**\n\nCreate `benches/rrf_fusion.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId};\nuse ms::search::rrf::{rrf_fusion, RankedList};\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"rrf_fusion\");\n    \n    // Generate test rankings\n    let rankings: Vec\u003cRankedList\u003e = (0..3).map(|i| {\n        RankedList {\n            source: format!(\"source_{}\", i),\n            results: (0..100).map(|j| (format!(\"skill-{}\", j + i * 10), 1.0 / (j as f64 + 1.0))).collect(),\n        }\n    }).collect();\n    \n    // Benchmark different ranking sizes\n    for size in [10, 50, 100, 500].iter() {\n        let rankings: Vec\u003cRankedList\u003e = (0..3).map(|i| {\n            RankedList {\n                source: format!(\"source_{}\", i),\n                results: (0..*size).map(|j| (format!(\"skill-{}\", j), 1.0 / (j as f64 + 1.0))).collect(),\n            }\n        }).collect();\n        \n        group.bench_with_input(\n            BenchmarkId::new(\"ranking_size\", size),\n            \u0026rankings,\n            |b, rankings| {\n                b.iter(|| rrf_fusion(black_box(rankings), 60))\n            },\n        );\n    }\n    \n    // Benchmark different k values\n    for k in [20, 40, 60, 80, 100].iter() {\n        group.bench_with_input(\n            BenchmarkId::new(\"k_value\", k),\n            k,\n            |b, k| {\n                b.iter(|| rrf_fusion(black_box(\u0026rankings), *k))\n            },\n        );\n    }\n    \n    group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_rrf_fusion_performance_target() {\n    use std::time::Instant;\n    \n    let rankings: Vec\u003cRankedList\u003e = (0..5).map(|i| {\n        RankedList {\n            source: format!(\"source_{}\", i),\n            results: (0..1000).map(|j| (format!(\"skill-{}\", j), 1.0 / (j as f64 + 1.0))).collect(),\n        }\n    }).collect();\n    \n    let iterations = 100;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = rrf_fusion(black_box(\u0026rankings), 60);\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] rrf_fusion: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_millis(10),\n        \"rrf_fusion exceeded 10ms target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 5. Indexing Benchmarks\n\nTarget: **1000 skills/second**\n\nCreate `benches/indexing.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId, Throughput};\nuse ms::indexing::Indexer;\nuse tempfile::TempDir;\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"indexing\");\n    \n    // Generate test skills\n    let skills: Vec\u003c(String, String)\u003e = (0..1000).map(|i| {\n        (\n            format!(\"skill-{}\", i),\n            format!(\n                \"This is the description for skill number {}. It contains various keywords \\\n                 like rust, async, error handling, patterns, testing, and performance.\",\n                i\n            ),\n        )\n    }).collect();\n    \n    // Benchmark batch indexing\n    for batch_size in [10, 50, 100, 500, 1000].iter() {\n        let batch: Vec\u003c_\u003e = skills.iter().take(*batch_size).collect();\n        \n        group.throughput(Throughput::Elements(*batch_size as u64));\n        group.bench_with_input(\n            BenchmarkId::new(\"batch\", batch_size),\n            \u0026batch,\n            |b, batch| {\n                let temp_dir = TempDir::new().unwrap();\n                let mut indexer = Indexer::new(temp_dir.path()).unwrap();\n                \n                b.iter(|| {\n                    for (name, desc) in batch.iter() {\n                        indexer.index(black_box(name), black_box(desc)).unwrap();\n                    }\n                })\n            },\n        );\n    }\n    \n    group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_indexing_performance_target() {\n    use std::time::Instant;\n    \n    let temp_dir = TempDir::new().unwrap();\n    let mut indexer = Indexer::new(temp_dir.path()).unwrap();\n    \n    let skills: Vec\u003c(String, String)\u003e = (0..1000).map(|i| {\n        (\n            format!(\"skill-{}\", i),\n            format!(\"Description for skill {} with keywords\", i),\n        )\n    }).collect();\n    \n    let start = Instant::now();\n    for (name, desc) in \u0026skills {\n        indexer.index(name, desc).unwrap();\n    }\n    let elapsed = start.elapsed();\n    let per_skill = elapsed / 1000;\n    let skills_per_second = 1000.0 / elapsed.as_secs_f64();\n    \n    println!(\"[PERF] indexing: {:?} per skill ({:.0} skills/sec)\", per_skill, skills_per_second);\n    assert!(\n        skills_per_second \u003e= 1000.0,\n        \"indexing below 1000 skills/sec target: {:.0}\",\n        skills_per_second\n    );\n}\n```\n\n### 6. Load Benchmarks\n\nTarget: **\u003c 100ms for skill with dependencies**\n\nCreate `benches/loading.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId};\nuse ms::loading::SkillLoader;\nuse tempfile::TempDir;\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"loading\");\n    \n    // Setup: Create skills with dependencies\n    let temp_dir = TempDir::new().unwrap();\n    let loader = setup_skill_loader(\u0026temp_dir, 10);  // 10 skills with deps\n    \n    // Benchmark loading single skill\n    group.bench_function(\"single_skill\", |b| {\n        b.iter(|| loader.load(black_box(\"skill-0\")))\n    });\n    \n    // Benchmark loading skill with dependencies\n    group.bench_function(\"skill_with_deps\", |b| {\n        b.iter(|| loader.load_with_deps(black_box(\"skill-0\")))\n    });\n    \n    // Benchmark loading all skills\n    group.bench_function(\"all_skills\", |b| {\n        b.iter(|| loader.load_all())\n    });\n    \n    group.finish();\n}\n\nfn setup_skill_loader(temp_dir: \u0026TempDir, count: usize) -\u003e SkillLoader {\n    let mut loader = SkillLoader::new(temp_dir.path()).unwrap();\n    \n    for i in 0..count {\n        let deps = if i \u003e 0 {\n            vec![format!(\"skill-{}\", i - 1)]\n        } else {\n            vec![]\n        };\n        loader.register_skill(\u0026format!(\"skill-{}\", i), \u0026deps).unwrap();\n    }\n    \n    loader\n}\n\n// Target assertion for CI\n#[test]\nfn test_loading_performance_target() {\n    use std::time::Instant;\n    \n    let temp_dir = TempDir::new().unwrap();\n    let loader = setup_skill_loader(\u0026temp_dir, 10);\n    \n    let iterations = 100;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = loader.load_with_deps(black_box(\"skill-9\"));  // Skill with most deps\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] load_with_deps: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_millis(100),\n        \"load_with_deps exceeded 100ms target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 7. Pack Benchmarks\n\nTarget: **\u003c 50ms for constrained optimization**\n\nCreate `benches/packing.rs`:\n```rust\nuse criterion::{black_box, Criterion, BenchmarkId};\nuse ms::packing::{Packer, PackConstraints, Skill};\n\npub fn benches(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"packing\");\n    \n    // Generate test skills\n    let skills: Vec\u003cSkill\u003e = (0..100).map(|i| {\n        Skill {\n            name: format!(\"skill-{}\", i),\n            tokens: 100 + (i * 10) as usize,\n            priority: 1.0 - (i as f64 / 100.0),\n        }\n    }).collect();\n    \n    // Benchmark different constraint sizes\n    for max_tokens in [1000, 5000, 10000, 50000].iter() {\n        let constraints = PackConstraints {\n            max_tokens: *max_tokens,\n            max_skills: 50,\n        };\n        \n        group.bench_with_input(\n            BenchmarkId::new(\"max_tokens\", max_tokens),\n            \u0026constraints,\n            |b, constraints| {\n                b.iter(|| Packer::pack(black_box(\u0026skills), black_box(constraints)))\n            },\n        );\n    }\n    \n    // Benchmark different skill counts\n    for skill_count in [10, 50, 100, 500].iter() {\n        let skills: Vec\u003cSkill\u003e = (0..*skill_count).map(|i| {\n            Skill {\n                name: format!(\"skill-{}\", i),\n                tokens: 100 + (i * 10) as usize,\n                priority: 1.0 - (i as f64 / *skill_count as f64),\n            }\n        }).collect();\n        \n        let constraints = PackConstraints {\n            max_tokens: 10000,\n            max_skills: 50,\n        };\n        \n        group.bench_with_input(\n            BenchmarkId::new(\"skill_count\", skill_count),\n            \u0026skill_count,\n            |b, _| {\n                b.iter(|| Packer::pack(black_box(\u0026skills), black_box(\u0026constraints)))\n            },\n        );\n    }\n    \n    group.finish();\n}\n\n// Target assertion for CI\n#[test]\nfn test_packing_performance_target() {\n    use std::time::Instant;\n    \n    let skills: Vec\u003cSkill\u003e = (0..100).map(|i| {\n        Skill {\n            name: format!(\"skill-{}\", i),\n            tokens: 100 + (i * 10) as usize,\n            priority: 1.0 - (i as f64 / 100.0),\n        }\n    }).collect();\n    \n    let constraints = PackConstraints {\n        max_tokens: 10000,\n        max_skills: 50,\n    };\n    \n    let iterations = 100;\n    let start = Instant::now();\n    for _ in 0..iterations {\n        let _ = Packer::pack(black_box(\u0026skills), black_box(\u0026constraints));\n    }\n    let elapsed = start.elapsed();\n    let per_op = elapsed / iterations;\n    \n    println!(\"[PERF] pack: {:?} per operation\", per_op);\n    assert!(\n        per_op \u003c std::time::Duration::from_millis(50),\n        \"pack exceeded 50ms target: {:?}\",\n        per_op\n    );\n}\n```\n\n### 8. CI Integration\n\nAdd benchmark checks to CI:\n\n```yaml\nbenchmark-tests:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v4\n    \n    - name: Install Rust toolchain\n      uses: dtolnay/rust-action@stable\n    \n    - name: Run benchmarks\n      run: cargo bench --no-run\n    \n    - name: Run performance target tests\n      run: |\n        cargo test test_hash_embedding_performance_target -- --nocapture\n        cargo test test_search_performance_target -- --nocapture\n        cargo test test_rrf_fusion_performance_target -- --nocapture\n        cargo test test_indexing_performance_target -- --nocapture\n        cargo test test_loading_performance_target -- --nocapture\n        cargo test test_packing_performance_target -- --nocapture\n    \n    - name: Compare with baseline (on main branch)\n      if: github.ref == 'refs/heads/main'\n      run: |\n        cargo bench -- --save-baseline main\n    \n    - name: Check for regression (on PR)\n      if: github.event_name == 'pull_request'\n      run: |\n        cargo bench -- --baseline main\n```\n\n### 9. Performance Targets Summary\n\n| Operation | Target | Test |\n|-----------|--------|------|\n| hash_embedding | \u003c 1s | test_hash_embedding_performance_target |\n| search (p99, 1000 skills) | \u003c 50ms | test_search_performance_target |\n| rrf_fusion | \u003c 10ms | test_rrf_fusion_performance_target |\n| indexing | 1000 skills/sec | test_indexing_performance_target |\n| load (with deps) | \u003c 100ms | test_loading_performance_target |\n| pack | \u003c 50ms | test_packing_performance_target |\n\n## Acceptance Criteria\n\n1. [ ] Criterion benchmark suite configured\n2. [ ] hash_embedding benchmark with \u003c 1s target\n3. [ ] search benchmark with \u003c 50ms p99 target\n4. [ ] rrf_fusion benchmark with \u003c 10ms target\n5. [ ] indexing benchmark with 1000 skills/sec target\n6. [ ] load benchmark with \u003c 100ms target\n7. [ ] pack benchmark with \u003c 50ms target\n8. [ ] Performance target tests that fail on regression\n9. [ ] CI integration with baseline comparison\n10. [ ] HTML benchmark reports generated\n\n## Dependencies\n\n- meta_skill-5s0 (Rust Project Scaffolding) - provides project structure","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:57:39.72055569-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.885220875-05:00","labels":["benchmarks","performance","testing"],"dependencies":[{"issue_id":"meta_skill-ftb","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:57:44.88901878-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ftb","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.205437078-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ftj","title":"Tech Stack Detection","description":"# Tech Stack Detection\n\n## Overview\n\nTech Stack Detection automatically identifies the programming languages, frameworks, and tools used in a project by analyzing indicator files and directory structures. This enables stack-specific skill mining, intelligent suggestion filtering, and context-aware recommendations.\n\n**Example Detection:**\n```\nProject: /home/user/my-nextjs-app\nDetected Stack:\n  - TypeScript (tsconfig.json)\n  - Next.js 14 (next.config.js, package.json)\n  - React 18 (package.json dependency)\n  - Tailwind CSS (tailwind.config.js)\n  - ESLint (eslint.config.js)\n  - pnpm (pnpm-lock.yaml)\n```\n\nThis detection enables:\n- Loading TypeScript, Next.js, React, and Tailwind skills automatically\n- Filtering out irrelevant skills (Python, Go, etc.)\n- Suggesting stack-specific meta-skills like `nextjs-fullstack`\n\n## Background \u0026 Rationale\n\n### Section 5.8 Reference\n\nFrom the plan Section 5.8:\n\u003e \"Auto-detect project tech stack (Rust, Go, Next.js, React, Python, etc.) from indicators like Cargo.toml, package.json, go.mod. Stack-specific skill mining.\"\n\n### Why Detection Matters\n\nWithout detection, users must:\n1. Manually specify their tech stack\n2. Receive irrelevant suggestions (Python skills in a Rust project)\n3. Miss out on stack-specific skills they didn't know existed\n\nDetection provides:\n- **Zero Configuration**: Works out of the box\n- **Accurate Filtering**: Only relevant skills suggested\n- **Deep Awareness**: Knows specific versions and frameworks\n- **Toolchain Detection**: Identifies package managers, linters, etc.\n\n## Core Data Structures\n\n### TechStack Enum\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashSet;\n\n/// Primary technology/language in a project\n#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum TechStack {\n    /// Rust with Cargo\n    Rust,\n    /// Go with go.mod\n    Go,\n    /// Next.js (React + SSR)\n    NextJs,\n    /// React (without Next.js)\n    React,\n    /// Vue.js\n    Vue,\n    /// Angular\n    Angular,\n    /// Svelte / SvelteKit\n    Svelte,\n    /// Python (general)\n    Python,\n    /// Django (Python web)\n    Django,\n    /// FastAPI (Python API)\n    FastAPI,\n    /// Flask (Python web)\n    Flask,\n    /// Ruby on Rails\n    Rails,\n    /// Ruby (non-Rails)\n    Ruby,\n    /// Node.js (non-framework)\n    NodeJs,\n    /// Deno\n    Deno,\n    /// Java (general)\n    Java,\n    /// Spring Boot (Java)\n    SpringBoot,\n    /// Kotlin\n    Kotlin,\n    /// Swift\n    Swift,\n    /// .NET / C#\n    DotNet,\n    /// Elixir / Phoenix\n    Elixir,\n    /// PHP / Laravel\n    PHP,\n    /// Generic / Unknown\n    Generic,\n}\n\nimpl TechStack {\n    /// Get human-readable name\n    pub fn display_name(\u0026self) -\u003e \u0026'static str {\n        match self {\n            TechStack::Rust =\u003e \"Rust\",\n            TechStack::Go =\u003e \"Go\",\n            TechStack::NextJs =\u003e \"Next.js\",\n            TechStack::React =\u003e \"React\",\n            TechStack::Vue =\u003e \"Vue.js\",\n            TechStack::Angular =\u003e \"Angular\",\n            TechStack::Svelte =\u003e \"Svelte\",\n            TechStack::Python =\u003e \"Python\",\n            TechStack::Django =\u003e \"Django\",\n            TechStack::FastAPI =\u003e \"FastAPI\",\n            TechStack::Flask =\u003e \"Flask\",\n            TechStack::Rails =\u003e \"Ruby on Rails\",\n            TechStack::Ruby =\u003e \"Ruby\",\n            TechStack::NodeJs =\u003e \"Node.js\",\n            TechStack::Deno =\u003e \"Deno\",\n            TechStack::Java =\u003e \"Java\",\n            TechStack::SpringBoot =\u003e \"Spring Boot\",\n            TechStack::Kotlin =\u003e \"Kotlin\",\n            TechStack::Swift =\u003e \"Swift\",\n            TechStack::DotNet =\u003e \".NET\",\n            TechStack::Elixir =\u003e \"Elixir\",\n            TechStack::PHP =\u003e \"PHP\",\n            TechStack::Generic =\u003e \"Generic\",\n        }\n    }\n    \n    /// Get related stacks (e.g., NextJs implies React)\n    pub fn related_stacks(\u0026self) -\u003e Vec\u003cTechStack\u003e {\n        match self {\n            TechStack::NextJs =\u003e vec![TechStack::React, TechStack::NodeJs],\n            TechStack::Django =\u003e vec![TechStack::Python],\n            TechStack::FastAPI =\u003e vec![TechStack::Python],\n            TechStack::Flask =\u003e vec![TechStack::Python],\n            TechStack::Rails =\u003e vec![TechStack::Ruby],\n            TechStack::SpringBoot =\u003e vec![TechStack::Java],\n            _ =\u003e vec![],\n        }\n    }\n    \n    /// Get skill tags that match this stack\n    pub fn skill_tags(\u0026self) -\u003e Vec\u003c\u0026'static str\u003e {\n        match self {\n            TechStack::Rust =\u003e vec![\"rust\", \"cargo\", \"rustfmt\", \"clippy\"],\n            TechStack::Go =\u003e vec![\"go\", \"golang\"],\n            TechStack::NextJs =\u003e vec![\"nextjs\", \"next\", \"react\", \"typescript\", \"javascript\"],\n            TechStack::React =\u003e vec![\"react\", \"typescript\", \"javascript\", \"jsx\", \"tsx\"],\n            TechStack::Python =\u003e vec![\"python\", \"pip\", \"poetry\"],\n            TechStack::Django =\u003e vec![\"django\", \"python\", \"orm\"],\n            // ... etc\n            _ =\u003e vec![],\n        }\n    }\n}\n```\n\n### TechStackDetector Struct\n\n```rust\nuse std::path::{Path, PathBuf};\nuse std::collections::HashMap;\n\n/// Detector for identifying project tech stacks\n#[derive(Debug, Clone)]\npub struct TechStackDetector {\n    /// Map of tech stack to its indicators\n    pub indicators: HashMap\u003cTechStack, Vec\u003cIndicator\u003e\u003e,\n    \n    /// Cached detection results (path -\u003e result)\n    cache: HashMap\u003cPathBuf, DetectionResult\u003e,\n    \n    /// Logger for detailed tracing\n    logger: Option\u003cArc\u003cdyn DetectionLogger\u003e\u003e,\n}\n\n/// An indicator that suggests a particular tech stack\n#[derive(Debug, Clone)]\npub struct Indicator {\n    /// Type of indicator\n    pub kind: IndicatorKind,\n    \n    /// Confidence weight (0.0 - 1.0)\n    pub weight: f64,\n    \n    /// Whether this indicator is definitive (implies the stack)\n    pub definitive: bool,\n    \n    /// Optional version extraction pattern\n    pub version_pattern: Option\u003cString\u003e,\n}\n\n/// Types of indicators\n#[derive(Debug, Clone)]\npub enum IndicatorKind {\n    /// File exists (e.g., \"Cargo.toml\")\n    FileExists(String),\n    \n    /// File with specific content pattern\n    FileContains {\n        file: String,\n        pattern: String,\n    },\n    \n    /// Directory exists (e.g., \".git\")\n    DirectoryExists(String),\n    \n    /// Dependency in package manifest\n    Dependency {\n        manifest: String,\n        package: String,\n    },\n    \n    /// File extension present in project\n    FileExtension(String),\n    \n    /// Binary/command available\n    CommandAvailable(String),\n}\n\nimpl TechStackDetector {\n    /// Create detector with default indicators\n    pub fn new() -\u003e Self {\n        let mut detector = Self {\n            indicators: HashMap::new(),\n            cache: HashMap::new(),\n            logger: None,\n        };\n        \n        detector.register_default_indicators();\n        detector\n    }\n    \n    /// Register all default indicators\n    fn register_default_indicators(\u0026mut self) {\n        // Rust indicators\n        self.indicators.insert(TechStack::Rust, vec![\n            Indicator {\n                kind: IndicatorKind::FileExists(\"Cargo.toml\".into()),\n                weight: 1.0,\n                definitive: true,\n                version_pattern: Some(r#\"edition\\s*=\\s*\"(\\d+)\"\"#.into()),\n            },\n            Indicator {\n                kind: IndicatorKind::FileExtension(\"rs\".into()),\n                weight: 0.8,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::DirectoryExists(\"target\".into()),\n                weight: 0.3,\n                definitive: false,\n                version_pattern: None,\n            },\n        ]);\n        \n        // Go indicators\n        self.indicators.insert(TechStack::Go, vec![\n            Indicator {\n                kind: IndicatorKind::FileExists(\"go.mod\".into()),\n                weight: 1.0,\n                definitive: true,\n                version_pattern: Some(r\"go\\s+(\\d+\\.\\d+)\".into()),\n            },\n            Indicator {\n                kind: IndicatorKind::FileExists(\"go.sum\".into()),\n                weight: 0.5,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExtension(\"go\".into()),\n                weight: 0.8,\n                definitive: false,\n                version_pattern: None,\n            },\n        ]);\n        \n        // Next.js indicators\n        self.indicators.insert(TechStack::NextJs, vec![\n            Indicator {\n                kind: IndicatorKind::FileExists(\"next.config.js\".into()),\n                weight: 1.0,\n                definitive: true,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExists(\"next.config.mjs\".into()),\n                weight: 1.0,\n                definitive: true,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExists(\"next.config.ts\".into()),\n                weight: 1.0,\n                definitive: true,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::Dependency {\n                    manifest: \"package.json\".into(),\n                    package: \"next\".into(),\n                },\n                weight: 1.0,\n                definitive: true,\n                version_pattern: Some(r#\"\"next\"\\s*:\\s*\"[~^]?(\\d+\\.\\d+)\"#.into()),\n            },\n            Indicator {\n                kind: IndicatorKind::DirectoryExists(\"app\".into()),\n                weight: 0.3,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::DirectoryExists(\"pages\".into()),\n                weight: 0.3,\n                definitive: false,\n                version_pattern: None,\n            },\n        ]);\n        \n        // React indicators (non-Next.js)\n        self.indicators.insert(TechStack::React, vec![\n            Indicator {\n                kind: IndicatorKind::Dependency {\n                    manifest: \"package.json\".into(),\n                    package: \"react\".into(),\n                },\n                weight: 1.0,\n                definitive: true,\n                version_pattern: Some(r#\"\"react\"\\s*:\\s*\"[~^]?(\\d+\\.\\d+)\"#.into()),\n            },\n            Indicator {\n                kind: IndicatorKind::FileExtension(\"jsx\".into()),\n                weight: 0.7,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExtension(\"tsx\".into()),\n                weight: 0.7,\n                definitive: false,\n                version_pattern: None,\n            },\n        ]);\n        \n        // Python indicators\n        self.indicators.insert(TechStack::Python, vec![\n            Indicator {\n                kind: IndicatorKind::FileExists(\"requirements.txt\".into()),\n                weight: 0.8,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExists(\"pyproject.toml\".into()),\n                weight: 0.9,\n                definitive: false,\n                version_pattern: Some(r#\"python\\s*=\\s*\"[~^]?(\\d+\\.\\d+)\"#.into()),\n            },\n            Indicator {\n                kind: IndicatorKind::FileExists(\"setup.py\".into()),\n                weight: 0.8,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExists(\"Pipfile\".into()),\n                weight: 0.8,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExtension(\"py\".into()),\n                weight: 0.9,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::DirectoryExists(\"venv\".into()),\n                weight: 0.3,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::DirectoryExists(\".venv\".into()),\n                weight: 0.3,\n                definitive: false,\n                version_pattern: None,\n            },\n        ]);\n        \n        // Django indicators\n        self.indicators.insert(TechStack::Django, vec![\n            Indicator {\n                kind: IndicatorKind::FileExists(\"manage.py\".into()),\n                weight: 0.6,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::Dependency {\n                    manifest: \"requirements.txt\".into(),\n                    package: \"django\".into(),\n                },\n                weight: 1.0,\n                definitive: true,\n                version_pattern: Some(r\"django[=~\u003c\u003e]=?(\\d+\\.\\d+)\".into()),\n            },\n            Indicator {\n                kind: IndicatorKind::FileContains {\n                    file: \"settings.py\".into(),\n                    pattern: \"INSTALLED_APPS\".into(),\n                },\n                weight: 0.8,\n                definitive: true,\n                version_pattern: None,\n            },\n        ]);\n        \n        // Rails indicators\n        self.indicators.insert(TechStack::Rails, vec![\n            Indicator {\n                kind: IndicatorKind::FileExists(\"Gemfile\".into()),\n                weight: 0.5,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileContains {\n                    file: \"Gemfile\".into(),\n                    pattern: r\"gem\\s+['\\\"]rails['\\\"]\".into(),\n                },\n                weight: 1.0,\n                definitive: true,\n                version_pattern: Some(r#\"gem\\s+['\"]rails['\"],\\s*['\"]~\u003e\\s*(\\d+\\.\\d+)\"#.into()),\n            },\n            Indicator {\n                kind: IndicatorKind::DirectoryExists(\"app/controllers\".into()),\n                weight: 0.4,\n                definitive: false,\n                version_pattern: None,\n            },\n            Indicator {\n                kind: IndicatorKind::FileExists(\"config/routes.rb\".into()),\n                weight: 0.7,\n                definitive: true,\n                version_pattern: None,\n            },\n        ]);\n        \n        // Add more tech stacks...\n    }\n    \n    /// Detect tech stack for a project path\n    pub fn detect(\u0026mut self, project_path: \u0026Path) -\u003e DetectionResult {\n        // Check cache\n        if let Some(cached) = self.cache.get(project_path) {\n            if let Some(logger) = \u0026self.logger {\n                logger.log_cache_hit(project_path);\n            }\n            return cached.clone();\n        }\n        \n        if let Some(logger) = \u0026self.logger {\n            logger.log_detection_start(project_path);\n        }\n        \n        let mut detected_stacks = Vec::new();\n        let mut toolchain = ProjectToolchain::default();\n        \n        // Check each tech stack's indicators\n        for (stack, indicators) in \u0026self.indicators {\n            let mut total_weight = 0.0;\n            let mut max_weight = 0.0;\n            let mut is_definitive = false;\n            let mut detected_version = None;\n            \n            for indicator in indicators {\n                max_weight += indicator.weight;\n                \n                if self.check_indicator(project_path, indicator) {\n                    total_weight += indicator.weight;\n                    \n                    if indicator.definitive {\n                        is_definitive = true;\n                    }\n                    \n                    // Extract version if pattern provided\n                    if let Some(pattern) = \u0026indicator.version_pattern {\n                        if let Some(version) = self.extract_version(project_path, indicator, pattern) {\n                            detected_version = Some(version);\n                        }\n                    }\n                    \n                    if let Some(logger) = \u0026self.logger {\n                        logger.log_indicator_matched(*stack, indicator);\n                    }\n                }\n            }\n            \n            let confidence = if max_weight \u003e 0.0 {\n                total_weight / max_weight\n            } else {\n                0.0\n            };\n            \n            // Include if definitive or confidence above threshold\n            if is_definitive || confidence \u003e 0.5 {\n                detected_stacks.push(DetectedStack {\n                    stack: *stack,\n                    confidence,\n                    version: detected_version,\n                    is_primary: false, // Set later\n                });\n                \n                if let Some(logger) = \u0026self.logger {\n                    logger.log_stack_detected(*stack, confidence);\n                }\n            }\n        }\n        \n        // Sort by confidence and mark primary\n        detected_stacks.sort_by(|a, b| b.confidence.partial_cmp(\u0026a.confidence).unwrap());\n        if let Some(first) = detected_stacks.first_mut() {\n            first.is_primary = true;\n        }\n        \n        // Detect toolchain\n        toolchain = self.detect_toolchain(project_path);\n        \n        let result = DetectionResult {\n            project_path: project_path.to_path_buf(),\n            detected_stacks,\n            toolchain,\n            detected_at: chrono::Utc::now(),\n        };\n        \n        // Cache result\n        self.cache.insert(project_path.to_path_buf(), result.clone());\n        \n        if let Some(logger) = \u0026self.logger {\n            logger.log_detection_complete(\u0026result);\n        }\n        \n        result\n    }\n    \n    /// Check if a single indicator matches\n    fn check_indicator(\u0026self, project_path: \u0026Path, indicator: \u0026Indicator) -\u003e bool {\n        match \u0026indicator.kind {\n            IndicatorKind::FileExists(file) =\u003e {\n                project_path.join(file).exists()\n            }\n            IndicatorKind::FileContains { file, pattern } =\u003e {\n                let file_path = project_path.join(file);\n                if let Ok(content) = std::fs::read_to_string(\u0026file_path) {\n                    let re = regex::Regex::new(pattern).ok();\n                    re.map(|r| r.is_match(\u0026content)).unwrap_or(false)\n                } else {\n                    false\n                }\n            }\n            IndicatorKind::DirectoryExists(dir) =\u003e {\n                project_path.join(dir).is_dir()\n            }\n            IndicatorKind::Dependency { manifest, package } =\u003e {\n                self.check_dependency(project_path, manifest, package)\n            }\n            IndicatorKind::FileExtension(ext) =\u003e {\n                self.has_file_extension(project_path, ext)\n            }\n            IndicatorKind::CommandAvailable(cmd) =\u003e {\n                which::which(cmd).is_ok()\n            }\n        }\n    }\n    \n    /// Check if a dependency exists in a manifest\n    fn check_dependency(\u0026self, project_path: \u0026Path, manifest: \u0026str, package: \u0026str) -\u003e bool {\n        let manifest_path = project_path.join(manifest);\n        if let Ok(content) = std::fs::read_to_string(\u0026manifest_path) {\n            // Handle different manifest formats\n            if manifest.ends_with(\".json\") {\n                // JSON manifest (package.json)\n                if let Ok(json) = serde_json::from_str::\u003cserde_json::Value\u003e(\u0026content) {\n                    // Check dependencies, devDependencies, peerDependencies\n                    for key in [\"dependencies\", \"devDependencies\", \"peerDependencies\"] {\n                        if let Some(deps) = json.get(key) {\n                            if deps.get(package).is_some() {\n                                return true;\n                            }\n                        }\n                    }\n                }\n            } else if manifest == \"requirements.txt\" {\n                // Python requirements format\n                let package_lower = package.to_lowercase();\n                for line in content.lines() {\n                    let line = line.trim().to_lowercase();\n                    if line.starts_with(\u0026package_lower) {\n                        return true;\n                    }\n                }\n            } else if manifest == \"Gemfile\" {\n                // Ruby Gemfile format\n                let pattern = format!(r#\"gem\\s+['\"]{}['\"]\"#, regex::escape(package));\n                if let Ok(re) = regex::Regex::new(\u0026pattern) {\n                    return re.is_match(\u0026content);\n                }\n            }\n        }\n        false\n    }\n    \n    /// Check if project has files with given extension\n    fn has_file_extension(\u0026self, project_path: \u0026Path, ext: \u0026str) -\u003e bool {\n        // Quick check: look in common locations\n        let common_dirs = [\"src\", \"lib\", \"app\", \".\"];\n        \n        for dir in common_dirs {\n            let dir_path = project_path.join(dir);\n            if dir_path.is_dir() {\n                if let Ok(entries) = std::fs::read_dir(\u0026dir_path) {\n                    for entry in entries.take(100) {\n                        if let Ok(entry) = entry {\n                            let path = entry.path();\n                            if path.extension().map(|e| e == ext).unwrap_or(false) {\n                                return true;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        false\n    }\n    \n    /// Extract version from file using pattern\n    fn extract_version(\n        \u0026self,\n        project_path: \u0026Path,\n        indicator: \u0026Indicator,\n        pattern: \u0026str,\n    ) -\u003e Option\u003cString\u003e {\n        let file = match \u0026indicator.kind {\n            IndicatorKind::FileExists(f) =\u003e f.clone(),\n            IndicatorKind::FileContains { file, .. } =\u003e file.clone(),\n            IndicatorKind::Dependency { manifest, .. } =\u003e manifest.clone(),\n            _ =\u003e return None,\n        };\n        \n        let file_path = project_path.join(\u0026file);\n        let content = std::fs::read_to_string(\u0026file_path).ok()?;\n        \n        let re = regex::Regex::new(pattern).ok()?;\n        let captures = re.captures(\u0026content)?;\n        \n        captures.get(1).map(|m| m.as_str().to_string())\n    }\n    \n    /// Detect project toolchain (package managers, etc.)\n    fn detect_toolchain(\u0026self, project_path: \u0026Path) -\u003e ProjectToolchain {\n        ProjectToolchain {\n            node: self.detect_node_version(project_path),\n            rust: self.detect_rust_version(project_path),\n            go: self.detect_go_version(project_path),\n            python: self.detect_python_version(project_path),\n            package_manager: self.detect_package_manager(project_path),\n        }\n    }\n    \n    fn detect_node_version(\u0026self, project_path: \u0026Path) -\u003e Option\u003cString\u003e {\n        // Check .nvmrc\n        let nvmrc = project_path.join(\".nvmrc\");\n        if let Ok(content) = std::fs::read_to_string(\u0026nvmrc) {\n            return Some(content.trim().to_string());\n        }\n        \n        // Check .node-version\n        let node_version = project_path.join(\".node-version\");\n        if let Ok(content) = std::fs::read_to_string(\u0026node_version) {\n            return Some(content.trim().to_string());\n        }\n        \n        // Check package.json engines\n        let pkg = project_path.join(\"package.json\");\n        if let Ok(content) = std::fs::read_to_string(\u0026pkg) {\n            if let Ok(json) = serde_json::from_str::\u003cserde_json::Value\u003e(\u0026content) {\n                if let Some(engines) = json.get(\"engines\") {\n                    if let Some(node) = engines.get(\"node\") {\n                        return node.as_str().map(|s| s.to_string());\n                    }\n                }\n            }\n        }\n        \n        None\n    }\n    \n    fn detect_rust_version(\u0026self, project_path: \u0026Path) -\u003e Option\u003cString\u003e {\n        let rust_toolchain = project_path.join(\"rust-toolchain.toml\");\n        if let Ok(content) = std::fs::read_to_string(\u0026rust_toolchain) {\n            let re = regex::Regex::new(r#\"channel\\s*=\\s*\"([^\"]+)\"\"#).ok()?;\n            let captures = re.captures(\u0026content)?;\n            return captures.get(1).map(|m| m.as_str().to_string());\n        }\n        \n        let rust_toolchain = project_path.join(\"rust-toolchain\");\n        if let Ok(content) = std::fs::read_to_string(\u0026rust_toolchain) {\n            return Some(content.trim().to_string());\n        }\n        \n        None\n    }\n    \n    fn detect_go_version(\u0026self, project_path: \u0026Path) -\u003e Option\u003cString\u003e {\n        let go_mod = project_path.join(\"go.mod\");\n        if let Ok(content) = std::fs::read_to_string(\u0026go_mod) {\n            let re = regex::Regex::new(r\"^go\\s+(\\d+\\.\\d+)\").ok()?;\n            for line in content.lines() {\n                if let Some(captures) = re.captures(line) {\n                    return captures.get(1).map(|m| m.as_str().to_string());\n                }\n            }\n        }\n        None\n    }\n    \n    fn detect_python_version(\u0026self, project_path: \u0026Path) -\u003e Option\u003cString\u003e {\n        // Check .python-version\n        let python_version = project_path.join(\".python-version\");\n        if let Ok(content) = std::fs::read_to_string(\u0026python_version) {\n            return Some(content.trim().to_string());\n        }\n        \n        // Check pyproject.toml\n        let pyproject = project_path.join(\"pyproject.toml\");\n        if let Ok(content) = std::fs::read_to_string(\u0026pyproject) {\n            let re = regex::Regex::new(r#\"python\\s*=\\s*\"[~^\u003e=]*(\\d+\\.\\d+)\"#).ok()?;\n            if let Some(captures) = re.captures(\u0026content) {\n                return captures.get(1).map(|m| m.as_str().to_string());\n            }\n        }\n        \n        None\n    }\n    \n    fn detect_package_manager(\u0026self, project_path: \u0026Path) -\u003e Option\u003cPackageManager\u003e {\n        // Node.js package managers\n        if project_path.join(\"pnpm-lock.yaml\").exists() {\n            return Some(PackageManager::Pnpm);\n        }\n        if project_path.join(\"yarn.lock\").exists() {\n            return Some(PackageManager::Yarn);\n        }\n        if project_path.join(\"bun.lockb\").exists() {\n            return Some(PackageManager::Bun);\n        }\n        if project_path.join(\"package-lock.json\").exists() {\n            return Some(PackageManager::Npm);\n        }\n        \n        // Python package managers\n        if project_path.join(\"poetry.lock\").exists() {\n            return Some(PackageManager::Poetry);\n        }\n        if project_path.join(\"Pipfile.lock\").exists() {\n            return Some(PackageManager::Pipenv);\n        }\n        if project_path.join(\"uv.lock\").exists() {\n            return Some(PackageManager::Uv);\n        }\n        \n        // Ruby\n        if project_path.join(\"Gemfile.lock\").exists() {\n            return Some(PackageManager::Bundler);\n        }\n        \n        None\n    }\n    \n    /// Clear detection cache\n    pub fn clear_cache(\u0026mut self) {\n        self.cache.clear();\n    }\n    \n    /// Invalidate cache for specific path\n    pub fn invalidate(\u0026mut self, path: \u0026Path) {\n        self.cache.remove(path);\n    }\n}\n```\n\n### ProjectToolchain Struct\n\n```rust\n/// Detected toolchain information\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct ProjectToolchain {\n    /// Node.js version (from .nvmrc, package.json engines, etc.)\n    pub node: Option\u003cString\u003e,\n    \n    /// Rust version (from rust-toolchain.toml)\n    pub rust: Option\u003cString\u003e,\n    \n    /// Go version (from go.mod)\n    pub go: Option\u003cString\u003e,\n    \n    /// Python version (from .python-version, pyproject.toml)\n    pub python: Option\u003cString\u003e,\n    \n    /// Detected package manager\n    pub package_manager: Option\u003cPackageManager\u003e,\n}\n\n/// Package manager types\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum PackageManager {\n    // Node.js\n    Npm,\n    Yarn,\n    Pnpm,\n    Bun,\n    \n    // Python\n    Pip,\n    Poetry,\n    Pipenv,\n    Uv,\n    \n    // Ruby\n    Bundler,\n    \n    // Rust (cargo is implicit)\n    Cargo,\n}\n```\n\n### DetectionResult Struct\n\n```rust\nuse chrono::{DateTime, Utc};\n\n/// Result of tech stack detection\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DetectionResult {\n    /// Path that was analyzed\n    pub project_path: PathBuf,\n    \n    /// Detected tech stacks, sorted by confidence\n    pub detected_stacks: Vec\u003cDetectedStack\u003e,\n    \n    /// Detected toolchain information\n    pub toolchain: ProjectToolchain,\n    \n    /// When detection was performed\n    pub detected_at: DateTime\u003cUtc\u003e,\n}\n\n/// A single detected tech stack with metadata\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DetectedStack {\n    /// The detected stack\n    pub stack: TechStack,\n    \n    /// Confidence score (0.0 - 1.0)\n    pub confidence: f64,\n    \n    /// Detected version if available\n    pub version: Option\u003cString\u003e,\n    \n    /// Whether this is the primary stack\n    pub is_primary: bool,\n}\n\nimpl DetectionResult {\n    /// Get the primary tech stack\n    pub fn primary_stack(\u0026self) -\u003e Option\u003c\u0026DetectedStack\u003e {\n        self.detected_stacks.iter().find(|s| s.is_primary)\n    }\n    \n    /// Get all tech stacks with confidence above threshold\n    pub fn stacks_above_confidence(\u0026self, threshold: f64) -\u003e Vec\u003c\u0026DetectedStack\u003e {\n        self.detected_stacks\n            .iter()\n            .filter(|s| s.confidence \u003e= threshold)\n            .collect()\n    }\n    \n    /// Check if a specific stack was detected\n    pub fn has_stack(\u0026self, stack: TechStack) -\u003e bool {\n        self.detected_stacks.iter().any(|s| s.stack == stack)\n    }\n    \n    /// Get all skill tags for detected stacks\n    pub fn all_skill_tags(\u0026self) -\u003e Vec\u003c\u0026'static str\u003e {\n        let mut tags = Vec::new();\n        for detected in \u0026self.detected_stacks {\n            tags.extend(detected.stack.skill_tags());\n        }\n        tags.sort();\n        tags.dedup();\n        tags\n    }\n    \n    /// Format for display\n    pub fn format_summary(\u0026self) -\u003e String {\n        let mut lines = vec![format!(\"Project: {}\", self.project_path.display())];\n        \n        if self.detected_stacks.is_empty() {\n            lines.push(\"No specific tech stack detected\".to_string());\n        } else {\n            lines.push(\"Detected Stack:\".to_string());\n            for detected in \u0026self.detected_stacks {\n                let version = detected.version\n                    .as_ref()\n                    .map(|v| format!(\" {}\", v))\n                    .unwrap_or_default();\n                let primary = if detected.is_primary { \" (primary)\" } else { \"\" };\n                lines.push(format!(\n                    \"  - {}{} ({:.0}% confidence){}\",\n                    detected.stack.display_name(),\n                    version,\n                    detected.confidence * 100.0,\n                    primary,\n                ));\n            }\n        }\n        \n        if let Some(pm) = \u0026self.toolchain.package_manager {\n            lines.push(format!(\"Package Manager: {:?}\", pm));\n        }\n        \n        lines.join(\"\\n\")\n    }\n}\n```\n\n## CLI Integration\n\n```rust\n/// CLI command for tech stack detection\npub async fn detect_stack_command(path: Option\u003cPathBuf\u003e) -\u003e Result\u003c(), CliError\u003e {\n    let project_path = path.unwrap_or_else(|| std::env::current_dir().unwrap());\n    \n    let mut detector = TechStackDetector::new();\n    let result = detector.detect(\u0026project_path);\n    \n    println!(\"{}\", result.format_summary());\n    \n    Ok(())\n}\n```\n\n## Integration with Suggestion Engine\n\n```rust\n/// Filter suggestions by detected tech stack\npub struct TechStackFilter {\n    detector: TechStackDetector,\n}\n\nimpl TechStackFilter {\n    /// Filter skills to only those matching detected stack\n    pub fn filter_skills(\n        \u0026mut self,\n        skills: Vec\u003cSkill\u003e,\n        project_path: \u0026Path,\n    ) -\u003e Vec\u003cSkill\u003e {\n        let result = self.detector.detect(project_path);\n        let tags = result.all_skill_tags();\n        \n        skills.into_iter()\n            .filter(|skill| {\n                // Keep if skill has no tech_stack tags (generic)\n                // Or if skill matches detected stack\n                skill.tech_stack_tags().is_empty()\n                    || skill.tech_stack_tags().iter().any(|t| tags.contains(\u0026t.as_str()))\n            })\n            .collect()\n    }\n    \n    /// Boost skills matching detected stack\n    pub fn boost_matching_skills(\n        \u0026mut self,\n        skills: \u0026mut Vec\u003cScoredSkill\u003e,\n        project_path: \u0026Path,\n        boost_factor: f64,\n    ) {\n        let result = self.detector.detect(project_path);\n        let tags = result.all_skill_tags();\n        \n        for skill in skills {\n            let matches = skill.skill.tech_stack_tags()\n                .iter()\n                .any(|t| tags.contains(\u0026t.as_str()));\n            \n            if matches {\n                skill.score *= boost_factor;\n            }\n        }\n    }\n}\n```\n\n## Tasks\n\n### Task 1: Implement TechStack Enum\n- [ ] Create `src/detection/tech_stack.rs`\n- [ ] Define all tech stack variants\n- [ ] Implement display_name, related_stacks, skill_tags methods\n- [ ] Add serialization support\n\n### Task 2: Implement Indicator System\n- [ ] Create `src/detection/indicators.rs`\n- [ ] Define IndicatorKind enum\n- [ ] Implement indicator checking for each type\n- [ ] Add version extraction patterns\n\n### Task 3: Implement TechStackDetector\n- [ ] Create `src/detection/detector.rs`\n- [ ] Register default indicators for all stacks\n- [ ] Implement detection algorithm with confidence scoring\n- [ ] Add caching with invalidation\n\n### Task 4: Implement ProjectToolchain Detection\n- [ ] Create `src/detection/toolchain.rs`\n- [ ] Detect Node version from various sources\n- [ ] Detect Rust version from rust-toolchain\n- [ ] Detect Python version from various sources\n- [ ] Detect package managers\n\n### Task 5: Integrate with Suggestion Engine\n- [ ] Add TechStackFilter to suggestion pipeline\n- [ ] Wire up skill filtering by detected stack\n- [ ] Add stack-based score boosting\n- [ ] Update context fingerprint to include stack\n\n### Task 6: Add CLI Commands\n- [ ] Add `ms detect` command\n- [ ] Add `--show-indicators` flag for debugging\n- [ ] Add JSON output format\n- [ ] Add `ms suggest --stack \u003cstack\u003e` override\n\n## Acceptance Criteria\n\n1. **Detection Accuracy**: Common stacks detected with \u003e90% accuracy\n2. **Performance**: Detection completes in \u003c100ms for typical projects\n3. **Caching**: Repeated detection uses cache\n4. **Version Extraction**: Versions extracted where available\n5. **Toolchain Detection**: Package managers and versions detected\n6. **Integration**: Suggestions filtered by detected stack\n7. **CLI**: Detection results displayed clearly\n\n## Testing Requirements\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::tempdir;\n    \n    #[test]\n    fn test_detect_rust_project() {\n        let dir = tempdir().unwrap();\n        std::fs::write(dir.path().join(\"Cargo.toml\"), r#\"\n            [package]\n            name = \"test\"\n            edition = \"2021\"\n        \"#).unwrap();\n        \n        let mut detector = TechStackDetector::new();\n        let result = detector.detect(dir.path());\n        \n        assert!(result.has_stack(TechStack::Rust));\n        assert_eq!(result.primary_stack().unwrap().stack, TechStack::Rust);\n    }\n    \n    #[test]\n    fn test_detect_nextjs_project() {\n        let dir = tempdir().unwrap();\n        std::fs::write(dir.path().join(\"package.json\"), r#\"{\n            \"dependencies\": {\n                \"next\": \"^14.0.0\",\n                \"react\": \"^18.0.0\"\n            }\n        }\"#).unwrap();\n        std::fs::write(dir.path().join(\"next.config.js\"), \"\").unwrap();\n        \n        let mut detector = TechStackDetector::new();\n        let result = detector.detect(dir.path());\n        \n        assert!(result.has_stack(TechStack::NextJs));\n        assert!(result.has_stack(TechStack::React));\n    }\n    \n    #[test]\n    fn test_version_extraction() {\n        // Test version patterns\n    }\n    \n    #[test]\n    fn test_package_manager_detection() {\n        // Test lock file detection\n    }\n    \n    #[test]\n    fn test_caching() {\n        // Test that repeated calls use cache\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n#[tokio::test]\nasync fn test_suggestion_filtering_by_stack() {\n    // Create test project with stack\n    // Get suggestions\n    // Verify only matching skills returned\n}\n\n#[tokio::test]\nasync fn test_real_project_detection() {\n    // Test against actual project structures\n}\n```\n\n### Logging Requirements\n\n```rust\n// DEBUG level\nlog::debug!(\"Checking indicator {:?} for {:?}\", indicator.kind, stack);\nlog::debug!(\"Indicator matched: {:?}\", indicator);\nlog::debug!(\"Extracting version with pattern: {}\", pattern);\n\n// INFO level\nlog::info!(\"Detected tech stack: {} with {:.0}% confidence\", stack, confidence * 100.0);\nlog::info!(\"Project toolchain: {:?}\", toolchain);\n\n// WARN level\nlog::warn!(\"Could not read manifest file: {}\", path);\nlog::warn!(\"Version pattern failed to match\");\n\n// ERROR level\nlog::error!(\"Detection failed: {}\", error);\n```\n\n## Dependencies\n\n- **Depends on**: `meta_skill-o8o` (Context-Aware Suggestions) - Provides suggestion infrastructure\n- **Blocks**: ms build features in Phase 4 - Stack detection needed for build\n\n## References\n\n- Plan Section 5.8: Tech stack detection concept\n- Plan Section 7.2: Context-aware suggestions (uses detection)","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:59:50.156123903-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:50.156123903-05:00","labels":["detection","phase-3","techstack"],"dependencies":[{"issue_id":"meta_skill-ftj","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:00:23.718145895-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-fus","title":"[P1] Two-Phase Commit (2PC)","description":"## Overview\n\nImplement a lightweight two-phase commit (2PC) mechanism to prevent split-brain states between SQLite and Git. All writes that touch both storage backends are wrapped in a transaction record that enables automatic recovery on startup. This is critical for maintaining data integrity in the dual persistence architecture.\n\n## Background \u0026 Rationale\n\n### The Split-Brain Problem\n\nWith dual persistence (SQLite + Git), a crash between writes can leave data inconsistent:\n\n```\n1. Write skill to SQLite \n2. [CRASH]\n3. Git commit never happens\n SQLite has skill, Git does not = SPLIT BRAIN\n```\n\nWithout 2PC, recovery is manual and error-prone:\n- User must detect inconsistency\n- User must decide which version is correct\n- User must manually fix the mismatch\n\n### Why 2PC Solves This\n\nTwo-phase commit provides:\n1. **Atomicity**: Either both stores update, or neither does\n2. **Durability**: Crash at any point is recoverable\n3. **Automatic Recovery**: Startup detects and completes/rolls back incomplete transactions\n\n### The Algorithm\n\n```\nPhase 1 (Prepare):\n1. Generate unique transaction ID (UUID)\n2. Write intent record to .ms/tx/\u003cuuid\u003e.json\n3. Record: operation type, target skill, timestamp\n\nPhase 2 (Commit):\n4. Apply SQLite changes\n5. Apply Git changes  \n6. Remove intent record (transaction complete)\n\nRecovery (on startup):\n- Scan .ms/tx/ for incomplete transactions\n- For each: determine state and complete or rollback\n```\n\n### Why This Is Lightweight\n\nUnlike distributed 2PC (which requires network coordination), this is:\n- **Single-node**: Both stores are local\n- **Sequential**: No parallel coordination needed\n- **File-based**: Intent record is just a JSON file\n- **Idempotent**: Recovery can be run multiple times safely\n\n## Key Data Structures (from Plan Section 3.7)\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse uuid::Uuid;\nuse std::path::{Path, PathBuf};\n\n/// Transaction manager for dual persistence\npub struct TxManager {\n    /// Path to transaction intent directory\n    tx_dir: PathBuf,\n    /// Reference to SQLite database\n    db: Arc\u003cMutex\u003cDatabase\u003e\u003e,\n    /// Reference to Git archive\n    git: Arc\u003cMutex\u003cGitArchive\u003e\u003e,\n}\n\n/// A single transaction record\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TxRecord {\n    /// Unique transaction identifier\n    pub id: Uuid,\n    /// Operation being performed\n    pub operation: TxOperation,\n    /// When the transaction started\n    pub started_at: DateTime\u003cUtc\u003e,\n    /// Current state of the transaction\n    pub state: TxState,\n    /// Target skill ID (if applicable)\n    pub skill_id: Option\u003cString\u003e,\n    /// Additional operation metadata\n    pub metadata: serde_json::Value,\n}\n\n/// Transaction operations\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"snake_case\")]\npub enum TxOperation {\n    /// Create a new skill\n    CreateSkill { skill_id: String },\n    /// Update an existing skill\n    UpdateSkill { skill_id: String },\n    /// Delete a skill\n    DeleteSkill { skill_id: String },\n    /// Bulk index operation\n    BulkIndex { skill_count: usize },\n    /// Run migrations\n    Migrate { from_version: u32, to_version: u32 },\n}\n\n/// Transaction state machine\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum TxState {\n    /// Intent recorded, no changes made yet\n    Prepared,\n    /// SQLite changes applied\n    SqliteCommitted,\n    /// Git changes applied (transaction complete)\n    GitCommitted,\n    /// Transaction rolled back\n    RolledBack,\n    /// Transaction abandoned (error during recovery)\n    Abandoned,\n}\n\nimpl TxManager {\n    /// Create a new transaction manager\n    pub fn new(\n        tx_dir: impl AsRef\u003cPath\u003e,\n        db: Arc\u003cMutex\u003cDatabase\u003e\u003e,\n        git: Arc\u003cMutex\u003cGitArchive\u003e\u003e,\n    ) -\u003e Result\u003cSelf\u003e {\n        let tx_dir = tx_dir.as_ref().to_path_buf();\n        std::fs::create_dir_all(\u0026tx_dir)?;\n        \n        Ok(Self { tx_dir, db, git })\n    }\n    \n    /// Run recovery on startup\n    pub fn recover(\u0026self) -\u003e Result\u003cRecoveryReport\u003e {\n        tracing::info\\!(\"Running transaction recovery...\");\n        \n        let mut report = RecoveryReport::default();\n        \n        for entry in std::fs::read_dir(\u0026self.tx_dir)? {\n            let entry = entry?;\n            if entry.path().extension() == Some(\"json\".as_ref()) {\n                let record = self.load_record(\u0026entry.path())?;\n                \n                match self.recover_transaction(\u0026record) {\n                    Ok(action) =\u003e {\n                        report.recovered.push((record.id, action));\n                    }\n                    Err(e) =\u003e {\n                        tracing::error\\!(\"Failed to recover tx {}: {}\", record.id, e);\n                        report.failed.push((record.id, e.to_string()));\n                    }\n                }\n            }\n        }\n        \n        tracing::info\\!(\"Recovery complete: {} recovered, {} failed\",\n            report.recovered.len(), report.failed.len());\n        \n        Ok(report)\n    }\n    \n    /// Recover a single incomplete transaction\n    fn recover_transaction(\u0026self, record: \u0026TxRecord) -\u003e Result\u003cRecoveryAction\u003e {\n        match record.state {\n            TxState::Prepared =\u003e {\n                // No changes made yet, safe to abort\n                self.remove_record(record.id)?;\n                Ok(RecoveryAction::Aborted)\n            }\n            TxState::SqliteCommitted =\u003e {\n                // SQLite done, need to complete Git\n                self.complete_git_phase(record)?;\n                self.remove_record(record.id)?;\n                Ok(RecoveryAction::CompletedGit)\n            }\n            TxState::GitCommitted =\u003e {\n                // Already complete, just remove stale record\n                self.remove_record(record.id)?;\n                Ok(RecoveryAction::CleanedUp)\n            }\n            TxState::RolledBack | TxState::Abandoned =\u003e {\n                // Already handled\n                self.remove_record(record.id)?;\n                Ok(RecoveryAction::CleanedUp)\n            }\n        }\n    }\n    \n    /// Begin a new transaction\n    pub fn begin(\u0026self, operation: TxOperation) -\u003e Result\u003cTxRecord\u003e {\n        let record = TxRecord {\n            id: Uuid::new_v4(),\n            operation,\n            started_at: Utc::now(),\n            state: TxState::Prepared,\n            skill_id: None,\n            metadata: serde_json::Value::Null,\n        };\n        \n        self.save_record(\u0026record)?;\n        tracing::debug\\!(\"Transaction {} begun: {:?}\", record.id, record.operation);\n        \n        Ok(record)\n    }\n    \n    /// Execute a skill write operation with 2PC\n    pub fn write_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003c()\u003e {\n        let tx = self.begin(TxOperation::UpdateSkill {\n            skill_id: skill.id.clone(),\n        })?;\n        \n        // Phase 1: SQLite\n        {\n            let db = self.db.lock().unwrap();\n            db.upsert_skill(skill)?;\n        }\n        self.update_state(\u0026tx, TxState::SqliteCommitted)?;\n        \n        // Phase 2: Git\n        {\n            let git = self.git.lock().unwrap();\n            git.write_skill(skill)?;\n        }\n        self.update_state(\u0026tx, TxState::GitCommitted)?;\n        \n        // Complete\n        self.remove_record(tx.id)?;\n        tracing::info\\!(\"Transaction {} completed successfully\", tx.id);\n        \n        Ok(())\n    }\n    \n    /// Delete a skill with 2PC\n    pub fn delete_skill(\u0026self, skill_id: \u0026str) -\u003e Result\u003c()\u003e {\n        let tx = self.begin(TxOperation::DeleteSkill {\n            skill_id: skill_id.to_string(),\n        })?;\n        \n        // Phase 1: SQLite\n        {\n            let db = self.db.lock().unwrap();\n            db.delete_skill(skill_id)?;\n        }\n        self.update_state(\u0026tx, TxState::SqliteCommitted)?;\n        \n        // Phase 2: Git\n        {\n            let git = self.git.lock().unwrap();\n            git.delete_skill(skill_id)?;\n        }\n        self.update_state(\u0026tx, TxState::GitCommitted)?;\n        \n        // Complete\n        self.remove_record(tx.id)?;\n        tracing::info\\!(\"Transaction {} (delete) completed\", tx.id);\n        \n        Ok(())\n    }\n    \n    /// Save transaction record to disk\n    fn save_record(\u0026self, record: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        let path = self.tx_dir.join(format\\!(\"{}.json\", record.id));\n        let json = serde_json::to_string_pretty(record)?;\n        \n        // Atomic write: temp file + rename\n        let temp_path = path.with_extension(\"json.tmp\");\n        std::fs::write(\u0026temp_path, \u0026json)?;\n        std::fs::rename(\u0026temp_path, \u0026path)?;\n        \n        Ok(())\n    }\n    \n    /// Update transaction state\n    fn update_state(\u0026self, tx: \u0026TxRecord, state: TxState) -\u003e Result\u003c()\u003e {\n        let mut updated = tx.clone();\n        updated.state = state;\n        self.save_record(\u0026updated)?;\n        tracing::debug\\!(\"Transaction {} state: {:?} -\u003e {:?}\", tx.id, tx.state, state);\n        Ok(())\n    }\n    \n    /// Load transaction record from disk\n    fn load_record(\u0026self, path: \u0026Path) -\u003e Result\u003cTxRecord\u003e {\n        let json = std::fs::read_to_string(path)?;\n        let record = serde_json::from_str(\u0026json)?;\n        Ok(record)\n    }\n    \n    /// Remove completed transaction record\n    fn remove_record(\u0026self, id: Uuid) -\u003e Result\u003c()\u003e {\n        let path = self.tx_dir.join(format\\!(\"{}.json\", id));\n        if path.exists() {\n            std::fs::remove_file(\u0026path)?;\n        }\n        Ok(())\n    }\n    \n    /// Complete the Git phase of a recovered transaction\n    fn complete_git_phase(\u0026self, record: \u0026TxRecord) -\u003e Result\u003c()\u003e {\n        match \u0026record.operation {\n            TxOperation::UpdateSkill { skill_id } | TxOperation::CreateSkill { skill_id } =\u003e {\n                // Re-read from SQLite and write to Git\n                let db = self.db.lock().unwrap();\n                if let Some(skill) = db.get_skill(skill_id)? {\n                    drop(db);\n                    let git = self.git.lock().unwrap();\n                    git.write_skill(\u0026skill)?;\n                }\n            }\n            TxOperation::DeleteSkill { skill_id } =\u003e {\n                let git = self.git.lock().unwrap();\n                git.delete_skill(skill_id)?;\n            }\n            _ =\u003e {\n                tracing::warn\\!(\"Cannot complete {:?} operation\", record.operation);\n            }\n        }\n        Ok(())\n    }\n}\n\n/// Recovery action taken for a transaction\n#[derive(Debug, Clone, Copy)]\npub enum RecoveryAction {\n    /// Transaction was aborted (no changes had been made)\n    Aborted,\n    /// Completed the Git phase\n    CompletedGit,\n    /// Rolled back SQLite changes\n    RolledBackSqlite,\n    /// Just cleaned up stale record\n    CleanedUp,\n}\n\n/// Report from recovery process\n#[derive(Debug, Default)]\npub struct RecoveryReport {\n    /// Successfully recovered transactions\n    pub recovered: Vec\u003c(Uuid, RecoveryAction)\u003e,\n    /// Failed recovery attempts\n    pub failed: Vec\u003c(Uuid, String)\u003e,\n}\n```\n\n## Transaction Intent File Format\n\n```json\n// .ms/tx/550e8400-e29b-41d4-a716-446655440000.json\n{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"operation\": {\n    \"type\": \"update_skill\",\n    \"skill_id\": \"rust-debugging\"\n  },\n  \"started_at\": \"2026-01-13T12:00:00.000Z\",\n  \"state\": \"sqlite_committed\",\n  \"skill_id\": \"rust-debugging\",\n  \"metadata\": null\n}\n```\n\n## Tasks\n\n### Task 1: Create TxManager Structure\n- [ ] Create `src/tx/mod.rs` module\n- [ ] Define `TxManager` struct with db and git references\n- [ ] Create `.ms/tx/` directory on initialization\n- [ ] Implement `TxManager::new()` constructor\n\n### Task 2: Define Transaction Record Types\n- [ ] Define `TxRecord` struct with serde\n- [ ] Define `TxOperation` enum with all operation types\n- [ ] Define `TxState` enum with state machine states\n- [ ] Implement serialization/deserialization\n\n### Task 3: Implement Record Persistence\n- [ ] Implement `save_record()` with atomic write\n- [ ] Implement `load_record()` from JSON file\n- [ ] Implement `update_state()` for state transitions\n- [ ] Implement `remove_record()` on completion\n- [ ] Use temp file + rename for atomicity\n\n### Task 4: Implement 2PC Write Flow\n- [ ] Implement `begin()` to create transaction record\n- [ ] Implement `write_skill()` with both phases\n- [ ] Update state after each phase\n- [ ] Remove record on success\n- [ ] Handle errors at each phase\n\n### Task 5: Implement 2PC Delete Flow\n- [ ] Implement `delete_skill()` with both phases\n- [ ] Ensure SQLite foreign keys cascade properly\n- [ ] Handle non-existent skills gracefully\n- [ ] Clean up Git directory\n\n### Task 6: Implement Recovery\n- [ ] Implement `recover()` to scan tx directory\n- [ ] Implement `recover_transaction()` per-record\n- [ ] Handle each state appropriately\n- [ ] Complete incomplete transactions\n- [ ] Abort uncommitted transactions\n- [ ] Clean up stale records\n\n### Task 7: Integration with doctor Command\n- [ ] Add `doctor --fix` support for manual recovery\n- [ ] Report incomplete transactions in `doctor` output\n- [ ] Allow force-abort of stuck transactions\n- [ ] Log all recovery actions\n\n### Task 8: Bulk Operations\n- [ ] Implement bulk `index` operation with single tx\n- [ ] Batch multiple skill writes efficiently\n- [ ] Handle partial failures in bulk ops\n- [ ] Report progress during bulk operations\n\n## Acceptance Criteria\n\n1. **No Split Brain**: Crash at any point is recoverable\n2. **Automatic Recovery**: Startup completes incomplete transactions\n3. **Idempotent Recovery**: Running recovery twice is safe\n4. **State Transitions**: States progress correctly through machine\n5. **Atomic Writes**: Intent files written atomically\n6. **Clean Completion**: No stale tx files after success\n7. **doctor Integration**: `ms doctor --fix` triggers recovery\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::tempdir;\n    \n    #[test]\n    fn test_transaction_lifecycle() {\n        let dir = tempdir().unwrap();\n        let tx_dir = dir.path().join(\"tx\");\n        \n        let db = Arc::new(Mutex::new(Database::open_in_memory().unwrap()));\n        let git = Arc::new(Mutex::new(GitArchive::open(dir.path()).unwrap()));\n        let manager = TxManager::new(\u0026tx_dir, db.clone(), git.clone()).unwrap();\n        \n        // Begin transaction\n        let tx = manager.begin(TxOperation::CreateSkill {\n            skill_id: \"test\".into(),\n        }).unwrap();\n        \n        // Verify record exists\n        let record_path = tx_dir.join(format\\!(\"{}.json\", tx.id));\n        assert\\!(record_path.exists());\n        \n        // Verify state\n        let loaded = manager.load_record(\u0026record_path).unwrap();\n        assert_eq\\!(loaded.state, TxState::Prepared);\n    }\n    \n    #[test]\n    fn test_state_transitions() {\n        let dir = tempdir().unwrap();\n        let tx_dir = dir.path().join(\"tx\");\n        \n        let db = Arc::new(Mutex::new(Database::open_in_memory().unwrap()));\n        let git = Arc::new(Mutex::new(GitArchive::open(dir.path()).unwrap()));\n        let manager = TxManager::new(\u0026tx_dir, db, git).unwrap();\n        \n        let tx = manager.begin(TxOperation::UpdateSkill {\n            skill_id: \"test\".into(),\n        }).unwrap();\n        \n        // Progress through states\n        manager.update_state(\u0026tx, TxState::SqliteCommitted).unwrap();\n        \n        let record_path = tx_dir.join(format\\!(\"{}.json\", tx.id));\n        let loaded = manager.load_record(\u0026record_path).unwrap();\n        assert_eq\\!(loaded.state, TxState::SqliteCommitted);\n    }\n    \n    #[test]\n    fn test_recovery_aborts_prepared() {\n        let dir = tempdir().unwrap();\n        let tx_dir = dir.path().join(\"tx\");\n        std::fs::create_dir_all(\u0026tx_dir).unwrap();\n        \n        // Create a \"stuck\" prepared transaction\n        let record = TxRecord {\n            id: Uuid::new_v4(),\n            operation: TxOperation::CreateSkill { skill_id: \"orphan\".into() },\n            started_at: Utc::now(),\n            state: TxState::Prepared,\n            skill_id: Some(\"orphan\".into()),\n            metadata: serde_json::Value::Null,\n        };\n        let record_path = tx_dir.join(format\\!(\"{}.json\", record.id));\n        std::fs::write(\u0026record_path, serde_json::to_string(\u0026record).unwrap()).unwrap();\n        \n        let db = Arc::new(Mutex::new(Database::open_in_memory().unwrap()));\n        let git = Arc::new(Mutex::new(GitArchive::open(dir.path()).unwrap()));\n        let manager = TxManager::new(\u0026tx_dir, db, git).unwrap();\n        \n        let report = manager.recover().unwrap();\n        \n        // Should have aborted the prepared transaction\n        assert_eq\\!(report.recovered.len(), 1);\n        assert\\!(matches\\!(report.recovered[0].1, RecoveryAction::Aborted));\n        \n        // Record should be removed\n        assert\\!(\\!record_path.exists());\n    }\n    \n    #[test]\n    fn test_recovery_completes_sqlite_committed() {\n        let dir = tempdir().unwrap();\n        let tx_dir = dir.path().join(\"tx\");\n        std::fs::create_dir_all(\u0026tx_dir).unwrap();\n        \n        let db = Arc::new(Mutex::new(Database::open_in_memory().unwrap()));\n        let git = Arc::new(Mutex::new(GitArchive::open(dir.path()).unwrap()));\n        \n        // Insert skill to SQLite (simulating partial commit)\n        {\n            let db_lock = db.lock().unwrap();\n            db_lock.insert_skill(\u0026Skill {\n                id: \"partial\".into(),\n                spec: SkillSpec::default(),\n                ..Default::default()\n            }).unwrap();\n        }\n        \n        // Create tx record in SqliteCommitted state\n        let record = TxRecord {\n            id: Uuid::new_v4(),\n            operation: TxOperation::UpdateSkill { skill_id: \"partial\".into() },\n            started_at: Utc::now(),\n            state: TxState::SqliteCommitted,\n            skill_id: Some(\"partial\".into()),\n            metadata: serde_json::Value::Null,\n        };\n        let record_path = tx_dir.join(format\\!(\"{}.json\", record.id));\n        std::fs::write(\u0026record_path, serde_json::to_string(\u0026record).unwrap()).unwrap();\n        \n        let manager = TxManager::new(\u0026tx_dir, db, git.clone()).unwrap();\n        let report = manager.recover().unwrap();\n        \n        // Should have completed Git phase\n        assert_eq\\!(report.recovered.len(), 1);\n        assert\\!(matches\\!(report.recovered[0].1, RecoveryAction::CompletedGit));\n        \n        // Verify skill now exists in Git\n        let git_lock = git.lock().unwrap();\n        assert\\!(git_lock.skill_path(\"partial\").join(\"skill.spec.yaml\").exists());\n    }\n    \n    #[test]\n    fn test_atomic_write() {\n        let dir = tempdir().unwrap();\n        let tx_dir = dir.path().join(\"tx\");\n        \n        let db = Arc::new(Mutex::new(Database::open_in_memory().unwrap()));\n        let git = Arc::new(Mutex::new(GitArchive::open(dir.path()).unwrap()));\n        let manager = TxManager::new(\u0026tx_dir, db, git).unwrap();\n        \n        let tx = manager.begin(TxOperation::CreateSkill {\n            skill_id: \"atomic\".into(),\n        }).unwrap();\n        \n        // Verify no .tmp file remains\n        let tmp_path = tx_dir.join(format\\!(\"{}.json.tmp\", tx.id));\n        assert\\!(\\!tmp_path.exists());\n        \n        // Verify main file exists\n        let main_path = tx_dir.join(format\\!(\"{}.json\", tx.id));\n        assert\\!(main_path.exists());\n    }\n}\n```\n\n### Crash Simulation Tests\n```rust\n#[test]\nfn test_simulated_crash_after_sqlite() {\n    // This test simulates what happens if ms crashes after SQLite commit\n    // but before Git commit\n    \n    let dir = tempdir().unwrap();\n    let tx_dir = dir.path().join(\"tx\");\n    \n    let db = Arc::new(Mutex::new(Database::open_in_memory().unwrap()));\n    let git = Arc::new(Mutex::new(GitArchive::open(dir.path()).unwrap()));\n    \n    let skill = Skill {\n        id: \"crash-test\".into(),\n        spec: SkillSpec { name: \"Crash Test\".into(), ..Default::default() },\n        ..Default::default()\n    };\n    \n    // Manually execute partial transaction (simulating crash)\n    let tx_id = Uuid::new_v4();\n    let record = TxRecord {\n        id: tx_id,\n        operation: TxOperation::CreateSkill { skill_id: skill.id.clone() },\n        started_at: Utc::now(),\n        state: TxState::SqliteCommitted,\n        skill_id: Some(skill.id.clone()),\n        metadata: serde_json::Value::Null,\n    };\n    \n    std::fs::create_dir_all(\u0026tx_dir).unwrap();\n    let record_path = tx_dir.join(format\\!(\"{}.json\", tx_id));\n    std::fs::write(\u0026record_path, serde_json::to_string(\u0026record).unwrap()).unwrap();\n    \n    // Write to SQLite only\n    {\n        let db_lock = db.lock().unwrap();\n        db_lock.insert_skill(\u0026skill).unwrap();\n    }\n    \n    // Verify Git does not have the skill\n    {\n        let git_lock = git.lock().unwrap();\n        assert\\!(\\!git_lock.skill_path(\"crash-test\").exists());\n    }\n    \n    // Run recovery (simulating restart after crash)\n    let manager = TxManager::new(\u0026tx_dir, db.clone(), git.clone()).unwrap();\n    let report = manager.recover().unwrap();\n    \n    // Recovery should have completed the transaction\n    assert_eq\\!(report.recovered.len(), 1);\n    assert\\!(matches\\!(report.recovered[0].1, RecoveryAction::CompletedGit));\n    \n    // Now Git should have the skill\n    {\n        let git_lock = git.lock().unwrap();\n        assert\\!(git_lock.skill_path(\"crash-test\").exists());\n    }\n}\n```\n\n### Logging Requirements\nAll 2PC operations must log:\n- `DEBUG`: State transitions, record file operations\n- `INFO`: Transaction begin/complete, recovery actions\n- `WARN`: Recovery needed, incomplete transactions found\n- `ERROR`: Recovery failures, unrecoverable states\n\nExample log output:\n```\n[INFO] Transaction 550e8400... begun: UpdateSkill { skill_id: \"rust-debugging\" }\n[DEBUG] Transaction 550e8400... state: Prepared -\u003e SqliteCommitted\n[DEBUG] Transaction 550e8400... state: SqliteCommitted -\u003e GitCommitted\n[INFO] Transaction 550e8400... completed successfully\n\n[WARN] Running transaction recovery...\n[WARN] Found 1 incomplete transaction(s)\n[INFO] Recovering transaction 660f9500... (state: SqliteCommitted)\n[INFO] Completed Git phase for transaction 660f9500...\n[INFO] Recovery complete: 1 recovered, 0 failed\n```\n\n## References\n\n- Plan Section 3.7: Two-Phase Commit for Dual Persistence\n- Plan Section 3.7.1: Global File Locking\n- mcp_agent_mail dual persistence implementation\n- Depends on: meta_skill-qs1 (SQLite), meta_skill-b98 (Git Archive)\n- Blocks: meta_skill-igx (Global File Locking)\n\nLabels: [phase-1 safety transaction]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:02.680560145-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:42:29.670753383-05:00","labels":["phase-1","safety","transaction"],"dependencies":[{"issue_id":"meta_skill-fus","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:22:14.902149258-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-fus","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:22:14.928783293-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-hax","title":"CASS Mining: Caching/Memoization Patterns","description":"Deep dive into topk heap-based collectors, lazy cached accessors (TriageContext), memoization patterns, LRU cache implementations.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T17:47:29.587581417-05:00","created_by":"ubuntu","updated_at":"2026-01-13T20:59:57.634708683-05:00","closed_at":"2026-01-13T20:59:57.634708683-05:00","close_reason":"Completed CASS mining for caching/memoization patterns. Added Section 36 (~1105 lines) covering: lazy initialization (OnceLock, sync.Once), TriageContext pattern for unified lazy caching, heap-based top-K collectors (Go generics + Rust BinaryHeap), LRU cache with disk persistence, in-memory cache with TTL, SIMD-optimized dot product, parallel k-NN search with thread-local heaps, cache-efficient SoA data layouts, hash-based content deduplication, and cache invalidation strategies. Sources: beads_viewer, xf, cass vector search implementations.","labels":["cass-mining"]}
{"id":"meta_skill-hhu","title":"[P4] CASS Client Integration","description":"# CASS Client Integration\n\nSubprocess-based integration with CASS (Coding Agent Session Search).\n\n## Tasks\n1. Define CassClient struct\n2. Spawn cass subprocess with --robot mode\n3. Parse JSON responses\n4. Handle errors and timeouts\n5. Query interface for skill mining\n\n## CASS Commands Used (from Section 8.1)\n- `cass search --robot \"query\"` - Full-text search\n- `cass recent --robot --limit N` - Recent sessions\n- `cass show --robot \u003csession_id\u003e` - Session details\n- `cass index --robot` - Trigger re-indexing\n\n## Query Interface\n```rust\ntrait CassClient {\n    async fn search(\u0026self, query: \u0026str, limit: usize) -\u003e Result\u003cVec\u003cSessionMatch\u003e\u003e;\n    async fn get_session(\u0026self, id: \u0026str) -\u003e Result\u003cSession\u003e;\n    async fn recent(\u0026self, limit: usize) -\u003e Result\u003cVec\u003cSessionSummary\u003e\u003e;\n}\n```\n\n## Error Handling\n- CASS not installed  helpful error message\n- Timeout  configurable, default 30s\n- Parse error  log and continue\n\n## Acceptance Criteria\n- CASS queries work via subprocess\n- Robot mode output parsed correctly\n- Errors handled gracefully","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:45.444283967-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:45.444283967-05:00","labels":["cass","integration","phase-4"],"dependencies":[{"issue_id":"meta_skill-hhu","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:26:12.90516122-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-hzg","title":"CASS Mining: APR Iterative Refinement Patterns","description":"Deep dive into APR (Automated Plan Reviser Pro) sessions - iterative specification refinement, steady state convergence, robot mode JSON API for automation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:30.342465618-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:09:50.920724713-05:00","closed_at":"2026-01-13T18:09:50.920724713-05:00","close_reason":"Section 29 added to plan: APR iterative refinement patterns, convergence algorithm, grounded abstraction principle, reliability features, dual interface pattern","labels":["cass-mining"]}
{"id":"meta_skill-igx","title":"[P1] Global File Locking","description":"# Global File Locking\n\nPrevent concurrent write corruption across multiple ms processes.\n\n## Tasks\n1. Implement GlobalLock struct with file-based locking\n2. Lock file with JSON payload (PID, timestamp)\n3. Stale lock detection (process no longer running)\n4. Acquire, try_acquire, acquire_timeout methods\n5. Integration with TxManager\n6. Diagnostics in doctor command\n\n## Lock Behavior by Command (from Section 3.7.1)\n| Command | Lock Type |\n|---------|-----------|\n| ms index | Exclusive |\n| ms load | None (read-only) |\n| ms search | None (read-only) |\n| ms edit | Exclusive |\n| ms mine | Exclusive |\n\n## Future: Optional msd daemon\n- Single-writer daemon holds hot indices\n- CLI becomes thin client when daemon running\n- Lower p95 latency for all operations\n\n## Acceptance Criteria\n- Concurrent writes blocked\n- Stale locks auto-cleaned\n- Lock diagnostics available","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:22:03.294997429-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:22:03.294997429-05:00","labels":["concurrency","locking","phase-1"],"dependencies":[{"issue_id":"meta_skill-igx","depends_on_id":"meta_skill-fus","type":"blocks","created_at":"2026-01-13T22:22:14.954734941-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-iim","title":"Skill Effectiveness Feedback Loop","description":"# Skill Effectiveness Feedback Loop\n\n**Phase 6 - Section 22**\n\nTrack whether skills actually help agents complete tasks successfully. This feature implements usage tracking, feedback collection, quality score updates, and A/B experimentation to continuously improve skill effectiveness.\n\n---\n\n## Overview\n\nNot all skills are equally helpful. Some may be outdated, too generic, or simply wrong. The effectiveness feedback loop measures real-world skill performance by:\n\n1. **Usage Tracking**: Record when skills are retrieved and used\n2. **Feedback Collection**: Gather explicit and implicit feedback on skill helpfulness\n3. **Quality Score Updates**: Adjust skill rankings based on evidence\n4. **A/B Experiments**: Test skill variations to find what works best\n\n---\n\n## Core Data Structures\n\n### Effectiveness Tracker\n\n```rust\nuse chrono::{DateTime, Utc};\nuse rusqlite::Connection;\nuse std::collections::HashMap;\n\n/// Main effectiveness tracking system\npub struct EffectivenessTracker {\n    /// SQLite database for persistent storage\n    db: Database,\n    \n    /// CASS client for session context\n    cass: CassClient,\n    \n    /// In-memory cache of recent events\n    event_cache: EventCache,\n    \n    /// Active experiments\n    experiments: HashMap\u003cString, SkillExperiment\u003e,\n}\n\n/// Database wrapper with effectiveness-specific operations\npub struct Database {\n    conn: Connection,\n}\n\nimpl Database {\n    /// Initialize effectiveness tracking tables\n    pub fn init_schema(\u0026self) -\u003e Result\u003c(), DbError\u003e {\n        self.conn.execute_batch(r#\"\n            -- Skill usage events\n            CREATE TABLE IF NOT EXISTS skill_usage (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                skill_id TEXT NOT NULL,\n                session_id TEXT NOT NULL,\n                timestamp TEXT NOT NULL,\n                context_type TEXT NOT NULL,\n                retrieval_rank INTEGER,\n                tokens_used INTEGER,\n                experiment_id TEXT,\n                variant_id TEXT\n            );\n            \n            -- Explicit feedback\n            CREATE TABLE IF NOT EXISTS skill_feedback (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                skill_id TEXT NOT NULL,\n                session_id TEXT NOT NULL,\n                timestamp TEXT NOT NULL,\n                feedback_type TEXT NOT NULL,\n                rating INTEGER,\n                comment TEXT,\n                section TEXT,\n                slice TEXT\n            );\n            \n            -- Session outcomes (implicit feedback)\n            CREATE TABLE IF NOT EXISTS session_outcomes (\n                session_id TEXT PRIMARY KEY,\n                skills_used TEXT NOT NULL,  -- JSON array\n                outcome TEXT NOT NULL,\n                duration_seconds INTEGER,\n                error_count INTEGER,\n                completion_signals TEXT  -- JSON\n            );\n            \n            -- Aggregated skill scores\n            CREATE TABLE IF NOT EXISTS skill_scores (\n                skill_id TEXT PRIMARY KEY,\n                usage_count INTEGER DEFAULT 0,\n                positive_feedback INTEGER DEFAULT 0,\n                negative_feedback INTEGER DEFAULT 0,\n                success_rate REAL DEFAULT 0.5,\n                avg_helpfulness REAL DEFAULT 0.5,\n                last_updated TEXT NOT NULL,\n                score_version INTEGER DEFAULT 1\n            );\n            \n            -- Experiments\n            CREATE TABLE IF NOT EXISTS experiments (\n                id TEXT PRIMARY KEY,\n                skill_id TEXT NOT NULL,\n                scope TEXT NOT NULL,\n                status TEXT NOT NULL,\n                created_at TEXT NOT NULL,\n                started_at TEXT,\n                ended_at TEXT,\n                config TEXT NOT NULL  -- JSON\n            );\n            \n            -- Experiment variants\n            CREATE TABLE IF NOT EXISTS experiment_variants (\n                id TEXT PRIMARY KEY,\n                experiment_id TEXT NOT NULL,\n                name TEXT NOT NULL,\n                content TEXT NOT NULL,\n                allocation_percent REAL NOT NULL,\n                usage_count INTEGER DEFAULT 0,\n                success_count INTEGER DEFAULT 0,\n                FOREIGN KEY (experiment_id) REFERENCES experiments(id)\n            );\n            \n            -- Indexes\n            CREATE INDEX IF NOT EXISTS idx_usage_skill ON skill_usage(skill_id);\n            CREATE INDEX IF NOT EXISTS idx_usage_session ON skill_usage(session_id);\n            CREATE INDEX IF NOT EXISTS idx_feedback_skill ON skill_feedback(skill_id);\n            CREATE INDEX IF NOT EXISTS idx_outcomes_session ON session_outcomes(session_id);\n        \"#)?;\n        Ok(())\n    }\n}\n\n/// Cache for recent events before batch persistence\npub struct EventCache {\n    usage_events: Vec\u003cUsageEvent\u003e,\n    feedback_events: Vec\u003cFeedbackEvent\u003e,\n    max_size: usize,\n    flush_interval: std::time::Duration,\n    last_flush: std::time::Instant,\n}\n\nimpl EventCache {\n    pub fn new(max_size: usize, flush_interval_secs: u64) -\u003e Self {\n        Self {\n            usage_events: Vec::new(),\n            feedback_events: Vec::new(),\n            max_size,\n            flush_interval: std::time::Duration::from_secs(flush_interval_secs),\n            last_flush: std::time::Instant::now(),\n        }\n    }\n    \n    pub fn should_flush(\u0026self) -\u003e bool {\n        self.usage_events.len() \u003e= self.max_size\n            || self.feedback_events.len() \u003e= self.max_size\n            || self.last_flush.elapsed() \u003e= self.flush_interval\n    }\n}\n```\n\n### Usage Events\n\n```rust\n/// Record of a skill being used\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UsageEvent {\n    /// Skill that was used\n    pub skill_id: SkillId,\n    \n    /// Session in which skill was used\n    pub session_id: SessionId,\n    \n    /// When the skill was retrieved\n    pub timestamp: DateTime\u003cUtc\u003e,\n    \n    /// Context that triggered retrieval\n    pub context: UsageContext,\n    \n    /// Position in retrieval results (1 = top result)\n    pub retrieval_rank: Option\u003cu32\u003e,\n    \n    /// Tokens consumed by this skill\n    pub tokens_used: u32,\n    \n    /// If part of an experiment\n    pub experiment_info: Option\u003cExperimentAssignment\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct UsageContext {\n    /// Type of suggestion context\n    pub context_type: ContextType,\n    \n    /// Query that triggered retrieval (if any)\n    pub query: Option\u003cString\u003e,\n    \n    /// Files being worked on\n    pub active_files: Vec\u003cString\u003e,\n    \n    /// Project type\n    pub project_type: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ContextType {\n    /// Automatic suggestion based on context\n    Automatic,\n    \n    /// Explicit query via `ms search`\n    ExplicitSearch,\n    \n    /// Direct access via `ms show \u003cskill\u003e`\n    DirectAccess,\n    \n    /// MCP server suggestion\n    McpSuggestion,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentAssignment {\n    pub experiment_id: String,\n    pub variant_id: String,\n}\n```\n\n### Feedback Types\n\n```rust\n/// Explicit feedback on skill helpfulness\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct FeedbackEvent {\n    /// Skill being rated\n    pub skill_id: SkillId,\n    \n    /// Session providing feedback\n    pub session_id: SessionId,\n    \n    /// When feedback was provided\n    pub timestamp: DateTime\u003cUtc\u003e,\n    \n    /// Type of feedback\n    pub feedback_type: FeedbackType,\n    \n    /// Specific section if applicable\n    pub section: Option\u003cString\u003e,\n    \n    /// Specific slice if applicable\n    pub slice: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum FeedbackType {\n    /// Explicit thumbs up\n    Positive { comment: Option\u003cString\u003e },\n    \n    /// Explicit thumbs down\n    Negative { reason: NegativeReason, comment: Option\u003cString\u003e },\n    \n    /// Numeric rating (1-5)\n    Rating { value: u8, comment: Option\u003cString\u003e },\n    \n    /// Specific correction suggested\n    Correction { original: String, suggested: String },\n    \n    /// Section marked as outdated\n    Outdated { section: String },\n    \n    /// Request for more detail\n    NeedsMore { topic: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum NegativeReason {\n    NotRelevant,\n    Outdated,\n    Incorrect,\n    TooGeneric,\n    TooVerbose,\n    MissingContext,\n    Other(String),\n}\n\n/// Session outcome for implicit feedback\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SessionOutcome {\n    pub session_id: SessionId,\n    \n    /// Skills that were used in this session\n    pub skills_used: Vec\u003cSkillId\u003e,\n    \n    /// Overall outcome\n    pub outcome: Outcome,\n    \n    /// Session duration\n    pub duration: std::time::Duration,\n    \n    /// Number of errors encountered\n    pub error_count: u32,\n    \n    /// Signals indicating completion\n    pub completion_signals: Vec\u003cCompletionSignal\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Outcome {\n    /// Task completed successfully\n    Success,\n    \n    /// Task completed with issues\n    PartialSuccess,\n    \n    /// Task abandoned or failed\n    Failure,\n    \n    /// Unknown (session still active or no signal)\n    Unknown,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum CompletionSignal {\n    /// Explicit success indicator (e.g., \"Thanks, that worked!\")\n    ExplicitSuccess(String),\n    \n    /// Tests passing\n    TestsPassed { count: u32 },\n    \n    /// Build succeeded\n    BuildSucceeded,\n    \n    /// Git commit made\n    CommitMade { message: String },\n    \n    /// Explicit failure indicator\n    ExplicitFailure(String),\n    \n    /// Session ended abruptly\n    Abandoned,\n}\n```\n\n### A/B Experiments\n\n```rust\n/// Skill experiment definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillExperiment {\n    /// Unique experiment identifier\n    pub id: String,\n    \n    /// Skill being experimented on\n    pub skill_id: SkillId,\n    \n    /// Scope of the experiment\n    pub scope: ExperimentScope,\n    \n    /// Experiment variants\n    pub variants: Vec\u003cExperimentVariant\u003e,\n    \n    /// Current status\n    pub status: ExperimentStatus,\n    \n    /// When experiment was created\n    pub created_at: DateTime\u003cUtc\u003e,\n    \n    /// When experiment started\n    pub started_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// When experiment ended\n    pub ended_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// Experiment configuration\n    pub config: ExperimentConfig,\n    \n    /// Results (populated when experiment ends)\n    pub results: Option\u003cExperimentResults\u003e,\n}\n\n/// What part of the skill to experiment with\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ExperimentScope {\n    /// Experiment with the entire skill\n    EntireSkill,\n    \n    /// Experiment with a specific section\n    Section(String),\n    \n    /// Experiment with a specific slice (finest granularity)\n    Slice(String),\n}\n\n/// A variant in an experiment\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentVariant {\n    /// Variant identifier\n    pub id: String,\n    \n    /// Human-readable name\n    pub name: String,\n    \n    /// Content for this variant\n    pub content: VariantContent,\n    \n    /// Traffic allocation (0.0 - 1.0)\n    pub allocation: f64,\n    \n    /// Collected metrics\n    pub metrics: VariantMetrics,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum VariantContent {\n    /// Full skill content\n    FullSkill(Skill),\n    \n    /// Section content\n    SectionContent(String),\n    \n    /// Slice content\n    SliceContent(String),\n    \n    /// Reference to existing content (control group)\n    Control,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct VariantMetrics {\n    pub usage_count: u32,\n    pub success_count: u32,\n    pub positive_feedback: u32,\n    pub negative_feedback: u32,\n    pub avg_helpfulness: f64,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ExperimentStatus {\n    /// Experiment created but not started\n    Draft,\n    \n    /// Experiment is running\n    Running,\n    \n    /// Experiment paused\n    Paused,\n    \n    /// Experiment completed\n    Completed { winner: Option\u003cString\u003e },\n    \n    /// Experiment cancelled\n    Cancelled { reason: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentConfig {\n    /// Minimum sample size before declaring winner\n    pub min_sample_size: u32,\n    \n    /// Statistical significance threshold (e.g., 0.95)\n    pub significance_threshold: f64,\n    \n    /// Maximum duration before auto-ending\n    pub max_duration_days: u32,\n    \n    /// Primary metric to optimize\n    pub primary_metric: MetricType,\n    \n    /// Whether to auto-apply winner when significant\n    pub auto_apply_winner: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum MetricType {\n    SuccessRate,\n    PositiveFeedbackRate,\n    Helpfulness,\n    UsageRetention,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExperimentResults {\n    /// Metrics per variant\n    pub variant_results: HashMap\u003cString, VariantMetrics\u003e,\n    \n    /// Statistical analysis\n    pub analysis: StatisticalAnalysis,\n    \n    /// Winning variant (if significant)\n    pub winner: Option\u003cString\u003e,\n    \n    /// Confidence in the winner\n    pub confidence: f64,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct StatisticalAnalysis {\n    /// P-value for the comparison\n    pub p_value: f64,\n    \n    /// Effect size\n    pub effect_size: f64,\n    \n    /// Confidence interval for effect\n    pub confidence_interval: (f64, f64),\n    \n    /// Whether result is statistically significant\n    pub is_significant: bool,\n}\n```\n\n---\n\n## Slice-Level Experiments\n\nSlice-level experiments enable fine-grained A/B testing by targeting individual slices while keeping the rest of the skill constant. This approach offers several advantages:\n\n### Benefits\n\n1. **Faster Convergence**: Smaller units require fewer samples to reach significance\n2. **Precise Attribution**: Know exactly which content change caused the improvement\n3. **Lower Risk**: Testing a single slice doesn't risk the entire skill\n4. **Incremental Improvement**: Optimize skills one slice at a time\n\n### Implementation\n\n```rust\nimpl SkillExperiment {\n    /// Create a slice-level experiment\n    pub fn create_slice_experiment(\n        skill: \u0026Skill,\n        slice_id: \u0026str,\n        variants: Vec\u003c(String, String)\u003e, // (name, content) pairs\n    ) -\u003e Result\u003cSelf, ExperimentError\u003e {\n        // Validate slice exists\n        let slice = skill.find_slice(slice_id)\n            .ok_or(ExperimentError::SliceNotFound(slice_id.to_string()))?;\n        \n        // Create control variant from existing content\n        let mut experiment_variants = vec![ExperimentVariant {\n            id: \"control\".to_string(),\n            name: \"Control (Current)\".to_string(),\n            content: VariantContent::Control,\n            allocation: 0.5 / variants.len() as f64,\n            metrics: VariantMetrics::default(),\n        }];\n        \n        // Add test variants\n        let allocation_per_variant = 0.5 / variants.len() as f64;\n        for (name, content) in variants {\n            experiment_variants.push(ExperimentVariant {\n                id: format!(\"variant-{}\", name.to_lowercase().replace(' ', \"-\")),\n                name,\n                content: VariantContent::SliceContent(content),\n                allocation: allocation_per_variant,\n                metrics: VariantMetrics::default(),\n            });\n        }\n        \n        Ok(Self {\n            id: format!(\"exp-{}-{}\", skill.id.0, Uuid::new_v4()),\n            skill_id: skill.id.clone(),\n            scope: ExperimentScope::Slice(slice_id.to_string()),\n            variants: experiment_variants,\n            status: ExperimentStatus::Draft,\n            created_at: Utc::now(),\n            started_at: None,\n            ended_at: None,\n            config: ExperimentConfig::default(),\n            results: None,\n        })\n    }\n    \n    /// Get content for a session (with experiment assignment)\n    pub fn get_content_for_session(\n        \u0026mut self,\n        skill: \u0026Skill,\n        session_id: \u0026SessionId,\n    ) -\u003e (String, ExperimentAssignment) {\n        // Deterministic variant assignment based on session ID\n        let variant = self.assign_variant(session_id);\n        \n        let content = match \u0026variant.content {\n            VariantContent::Control =\u003e {\n                match \u0026self.scope {\n                    ExperimentScope::Slice(slice_id) =\u003e {\n                        skill.find_slice(slice_id)\n                            .map(|s| s.content.clone())\n                            .unwrap_or_default()\n                    }\n                    ExperimentScope::Section(section_name) =\u003e {\n                        skill.sections.get(section_name)\n                            .map(|s| s.content.clone())\n                            .unwrap_or_default()\n                    }\n                    ExperimentScope::EntireSkill =\u003e skill.render_full(),\n                }\n            }\n            VariantContent::SliceContent(content) =\u003e content.clone(),\n            VariantContent::SectionContent(content) =\u003e content.clone(),\n            VariantContent::FullSkill(skill) =\u003e skill.render_full(),\n        };\n        \n        let assignment = ExperimentAssignment {\n            experiment_id: self.id.clone(),\n            variant_id: variant.id.clone(),\n        };\n        \n        (content, assignment)\n    }\n    \n    /// Assign variant based on session ID (deterministic)\n    fn assign_variant(\u0026self, session_id: \u0026SessionId) -\u003e \u0026ExperimentVariant {\n        use std::hash::{Hash, Hasher};\n        use std::collections::hash_map::DefaultHasher;\n        \n        let mut hasher = DefaultHasher::new();\n        session_id.0.hash(\u0026mut hasher);\n        self.id.hash(\u0026mut hasher);\n        let hash = hasher.finish();\n        \n        // Convert to 0.0-1.0 range\n        let bucket = (hash % 10000) as f64 / 10000.0;\n        \n        // Find variant based on allocation\n        let mut cumulative = 0.0;\n        for variant in \u0026self.variants {\n            cumulative += variant.allocation;\n            if bucket \u003c cumulative {\n                return variant;\n            }\n        }\n        \n        // Fallback to last variant\n        self.variants.last().unwrap()\n    }\n}\n```\n\n---\n\n## Quality Score Updates\n\n```rust\nimpl EffectivenessTracker {\n    /// Update skill quality score based on new evidence\n    pub fn update_score(\u0026mut self, skill_id: \u0026SkillId) -\u003e Result\u003cQualityScore, EffectivenessError\u003e {\n        // Fetch all relevant data\n        let usage_count = self.db.get_usage_count(skill_id)?;\n        let feedback = self.db.get_feedback_summary(skill_id)?;\n        let outcomes = self.db.get_outcome_summary(skill_id)?;\n        \n        // Calculate component scores\n        let feedback_score = self.calculate_feedback_score(\u0026feedback);\n        let success_rate = self.calculate_success_rate(\u0026outcomes);\n        let recency_factor = self.calculate_recency_factor(skill_id)?;\n        \n        // Weighted combination\n        let weights = ScoreWeights::default();\n        let overall_score = \n            weights.feedback * feedback_score +\n            weights.success * success_rate +\n            weights.recency * recency_factor;\n        \n        // Apply Bayesian smoothing for low sample sizes\n        let smoothed_score = self.bayesian_smooth(overall_score, usage_count);\n        \n        // Update database\n        let score = QualityScore {\n            skill_id: skill_id.clone(),\n            overall: smoothed_score,\n            components: ScoreComponents {\n                feedback_score,\n                success_rate,\n                recency_factor,\n            },\n            confidence: self.calculate_confidence(usage_count),\n            sample_size: usage_count,\n            last_updated: Utc::now(),\n        };\n        \n        self.db.upsert_score(\u0026score)?;\n        \n        Ok(score)\n    }\n    \n    /// Bayesian smoothing to handle low sample sizes\n    fn bayesian_smooth(\u0026self, score: f64, sample_size: u32) -\u003e f64 {\n        // Prior: assume average skill (0.5)\n        let prior_mean = 0.5;\n        let prior_strength = 10.0; // Equivalent to 10 observations\n        \n        let smoothed = (prior_strength * prior_mean + sample_size as f64 * score) \n            / (prior_strength + sample_size as f64);\n        \n        smoothed\n    }\n    \n    /// Calculate confidence based on sample size\n    fn calculate_confidence(\u0026self, sample_size: u32) -\u003e f64 {\n        // Confidence grows with sample size, asymptotic to 1.0\n        let max_samples = 1000.0;\n        1.0 - (-(sample_size as f64) / max_samples).exp()\n    }\n    \n    fn calculate_feedback_score(\u0026self, feedback: \u0026FeedbackSummary) -\u003e f64 {\n        let total = feedback.positive + feedback.negative;\n        if total == 0 {\n            return 0.5; // No feedback, neutral score\n        }\n        \n        feedback.positive as f64 / total as f64\n    }\n    \n    fn calculate_success_rate(\u0026self, outcomes: \u0026OutcomeSummary) -\u003e f64 {\n        let total = outcomes.successes + outcomes.failures;\n        if total == 0 {\n            return 0.5;\n        }\n        \n        // Weight partial successes at 0.5\n        let effective_successes = outcomes.successes as f64 \n            + 0.5 * outcomes.partial_successes as f64;\n        \n        effective_successes / total as f64\n    }\n    \n    fn calculate_recency_factor(\u0026self, skill_id: \u0026SkillId) -\u003e Result\u003cf64, EffectivenessError\u003e {\n        let last_positive = self.db.get_last_positive_feedback(skill_id)?;\n        \n        match last_positive {\n            Some(timestamp) =\u003e {\n                let days_ago = (Utc::now() - timestamp).num_days() as f64;\n                // Decay factor: halve every 30 days\n                let decay = 0.5_f64.powf(days_ago / 30.0);\n                Ok(0.5 + 0.5 * decay) // Range: 0.5 to 1.0\n            }\n            None =\u003e Ok(0.5), // No recent positive feedback\n        }\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct QualityScore {\n    pub skill_id: SkillId,\n    pub overall: f64,\n    pub components: ScoreComponents,\n    pub confidence: f64,\n    pub sample_size: u32,\n    pub last_updated: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ScoreComponents {\n    pub feedback_score: f64,\n    pub success_rate: f64,\n    pub recency_factor: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ScoreWeights {\n    pub feedback: f64,\n    pub success: f64,\n    pub recency: f64,\n}\n\nimpl Default for ScoreWeights {\n    fn default() -\u003e Self {\n        Self {\n            feedback: 0.4,\n            success: 0.4,\n            recency: 0.2,\n        }\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms effectiveness report`\n\n```\nGenerate effectiveness report for a skill\n\nUSAGE:\n    ms effectiveness report \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --period \u003cDAYS\u003e     Analysis period in days [default: 30]\n    --format \u003cFMT\u003e      Output format: text, json, markdown [default: text]\n    --detailed          Include per-section breakdown\n    --compare \u003cSKILL\u003e   Compare with another skill\n\nOUTPUT EXAMPLE:\n    Effectiveness Report: rust-error-handling\n    ==========================================\n    \n    Overall Score: 0.78 (High) [Confidence: 0.92]\n    \n    Components:\n      Feedback Score:    0.85 (42 positive, 8 negative)\n      Success Rate:      0.72 (38 successes, 15 failures)\n      Recency Factor:    0.91 (last positive: 2 days ago)\n    \n    Usage Statistics (last 30 days):\n      Total Uses:        53\n      Unique Sessions:   41\n      Avg Tokens:        1,247\n      Top Context:       Automatic (67%)\n    \n    Section Breakdown:\n      overview           0.82  (12 uses)\n      error-types        0.79  (28 uses)\n      best-practices     0.74  (18 uses)\n      examples           0.88  (31 uses)\n    \n    Trends:\n      Week 1:  0.71 -\u003e Week 4: 0.78 (+9.8%)\n    \n    Recommendations:\n      - Section \"best-practices\" has lower score, consider updating\n      - High usage of \"examples\" - consider expanding\n```\n\n### `ms feedback add`\n\n```\nAdd feedback for a skill\n\nUSAGE:\n    ms feedback add \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --positive          Mark as helpful (thumbs up)\n    --negative          Mark as not helpful (thumbs down)\n    --rating \u003c1-5\u003e      Provide numeric rating\n    --reason \u003cREASON\u003e   Reason for negative feedback\n    --comment \u003cTEXT\u003e    Additional comment\n    --section \u003cNAME\u003e    Feedback for specific section\n    --slice \u003cID\u003e        Feedback for specific slice\n    --outdated          Mark section as outdated\n    --needs-more \u003cTOPIC\u003e  Request more detail on topic\n\nEXAMPLES:\n    ms feedback add rust-error-handling --positive\n    ms feedback add rust-error-handling --negative --reason outdated\n    ms feedback add rust-error-handling --rating 4 --comment \"Good examples\"\n    ms feedback add rust-error-handling --section overview --outdated\n    ms feedback add rust-error-handling --needs-more \"async error handling\"\n\nNEGATIVE REASONS:\n    not-relevant, outdated, incorrect, too-generic, too-verbose, missing-context\n```\n\n### `ms experiment create`\n\n```\nCreate a skill experiment\n\nUSAGE:\n    ms experiment create \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --scope \u003cSCOPE\u003e         Experiment scope: entire, section:\u003cname\u003e, slice:\u003cid\u003e\n    --variant \u003cNAME:FILE\u003e   Add variant from file (can specify multiple)\n    --variant-inline \u003cNAME:CONTENT\u003e  Add variant with inline content\n    --min-samples \u003cN\u003e       Minimum samples before declaring winner [default: 100]\n    --significance \u003cF\u003e      Statistical significance threshold [default: 0.95]\n    --max-days \u003cN\u003e          Maximum experiment duration [default: 30]\n    --auto-apply            Auto-apply winner when significant\n    --start                 Start experiment immediately\n\nEXAMPLES:\n    # Experiment with entire skill\n    ms experiment create rust-error-handling \\\n        --variant \"concise:variants/concise.md\" \\\n        --variant \"verbose:variants/verbose.md\"\n    \n    # Slice-level experiment\n    ms experiment create rust-error-handling \\\n        --scope slice:error-types/result-usage \\\n        --variant-inline \"shorter:Use Result\u003cT, E\u003e for recoverable errors.\" \\\n        --min-samples 50 \\\n        --auto-apply\n    \n    # Start immediately\n    ms experiment create rust-error-handling \\\n        --scope section:examples \\\n        --variant \"new-examples:new_examples.md\" \\\n        --start\n\nOTHER SUBCOMMANDS:\n    ms experiment list              List all experiments\n    ms experiment status \u003cID\u003e       Show experiment status\n    ms experiment start \u003cID\u003e        Start a draft experiment\n    ms experiment stop \u003cID\u003e         Stop a running experiment\n    ms experiment results \u003cID\u003e      Show experiment results\n    ms experiment apply \u003cID\u003e        Apply winning variant\n```\n\n---\n\n## Event Collection\n\n```rust\nimpl EffectivenessTracker {\n    /// Record a skill usage event\n    pub fn record_usage(\u0026mut self, event: UsageEvent) -\u003e Result\u003c(), EffectivenessError\u003e {\n        // Add to cache\n        self.event_cache.usage_events.push(event.clone());\n        \n        // Update experiment metrics if applicable\n        if let Some(exp_info) = \u0026event.experiment_info {\n            self.update_experiment_usage(\u0026exp_info.experiment_id, \u0026exp_info.variant_id)?;\n        }\n        \n        // Flush if needed\n        if self.event_cache.should_flush() {\n            self.flush_cache()?;\n        }\n        \n        Ok(())\n    }\n    \n    /// Record explicit feedback\n    pub fn record_feedback(\u0026mut self, event: FeedbackEvent) -\u003e Result\u003c(), EffectivenessError\u003e {\n        self.event_cache.feedback_events.push(event.clone());\n        \n        // Update score immediately for feedback (more signal)\n        self.update_score(\u0026event.skill_id)?;\n        \n        if self.event_cache.should_flush() {\n            self.flush_cache()?;\n        }\n        \n        Ok(())\n    }\n    \n    /// Record session outcome (implicit feedback)\n    pub fn record_outcome(\u0026mut self, outcome: SessionOutcome) -\u003e Result\u003c(), EffectivenessError\u003e {\n        self.db.insert_outcome(\u0026outcome)?;\n        \n        // Update scores for all skills used in session\n        for skill_id in \u0026outcome.skills_used {\n            self.update_score(skill_id)?;\n        }\n        \n        // Update experiment metrics for skills in experiments\n        for skill_id in \u0026outcome.skills_used {\n            if let Some(experiment) = self.find_active_experiment(skill_id) {\n                let success = matches!(outcome.outcome, Outcome::Success | Outcome::PartialSuccess);\n                self.record_experiment_outcome(\u0026experiment.id, success)?;\n            }\n        }\n        \n        Ok(())\n    }\n    \n    /// Flush event cache to database\n    fn flush_cache(\u0026mut self) -\u003e Result\u003c(), EffectivenessError\u003e {\n        // Batch insert usage events\n        self.db.batch_insert_usage(\u0026self.event_cache.usage_events)?;\n        self.event_cache.usage_events.clear();\n        \n        // Batch insert feedback events\n        self.db.batch_insert_feedback(\u0026self.event_cache.feedback_events)?;\n        self.event_cache.feedback_events.clear();\n        \n        self.event_cache.last_flush = std::time::Instant::now();\n        \n        Ok(())\n    }\n}\n```\n\n---\n\n## Implicit Feedback Detection\n\n```rust\n/// Detect session outcomes from CASS session data\npub struct OutcomeDetector {\n    cass: CassClient,\n}\n\nimpl OutcomeDetector {\n    /// Analyze a completed session for implicit feedback signals\n    pub fn analyze_session(\u0026self, session_id: \u0026SessionId) -\u003e Result\u003cSessionOutcome, DetectorError\u003e {\n        let session = self.cass.get_session(session_id)?;\n        \n        let mut signals = Vec::new();\n        let mut error_count = 0;\n        \n        // Analyze conversation for signals\n        for message in \u0026session.messages {\n            // Check for explicit success signals\n            if self.is_success_message(\u0026message.content) {\n                signals.push(CompletionSignal::ExplicitSuccess(\n                    self.extract_signal_text(\u0026message.content)\n                ));\n            }\n            \n            // Check for explicit failure signals\n            if self.is_failure_message(\u0026message.content) {\n                signals.push(CompletionSignal::ExplicitFailure(\n                    self.extract_signal_text(\u0026message.content)\n                ));\n            }\n            \n            // Count errors in assistant responses\n            if message.role == Role::Assistant \u0026\u0026 self.contains_error(\u0026message.content) {\n                error_count += 1;\n            }\n        }\n        \n        // Check for tool use signals\n        if let Some(tool_results) = \u0026session.tool_results {\n            for result in tool_results {\n                match result {\n                    ToolResult::TestsPassed { count } =\u003e {\n                        signals.push(CompletionSignal::TestsPassed { count: *count });\n                    }\n                    ToolResult::BuildSucceeded =\u003e {\n                        signals.push(CompletionSignal::BuildSucceeded);\n                    }\n                    ToolResult::GitCommit { message } =\u003e {\n                        signals.push(CompletionSignal::CommitMade { \n                            message: message.clone() \n                        });\n                    }\n                    _ =\u003e {}\n                }\n            }\n        }\n        \n        // Determine overall outcome\n        let outcome = self.determine_outcome(\u0026signals, error_count);\n        \n        Ok(SessionOutcome {\n            session_id: session_id.clone(),\n            skills_used: session.skills_used.clone(),\n            outcome,\n            duration: session.duration(),\n            error_count,\n            completion_signals: signals,\n        })\n    }\n    \n    fn is_success_message(\u0026self, content: \u0026str) -\u003e bool {\n        let success_patterns = [\n            \"thanks\", \"thank you\", \"that worked\", \"perfect\", \"great\",\n            \"exactly what i needed\", \"solved\", \"fixed\", \"working now\",\n        ];\n        \n        let lower = content.to_lowercase();\n        success_patterns.iter().any(|p| lower.contains(p))\n    }\n    \n    fn is_failure_message(\u0026self, content: \u0026str) -\u003e bool {\n        let failure_patterns = [\n            \"doesn't work\", \"didn't work\", \"still broken\", \"not working\",\n            \"wrong\", \"incorrect\", \"that's not right\", \"failed\",\n        ];\n        \n        let lower = content.to_lowercase();\n        failure_patterns.iter().any(|p| lower.contains(p))\n    }\n    \n    fn determine_outcome(\u0026self, signals: \u0026[CompletionSignal], error_count: u32) -\u003e Outcome {\n        let has_success = signals.iter().any(|s| matches!(s, \n            CompletionSignal::ExplicitSuccess(_) |\n            CompletionSignal::TestsPassed { .. } |\n            CompletionSignal::BuildSucceeded |\n            CompletionSignal::CommitMade { .. }\n        ));\n        \n        let has_failure = signals.iter().any(|s| matches!(s,\n            CompletionSignal::ExplicitFailure(_) |\n            CompletionSignal::Abandoned\n        ));\n        \n        match (has_success, has_failure, error_count) {\n            (true, false, _) =\u003e Outcome::Success,\n            (true, true, _) =\u003e Outcome::PartialSuccess,\n            (false, true, _) =\u003e Outcome::Failure,\n            (false, false, e) if e \u003e 3 =\u003e Outcome::Failure,\n            _ =\u003e Outcome::Unknown,\n        }\n    }\n}\n```\n\n---\n\n## Dependencies\n\n- **SQLite Database Layer** (meta_skill-qs1): Persistent storage for effectiveness data\n- `rusqlite`: Database operations\n- `chrono`: Timestamps\n- `serde`, `serde_json`: Serialization\n- `uuid`: Experiment IDs\n- Statistical library for A/B testing (e.g., `statrs`)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:56:01.703772637-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:00.673499629-05:00","labels":["effectiveness","feedback","phase-6","tracking"],"dependencies":[{"issue_id":"meta_skill-iim","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:04:14.019616413-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-iim","depends_on_id":"meta_skill-7va","type":"blocks","created_at":"2026-01-13T23:43:47.424922484-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-iim","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:43:57.958376496-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ik6","title":"[P1] SkillSpec Data Model","description":"## Overview\n\nImplement the typed SkillSpec data model with deterministic SKILL.md compilation. SkillSpec is the source-of-truth for all skill contenta structured representation that can be serialized to YAML (for human editing) and compiled to Markdown (for agent consumption). The compiler supports multiple targets (Claude, OpenAI, Cursor, generic) and produces byte-range mappings for block-level editing.\n\n## Background \u0026 Rationale\n\n### Why SkillSpec Exists\n\nSKILL.md files are consumed by AI agents, but they have problems for authoring:\n- **No Structure**: Markdown is flat text with implicit structure\n- **Brittle Editing**: Small changes can break agent parsing\n- **No Validation**: Invalid content is not detected until runtime\n- **No Multi-Target**: One source must compile to different formats\n\nSkillSpec solves these by providing:\n1. **Typed Structure**: Every element has a defined type and schema\n2. **Validation**: Invalid skills fail at compile time\n3. **Determinism**: Same spec always produces same output\n4. **Multi-Target**: Compile once, output for any agent format\n\n### Why Deterministic Compilation Matters\n\nNon-deterministic output causes:\n- **Git Churn**: Unnecessary diffs from format changes\n- **Review Noise**: Reviewers can't see real changes\n- **Caching Issues**: Hash-based caching breaks\n\nDeterministic compilation ensures:\n- Identical input  identical output\n- Clean git diffs showing only semantic changes\n- Stable caching based on content hash\n\n### The Lens System\n\nWhen editing compiled SKILL.md, we need to map byte ranges back to SkillSpec blocks. The SpecLens provides this mapping:\n\n```\nSkillSpec Block ID        SKILL.md Byte Range\n\n\"critical-rules\"          [1024, 2048]\n\"examples.0\"              [2048, 3500]\n\"pitfalls\"                [3500, 4200]\n```\n\nThis enables:\n- Block-level overlay patching\n- Semantic diff between versions\n- Targeted content updates\n\n## Key Data Structures (from Plan Section 3.1)\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\n/// Complete skill with all components\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Skill {\n    /// Unique identifier (kebab-case, e.g., \"rust-debugging\")\n    pub id: String,\n    /// Skill metadata (author, version, dependencies, etc.)\n    pub metadata: SkillMetadata,\n    /// The structured skill content\n    pub spec: SkillSpec,\n    /// Associated assets (scripts, references)\n    pub assets: SkillAssets,\n    /// Source information (layer, path, etc.)\n    pub source: SkillSource,\n    /// Computed/cached values\n    pub computed: SkillComputed,\n}\n\n/// Skill metadata\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct SkillMetadata {\n    /// Human-readable name\n    pub name: String,\n    /// Brief description\n    pub description: String,\n    /// Semantic version\n    pub version: Option\u003csemver::Version\u003e,\n    /// Author name\n    pub author: Option\u003cString\u003e,\n    /// Tags for categorization\n    #[serde(default)]\n    pub tags: Vec\u003cString\u003e,\n    /// Required skills (dependencies)\n    #[serde(default)]\n    pub requires: Vec\u003cDependencySpec\u003e,\n    /// Capabilities this skill provides\n    #[serde(default)]\n    pub provides: Vec\u003cCapabilitySpec\u003e,\n    /// Platform constraints\n    #[serde(default)]\n    pub platforms: Vec\u003cPlatform\u003e,\n    /// Network requirements\n    pub network: Option\u003cNetworkRequirement\u003e,\n    /// Alternative names (for search/rename)\n    #[serde(default)]\n    pub aliases: Vec\u003cString\u003e,\n    /// Deprecation status\n    pub deprecated: Option\u003cDeprecationInfo\u003e,\n}\n\n/// Dependency specification\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DependencySpec {\n    /// Required skill ID\n    pub skill_id: String,\n    /// Version constraint\n    pub version: Option\u003csemver::VersionReq\u003e,\n    /// If true, skill loads even without this dep\n    #[serde(default)]\n    pub optional: bool,\n}\n\n/// Capability provided by a skill\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CapabilitySpec {\n    /// Capability name\n    pub name: String,\n    /// Capability version\n    pub version: semver::Version,\n}\n\n/// Platform constraint\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum Platform {\n    Linux,\n    MacOS,\n    Windows,\n    All,\n}\n\n/// Network requirement\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum NetworkRequirement {\n    None,\n    Optional,\n    Required,\n}\n\n/// Deprecation information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DeprecationInfo {\n    /// When deprecated\n    pub since: Option\u003cString\u003e,\n    /// What to use instead\n    pub replaced_by: Option\u003cString\u003e,\n    /// Why deprecated\n    pub reason: Option\u003cString\u003e,\n}\n\n/// The structured skill specification (source of truth)\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct SkillSpec {\n    /// Display name\n    pub name: String,\n    /// Brief description\n    pub description: String,\n    /// Ordered list of sections\n    #[serde(default)]\n    pub sections: Vec\u003cSkillSectionSpec\u003e,\n}\n\n/// A single section in the skill\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillSectionSpec {\n    /// Unique block ID for this section\n    pub id: String,\n    /// Section type (determines rendering)\n    #[serde(rename = \"type\")]\n    pub section_type: SectionType,\n    /// Section title (optional, derived from type if not set)\n    pub title: Option\u003cString\u003e,\n    /// Section content (structure depends on type)\n    pub content: SectionContent,\n    /// Optional predicate for conditional inclusion\n    pub predicate: Option\u003cPredicate\u003e,\n}\n\n/// Types of sections in a skill\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum SectionType {\n    /// Critical rules that must be followed\n    CriticalRules,\n    /// General guidelines and best practices\n    Guidelines,\n    /// Command reference\n    Commands,\n    /// Code examples\n    Examples,\n    /// Common pitfalls to avoid\n    Pitfalls,\n    /// Troubleshooting decision tree\n    Troubleshooting,\n    /// Checklist items\n    Checklist,\n    /// Raw markdown content\n    Markdown,\n    /// Code block\n    Code,\n    /// Reference links\n    References,\n}\n\n/// Section content (varies by section type)\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(untagged)]\npub enum SectionContent {\n    /// List of string items (rules, guidelines, etc.)\n    Items(Vec\u003cString\u003e),\n    /// List of examples with optional titles\n    Examples(Vec\u003cExampleItem\u003e),\n    /// List of commands with descriptions\n    Commands(Vec\u003cCommandItem\u003e),\n    /// Pitfall items with fix suggestions\n    Pitfalls(Vec\u003cPitfallItem\u003e),\n    /// Decision tree for troubleshooting\n    DecisionTree(DecisionNode),\n    /// Checklist items\n    Checklist(Vec\u003cChecklistItem\u003e),\n    /// Raw markdown text\n    Markdown(String),\n    /// Code with language\n    Code { language: String, code: String },\n    /// Reference links\n    References(Vec\u003cReferenceItem\u003e),\n}\n\n/// Example item with optional metadata\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExampleItem {\n    /// Optional title for the example\n    pub title: Option\u003cString\u003e,\n    /// Description of what the example shows\n    pub description: Option\u003cString\u003e,\n    /// The code example\n    pub code: String,\n    /// Programming language\n    pub language: Option\u003cString\u003e,\n}\n\n/// Command item\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CommandItem {\n    /// The command\n    pub command: String,\n    /// What it does\n    pub description: String,\n    /// Example usage\n    pub example: Option\u003cString\u003e,\n}\n\n/// Pitfall item\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PitfallItem {\n    /// Description of the pitfall\n    pub description: String,\n    /// How to fix/avoid it\n    pub fix: Option\u003cString\u003e,\n    /// Example of the bad pattern\n    pub bad_example: Option\u003cString\u003e,\n    /// Example of the correct pattern\n    pub good_example: Option\u003cString\u003e,\n}\n\n/// Decision tree node\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DecisionNode {\n    /// Question or condition\n    pub condition: String,\n    /// If condition is true/yes\n    pub yes: Box\u003cDecisionOutcome\u003e,\n    /// If condition is false/no\n    pub no: Box\u003cDecisionOutcome\u003e,\n}\n\n/// Decision outcome (either action or another node)\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(untagged)]\npub enum DecisionOutcome {\n    /// Terminal action\n    Action(String),\n    /// Another decision node\n    Node(DecisionNode),\n}\n\n/// Checklist item\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ChecklistItem {\n    /// The item text\n    pub item: String,\n    /// Whether checked by default\n    #[serde(default)]\n    pub checked: bool,\n}\n\n/// Reference link\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ReferenceItem {\n    /// Link text\n    pub title: String,\n    /// URL\n    pub url: String,\n    /// Optional description\n    pub description: Option\u003cString\u003e,\n}\n\n/// Predicate for conditional sections\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Predicate {\n    /// Predicate type\n    #[serde(rename = \"type\")]\n    pub pred_type: PredicateType,\n    /// Predicate value\n    pub value: String,\n}\n\n/// Types of predicates\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum PredicateType {\n    /// Check environment variable\n    EnvVar,\n    /// Check file exists\n    FileExists,\n    /// Check tech stack\n    TechStack,\n    /// Version comparison\n    Version,\n}\n\n/// Assets associated with a skill\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct SkillAssets {\n    /// Executable scripts\n    #[serde(default)]\n    pub scripts: HashMap\u003cString, String\u003e,\n    /// Reference documents\n    #[serde(default)]\n    pub references: HashMap\u003cString, String\u003e,\n}\n\n/// Source information\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct SkillSource {\n    /// Which layer this skill is from\n    pub layer: Option\u003cSkillLayer\u003e,\n    /// File path on disk\n    pub path: Option\u003cString\u003e,\n    /// Content hash (for change detection)\n    pub content_hash: Option\u003cString\u003e,\n}\n\n/// Skill layer\n#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum SkillLayer {\n    System = 0,\n    Global = 1,\n    Project = 2,\n    Session = 3,\n}\n\n/// Computed/cached values\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct SkillComputed {\n    /// Token count estimate\n    pub token_count: Option\u003cusize\u003e,\n    /// Last indexed timestamp\n    pub indexed_at: Option\u003cString\u003e,\n}\n\n/// Byte range mapping for lens system\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SpecLens {\n    /// Block ID to byte range mappings\n    pub blocks: Vec\u003cBlockLens\u003e,\n}\n\n/// Single block lens entry\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BlockLens {\n    /// Block ID\n    pub id: String,\n    /// Start byte in compiled output\n    pub byte_start: usize,\n    /// End byte in compiled output\n    pub byte_end: usize,\n}\n```\n\n## Compiler Implementation\n\n```rust\n/// Skill compiler for SkillSpec  SKILL.md\npub struct SkillCompiler {\n    /// Target format\n    target: CompileTarget,\n    /// Include lens file\n    emit_lens: bool,\n}\n\n/// Compilation target\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum CompileTarget {\n    /// Claude Code format\n    Claude,\n    /// OpenAI format\n    OpenAI,\n    /// Cursor format\n    Cursor,\n    /// Generic markdown\n    GenericMd,\n}\n\nimpl SkillCompiler {\n    pub fn new(target: CompileTarget) -\u003e Self {\n        Self { target, emit_lens: true }\n    }\n    \n    /// Compile SkillSpec to Markdown\n    pub fn compile(\u0026self, skill: \u0026Skill) -\u003e CompileResult {\n        let mut output = String::new();\n        let mut lens = SpecLens { blocks: vec![] };\n        \n        // Header\n        output.push_str(\u0026self.compile_header(skill));\n        \n        // Warning about auto-generation\n        output.push_str(\"\u003c!-- This file is auto-generated from skill.spec.yaml. Do not edit directly. --\u003e\\n\\n\");\n        \n        // Sections\n        for section in \u0026skill.spec.sections {\n            let start = output.len();\n            output.push_str(\u0026self.compile_section(section));\n            let end = output.len();\n            \n            lens.blocks.push(BlockLens {\n                id: section.id.clone(),\n                byte_start: start,\n                byte_end: end,\n            });\n        }\n        \n        CompileResult {\n            markdown: output,\n            lens: if self.emit_lens { Some(lens) } else { None },\n        }\n    }\n    \n    fn compile_header(\u0026self, skill: \u0026Skill) -\u003e String {\n        let mut header = String::new();\n        \n        // Title\n        header.push_str(\u0026format!(\"# {}\\n\\n\", skill.spec.name));\n        \n        // Description\n        if !skill.spec.description.is_empty() {\n            header.push_str(\u0026format!(\"{}\\n\\n\", skill.spec.description));\n        }\n        \n        // Frontmatter based on target\n        match self.target {\n            CompileTarget::Claude =\u003e {\n                // Claude-specific frontmatter\n            }\n            CompileTarget::OpenAI =\u003e {\n                // OpenAI-specific frontmatter\n            }\n            _ =\u003e {}\n        }\n        \n        header\n    }\n    \n    fn compile_section(\u0026self, section: \u0026SkillSectionSpec) -\u003e String {\n        let mut output = String::new();\n        \n        // Section title\n        let title = section.title.clone().unwrap_or_else(|| {\n            match section.section_type {\n                SectionType::CriticalRules =\u003e \"CRITICAL RULES\".to_string(),\n                SectionType::Guidelines =\u003e \"Guidelines\".to_string(),\n                SectionType::Commands =\u003e \"Commands\".to_string(),\n                SectionType::Examples =\u003e \"Examples\".to_string(),\n                SectionType::Pitfalls =\u003e \"Pitfalls\".to_string(),\n                SectionType::Troubleshooting =\u003e \"Troubleshooting\".to_string(),\n                SectionType::Checklist =\u003e \"Checklist\".to_string(),\n                SectionType::References =\u003e \"References\".to_string(),\n                _ =\u003e \"Section\".to_string(),\n            }\n        });\n        \n        output.push_str(\u0026format!(\"## {}\\n\\n\", title));\n        \n        // Section content\n        match \u0026section.content {\n            SectionContent::Items(items) =\u003e {\n                for item in items {\n                    output.push_str(\u0026format!(\"- {}\\n\", item));\n                }\n            }\n            SectionContent::Examples(examples) =\u003e {\n                for example in examples {\n                    if let Some(title) = \u0026example.title {\n                        output.push_str(\u0026format!(\"### {}\\n\\n\", title));\n                    }\n                    if let Some(desc) = \u0026example.description {\n                        output.push_str(\u0026format!(\"{}\\n\\n\", desc));\n                    }\n                    let lang = example.language.as_deref().unwrap_or(\"text\");\n                    output.push_str(\u0026format!(\"```{}\\n{}\\n```\\n\\n\", lang, example.code));\n                }\n            }\n            SectionContent::Commands(commands) =\u003e {\n                for cmd in commands {\n                    output.push_str(\u0026format!(\"- `{}` - {}\\n\", cmd.command, cmd.description));\n                    if let Some(ex) = \u0026cmd.example {\n                        output.push_str(\u0026format!(\"  Example: `{}`\\n\", ex));\n                    }\n                }\n            }\n            SectionContent::Pitfalls(pitfalls) =\u003e {\n                for pitfall in pitfalls {\n                    output.push_str(\u0026format!(\"- **{}**\\n\", pitfall.description));\n                    if let Some(fix) = \u0026pitfall.fix {\n                        output.push_str(\u0026format!(\"  Fix: {}\\n\", fix));\n                    }\n                }\n            }\n            SectionContent::Markdown(text) =\u003e {\n                output.push_str(text);\n            }\n            _ =\u003e {\n                // Handle other content types\n            }\n        }\n        \n        output.push_str(\"\\n\");\n        output\n    }\n}\n\n/// Result of compilation\npub struct CompileResult {\n    /// The compiled markdown\n    pub markdown: String,\n    /// Optional lens mapping\n    pub lens: Option\u003cSpecLens\u003e,\n}\n```\n\n## Tasks\n\n### Task 1: Define Core Skill Types\n- [ ] Create `src/skill/mod.rs` module\n- [ ] Define `Skill` struct with all fields\n- [ ] Define `SkillMetadata` struct\n- [ ] Define `SkillSpec` struct\n- [ ] Define `SkillAssets` struct\n- [ ] Define `SkillSource` struct\n- [ ] Define `SkillComputed` struct\n\n### Task 2: Define Section Types\n- [ ] Define `SkillSectionSpec` struct\n- [ ] Define `SectionType` enum with all variants\n- [ ] Define `SectionContent` enum (untagged)\n- [ ] Define all content item types (ExampleItem, etc.)\n- [ ] Implement Default for all types\n\n### Task 3: Define Auxiliary Types\n- [ ] Define `DependencySpec` struct\n- [ ] Define `CapabilitySpec` struct\n- [ ] Define `Platform` enum\n- [ ] Define `NetworkRequirement` enum\n- [ ] Define `DeprecationInfo` struct\n- [ ] Define `Predicate` struct\n\n### Task 4: Implement Serialization\n- [ ] Derive Serialize/Deserialize for all types\n- [ ] Configure serde rename_all for enums\n- [ ] Handle optional fields with skip_serializing_if\n- [ ] Support both YAML and JSON serialization\n- [ ] Ensure round-trip stability\n\n### Task 5: Implement SkillCompiler\n- [ ] Create `src/skill/compiler.rs`\n- [ ] Implement `compile()` method\n- [ ] Implement section-specific renderers\n- [ ] Generate deterministic output\n- [ ] Support all CompileTarget variants\n\n### Task 6: Implement Lens System\n- [ ] Define `SpecLens` and `BlockLens` structs\n- [ ] Track byte ranges during compilation\n- [ ] Output lens file alongside SKILL.md\n- [ ] Support lens-based block lookup\n\n### Task 7: Multi-Target Compilation\n- [ ] Implement Claude target format\n- [ ] Implement OpenAI target format\n- [ ] Implement Cursor target format\n- [ ] Implement generic markdown format\n- [ ] Handle target-specific frontmatter\n\n### Task 8: Validation and Defaults\n- [ ] Implement validation for SkillSpec\n- [ ] Provide helpful error messages\n- [ ] Set sensible defaults\n- [ ] Validate block IDs are unique\n- [ ] Validate dependencies exist\n\n## Acceptance Criteria\n\n1. **Serialization**: SkillSpec round-trips through YAML/JSON\n2. **Deterministic**: Same spec always produces same SKILL.md\n3. **Multi-Target**: Compiles to Claude, OpenAI, Cursor formats\n4. **Lens Output**: Generates block-to-byte mappings\n5. **Validation**: Invalid specs produce clear errors\n6. **All Types**: All section types render correctly\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_skill_yaml_roundtrip() {\n        let skill = Skill {\n            id: \"test-skill\".into(),\n            metadata: SkillMetadata {\n                name: \"Test Skill\".into(),\n                description: \"A test\".into(),\n                tags: vec![\"test\".into()],\n                ..Default::default()\n            },\n            spec: SkillSpec {\n                name: \"Test Skill\".into(),\n                description: \"A test skill\".into(),\n                sections: vec![\n                    SkillSectionSpec {\n                        id: \"rules\".into(),\n                        section_type: SectionType::CriticalRules,\n                        title: None,\n                        content: SectionContent::Items(vec![\n                            \"Rule 1\".into(),\n                            \"Rule 2\".into(),\n                        ]),\n                        predicate: None,\n                    }\n                ],\n            },\n            ..Default::default()\n        };\n        \n        let yaml = serde_yaml::to_string(\u0026skill).unwrap();\n        let parsed: Skill = serde_yaml::from_str(\u0026yaml).unwrap();\n        \n        assert_eq!(skill.id, parsed.id);\n        assert_eq!(skill.spec.sections.len(), parsed.spec.sections.len());\n    }\n    \n    #[test]\n    fn test_deterministic_compilation() {\n        let skill = create_test_skill();\n        let compiler = SkillCompiler::new(CompileTarget::GenericMd);\n        \n        let result1 = compiler.compile(\u0026skill);\n        let result2 = compiler.compile(\u0026skill);\n        \n        assert_eq!(result1.markdown, result2.markdown);\n    }\n    \n    #[test]\n    fn test_lens_byte_ranges() {\n        let skill = Skill {\n            id: \"lens-test\".into(),\n            spec: SkillSpec {\n                name: \"Lens Test\".into(),\n                description: \"Testing lens\".into(),\n                sections: vec![\n                    SkillSectionSpec {\n                        id: \"section-1\".into(),\n                        section_type: SectionType::Guidelines,\n                        title: Some(\"First\".into()),\n                        content: SectionContent::Items(vec![\"Item 1\".into()]),\n                        predicate: None,\n                    },\n                    SkillSectionSpec {\n                        id: \"section-2\".into(),\n                        section_type: SectionType::Guidelines,\n                        title: Some(\"Second\".into()),\n                        content: SectionContent::Items(vec![\"Item 2\".into()]),\n                        predicate: None,\n                    },\n                ],\n            },\n            ..Default::default()\n        };\n        \n        let compiler = SkillCompiler::new(CompileTarget::GenericMd);\n        let result = compiler.compile(\u0026skill);\n        \n        let lens = result.lens.unwrap();\n        assert_eq!(lens.blocks.len(), 2);\n        \n        // Verify byte ranges are valid\n        for block in \u0026lens.blocks {\n            assert!(block.byte_start \u003c block.byte_end);\n            assert!(block.byte_end \u003c= result.markdown.len());\n        }\n        \n        // Verify ranges don't overlap\n        assert!(lens.blocks[0].byte_end \u003c= lens.blocks[1].byte_start);\n    }\n    \n    #[test]\n    fn test_section_type_rendering() {\n        // Test CriticalRules\n        let section = SkillSectionSpec {\n            id: \"rules\".into(),\n            section_type: SectionType::CriticalRules,\n            title: None,\n            content: SectionContent::Items(vec![\"Never do X\".into()]),\n            predicate: None,\n        };\n        \n        let compiler = SkillCompiler::new(CompileTarget::GenericMd);\n        let output = compiler.compile_section(\u0026section);\n        \n        assert!(output.contains(\"## CRITICAL RULES\"));\n        assert!(output.contains(\"- Never do X\"));\n    }\n    \n    #[test]\n    fn test_example_section() {\n        let section = SkillSectionSpec {\n            id: \"examples\".into(),\n            section_type: SectionType::Examples,\n            title: None,\n            content: SectionContent::Examples(vec![\n                ExampleItem {\n                    title: Some(\"Basic Usage\".into()),\n                    description: Some(\"Shows basic usage\".into()),\n                    code: \"let x = 1;\".into(),\n                    language: Some(\"rust\".into()),\n                }\n            ]),\n            predicate: None,\n        };\n        \n        let compiler = SkillCompiler::new(CompileTarget::GenericMd);\n        let output = compiler.compile_section(\u0026section);\n        \n        assert!(output.contains(\"### Basic Usage\"));\n        assert!(output.contains(\"```rust\"));\n        assert!(output.contains(\"let x = 1;\"));\n    }\n    \n    #[test]\n    fn test_pitfall_section() {\n        let section = SkillSectionSpec {\n            id: \"pitfalls\".into(),\n            section_type: SectionType::Pitfalls,\n            title: None,\n            content: SectionContent::Pitfalls(vec![\n                PitfallItem {\n                    description: \"Forgetting to handle errors\".into(),\n                    fix: Some(\"Always use Result types\".into()),\n                    bad_example: None,\n                    good_example: None,\n                }\n            ]),\n            predicate: None,\n        };\n        \n        let compiler = SkillCompiler::new(CompileTarget::GenericMd);\n        let output = compiler.compile_section(\u0026section);\n        \n        assert!(output.contains(\"**Forgetting to handle errors**\"));\n        assert!(output.contains(\"Fix: Always use Result types\"));\n    }\n    \n    #[test]\n    fn test_multi_target_compilation() {\n        let skill = create_test_skill();\n        \n        let claude_compiler = SkillCompiler::new(CompileTarget::Claude);\n        let openai_compiler = SkillCompiler::new(CompileTarget::OpenAI);\n        \n        let claude_output = claude_compiler.compile(\u0026skill);\n        let openai_output = openai_compiler.compile(\u0026skill);\n        \n        // Both should contain the core content\n        assert!(claude_output.markdown.contains(\"Test Skill\"));\n        assert!(openai_output.markdown.contains(\"Test Skill\"));\n        \n        // But may have different formatting\n        // (specific assertions depend on format differences)\n    }\n    \n    fn create_test_skill() -\u003e Skill {\n        Skill {\n            id: \"test\".into(),\n            spec: SkillSpec {\n                name: \"Test Skill\".into(),\n                description: \"Test\".into(),\n                sections: vec![],\n            },\n            ..Default::default()\n        }\n    }\n}\n```\n\n### Logging Requirements\nAll skill operations must log:\n- `DEBUG`: Serialization steps, compilation phases\n- `INFO`: Skills loaded, compiled, validated\n- `WARN`: Deprecated fields, missing optional content\n- `ERROR`: Validation failures, serialization errors\n\nExample log output:\n```\n[DEBUG] Parsing SkillSpec from /skills/rust-debugging/skill.spec.yaml\n[DEBUG] Found 5 sections in skill spec\n[INFO] Loaded skill: rust-debugging (version 1.2.0)\n[DEBUG] Compiling skill to Claude target\n[DEBUG] Compiling section: critical-rules (bytes 0-1024)\n[DEBUG] Compiling section: examples (bytes 1024-3500)\n[INFO] Compilation complete: 4.2KB, 5 sections, lens generated\n```\n\n## References\n\n- Plan Section 3.1: Skill Structure\n- Plan Section 3.6: Skill Spec and Deterministic Compilation\n- Plan Section 0.2: What Are Claude Code Skills?\n- Depends on: meta_skill-5s0 (Rust Project Scaffolding)\n- Blocks: meta_skill-14h (CLI Commands), meta_skill-225 (Layering), meta_skill-jka (Dependencies)\n\nLabels: [datamodel phase-1 skill]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:04.62909506-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:44:18.227845448-05:00","labels":["datamodel","phase-1","skill"],"dependencies":[{"issue_id":"meta_skill-ik6","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.849118748-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-jka","title":"Dependency Graph Resolution","description":"## Overview\n\nImplement the dependency graph resolution system for meta_skill (Section 3.4 of PLAN_TO_MAKE_METASKILL_CLI.md). Skills declare dependencies (requires), capabilities (provides), and environment requirements. The `ms` CLI builds a dependency graph to resolve load order, detect cycles, and auto-load prerequisites.\n\n## Background \u0026 Rationale\n\n### Why Dependency Management Matters\nSkills in meta_skill are not isolated - they build upon each other. A skill for \"rust-testing\" might require \"rust-basics\" to be loaded first. Without proper dependency resolution:\n- Users must manually determine load order (error-prone)\n- Circular dependencies cause infinite loops or crashes\n- Missing prerequisites cause confusing runtime failures\n\n### Design Goals\n1. **Declarative Dependencies**: Skills declare what they need, not how to get it\n2. **Automatic Resolution**: The system figures out load order\n3. **Cycle Detection**: Fail fast with clear error messages\n4. **Graceful Degradation**: Optional dependencies don't block loading\n\n## Key Data Structures (from Plan Section 3.4)\n\n```rust\n/// Represents the full dependency graph for all known skills\nstruct DependencyGraph {\n    /// All skill nodes in the graph\n    nodes: Vec\u003cSkillId\u003e,\n    /// Directed edges representing dependencies\n    edges: Vec\u003cDependencyEdge\u003e,\n}\n\n/// A single dependency relationship\nstruct DependencyEdge {\n    /// The skill that has the dependency\n    skill_id: SkillId,\n    /// The skill it depends on\n    depends_on: SkillId,\n}\n\n/// Result of dependency resolution\nstruct ResolvedDependencyPlan {\n    /// Skills in topologically sorted load order\n    ordered: Vec\u003cSkillId\u003e,\n    /// Dependencies that could not be resolved\n    missing: Vec\u003cSkillId\u003e,\n}\n\n/// How aggressively to auto-load dependencies\nenum DependencyLoadMode {\n    /// Only load explicitly requested skills (fail if deps missing)\n    Manual,\n    /// Auto-load direct dependencies\n    Auto,\n    /// Auto-load entire transitive closure\n    Aggressive,\n}\n\n/// Dependency specification in a skill manifest\nstruct DependencySpec {\n    /// Required skill ID (must be loaded first)\n    skill_id: SkillId,\n    /// Minimum version constraint (optional)\n    version: Option\u003cVersionReq\u003e,\n    /// If true, skill loads even if this dep is missing\n    optional: bool,\n}\n\n/// What a skill provides to others\nstruct CapabilitySpec {\n    /// Unique capability identifier\n    name: String,\n    /// Semantic version of this capability\n    version: Version,\n}\n```\n\n## Tasks\n\n### Task 1: Define Core Graph Types\n- [ ] Create `src/deps/mod.rs` module\n- [ ] Define `DependencyGraph` struct with nodes and edges\n- [ ] Define `DependencyEdge` struct\n- [ ] Define `ResolvedDependencyPlan` struct\n- [ ] Define `DependencyLoadMode` enum\n- [ ] Implement `Default` for all types\n- [ ] Add comprehensive documentation\n\n### Task 2: Implement Graph Building\n- [ ] Create `DependencyGraph::new()` constructor\n- [ ] Implement `DependencyGraph::add_skill(skill_id, deps)` method\n- [ ] Implement `DependencyGraph::add_edge(from, to)` method\n- [ ] Implement `DependencyGraph::remove_skill(skill_id)` method\n- [ ] Handle duplicate node/edge insertion gracefully\n- [ ] Add logging for all graph mutations\n\n### Task 3: Implement Topological Sort\n- [ ] Implement Kahn's algorithm for topological sorting\n- [ ] Return `ResolvedDependencyPlan` with ordered skills\n- [ ] Populate `missing` field for unresolved dependencies\n- [ ] Handle empty graphs (return empty plan)\n- [ ] Add detailed logging at each step\n\n### Task 4: Implement Cycle Detection\n- [ ] Detect cycles during topological sort\n- [ ] Return detailed error with cycle path (e.g., \"A -\u003e B -\u003e C -\u003e A\")\n- [ ] Create `CycleError` type with participating skills\n- [ ] Add helper to find shortest cycle for error messages\n- [ ] Log cycle detection with full path\n\n### Task 5: Implement Dependency Resolution\n- [ ] Create `resolve_dependencies(graph, requested, mode)` function\n- [ ] For `Manual` mode: validate all deps present, fail if missing\n- [ ] For `Auto` mode: include direct dependencies of requested skills\n- [ ] For `Aggressive` mode: include full transitive closure\n- [ ] Return `ResolvedDependencyPlan` with load order\n- [ ] Log resolution mode and decisions\n\n### Task 6: Implement Optional Dependency Handling\n- [ ] Extend `DependencyEdge` with `optional: bool` field\n- [ ] Optional deps don't cause resolution failure\n- [ ] Optional deps included if available, skipped if not\n- [ ] Log which optional deps were skipped and why\n\n### Task 7: Implement Capability Matching\n- [ ] Create `CapabilityIndex` to track which skills provide what\n- [ ] Implement `provides_capability(skill_id, capability)` check\n- [ ] Support version constraints on capability requirements\n- [ ] Allow dependencies by capability name (not just skill ID)\n- [ ] Log capability resolution decisions\n\n### Task 8: Integration with SkillSpec\n- [ ] Parse `requires` field from skill manifests\n- [ ] Parse `provides` field from skill manifests\n- [ ] Build graph from loaded SkillSpec instances\n- [ ] Validate dependency declarations at parse time\n\n## Acceptance Criteria\n\n1. **Graph Construction**: Can build dependency graph from skill manifests\n2. **Topological Sort**: Returns correct load order for valid graphs\n3. **Cycle Detection**: Detects all cycles and reports clear error messages\n4. **Missing Dependencies**: Identifies and reports missing dependencies\n5. **Load Modes**: All three DependencyLoadMode variants work correctly\n6. **Optional Dependencies**: Optional deps handled gracefully\n7. **Capability Matching**: Skills can depend on capabilities, not just IDs\n8. **Performance**: Handles 1000+ skills in \u003c100ms\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[test]\nfn test_empty_graph_resolution() {\n    let graph = DependencyGraph::new();\n    let plan = resolve_dependencies(\u0026graph, vec![], DependencyLoadMode::Auto);\n    assert!(plan.ordered.is_empty());\n    assert!(plan.missing.is_empty());\n}\n\n#[test]\nfn test_linear_dependency_chain() {\n    // A -\u003e B -\u003e C should resolve to [C, B, A]\n    let mut graph = DependencyGraph::new();\n    graph.add_skill(\"A\", vec![\"B\"]);\n    graph.add_skill(\"B\", vec![\"C\"]);\n    graph.add_skill(\"C\", vec![]);\n    \n    let plan = resolve_dependencies(\u0026graph, vec![\"A\"], DependencyLoadMode::Aggressive);\n    assert_eq!(plan.ordered, vec![\"C\", \"B\", \"A\"]);\n}\n\n#[test]\nfn test_cycle_detection() {\n    // A -\u003e B -\u003e C -\u003e A should fail with cycle error\n    let mut graph = DependencyGraph::new();\n    graph.add_skill(\"A\", vec![\"B\"]);\n    graph.add_skill(\"B\", vec![\"C\"]);\n    graph.add_skill(\"C\", vec![\"A\"]);\n    \n    let result = resolve_dependencies(\u0026graph, vec![\"A\"], DependencyLoadMode::Auto);\n    assert!(result.is_err());\n    assert!(result.unwrap_err().to_string().contains(\"cycle\"));\n}\n\n#[test]\nfn test_diamond_dependency() {\n    //     A\n    //    / \\\n    //   B   C\n    //    \\ /\n    //     D\n    let mut graph = DependencyGraph::new();\n    graph.add_skill(\"A\", vec![\"B\", \"C\"]);\n    graph.add_skill(\"B\", vec![\"D\"]);\n    graph.add_skill(\"C\", vec![\"D\"]);\n    graph.add_skill(\"D\", vec![]);\n    \n    let plan = resolve_dependencies(\u0026graph, vec![\"A\"], DependencyLoadMode::Aggressive);\n    // D must come before B and C, which must come before A\n    let d_pos = plan.ordered.iter().position(|x| x == \"D\").unwrap();\n    let b_pos = plan.ordered.iter().position(|x| x == \"B\").unwrap();\n    let c_pos = plan.ordered.iter().position(|x| x == \"C\").unwrap();\n    let a_pos = plan.ordered.iter().position(|x| x == \"A\").unwrap();\n    assert!(d_pos \u003c b_pos \u0026\u0026 d_pos \u003c c_pos);\n    assert!(b_pos \u003c a_pos \u0026\u0026 c_pos \u003c a_pos);\n}\n\n#[test]\nfn test_optional_dependency_missing() {\n    let mut graph = DependencyGraph::new();\n    graph.add_skill(\"A\", vec![]);\n    graph.add_optional_dep(\"A\", \"optional-skill\");\n    \n    let plan = resolve_dependencies(\u0026graph, vec![\"A\"], DependencyLoadMode::Auto);\n    assert!(plan.is_ok());\n    assert!(!plan.ordered.contains(\u0026\"optional-skill\"));\n}\n```\n\n### Logging Requirements\nAll operations must log with appropriate levels:\n- `DEBUG`: Graph mutations, sort steps, resolution decisions\n- `INFO`: Resolution results, load order\n- `WARN`: Missing optional dependencies, version mismatches\n- `ERROR`: Cycles detected, required dependencies missing\n\nExample log output:\n```\n[DEBUG] DependencyGraph: Adding skill 'rust-testing' with deps ['rust-basics']\n[DEBUG] DependencyGraph: Adding edge rust-testing -\u003e rust-basics\n[INFO] Resolving dependencies for ['rust-testing'] in Auto mode\n[DEBUG] Topological sort: visiting 'rust-basics' (no unmet deps)\n[DEBUG] Topological sort: visiting 'rust-testing' (deps satisfied)\n[INFO] Resolution complete: load order = ['rust-basics', 'rust-testing']\n```\n\n## References\n\n- Plan Section 3.4: Dependency Management\n- Plan Section 3.1: SkillSpec structure (requires/provides fields)\n- Related: meta_skill-ik6 (SkillSpec Data Model)\n- Blocks: meta_skill-7va (ms load Command)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:51:45.322323586-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:51:45.322323586-05:00","labels":["datamodel","dependencies","phase-1"],"dependencies":[{"issue_id":"meta_skill-jka","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:54:01.214027387-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-llm","title":"[P4] Session Quality Scoring","description":"# Session Quality Scoring\n\nFilter low-quality sessions from extraction.\n\n## Tasks\n1. Define quality metrics\n2. Implement scoring algorithm\n3. Store scores in SQLite\n4. Filter sessions below threshold\n5. Update scores over time\n\n## Quality Metrics (from Section 8.8)\n- Completion: Did session reach goal?\n- Errors: How many failed attempts?\n- Retries: Excessive retries indicate struggles\n- Duration: Too short or too long\n- Tool usage: Diverse vs repetitive\n\n## Scoring Algorithm\n```rust\nfn score_session(session: \u0026Session) -\u003e f32 {\n    let completion_score = if session.completed { 1.0 } else { 0.5 };\n    let error_penalty = (session.error_count as f32 * 0.05).min(0.3);\n    let retry_penalty = (session.retry_count as f32 * 0.02).min(0.2);\n    let duration_score = score_duration(session.duration);\n    \n    (completion_score - error_penalty - retry_penalty + duration_score) / 2.0\n}\n```\n\n## Threshold\n- Default: 0.6\n- Configurable via config\n- Sessions below threshold excluded from extraction\n\n## Acceptance Criteria\n- Sessions scored on quality\n- Low-quality sessions filtered\n- Scores update with new data","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:52.476357365-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:52.476357365-05:00","labels":["phase-4","quality","scoring"],"dependencies":[{"issue_id":"meta_skill-llm","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T22:26:13.074292574-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-mc3","title":"CM (cass-memory) Integration","description":"## Section Reference\nIntegration with existing tooling - cass-memory (cm)\n\n## Overview\n\nIntegrate CM (cass-memory) from /data/projects/cass_memory_system as a complementary system for cross-agent learning. CM provides playbook rules with confidence tracking that can enrich skill mining and suggestion.\n\n## Why CM Integration\n\n| CM Feature | ms Application |\n|------------|----------------|\n| **Cross-agent learning** | Mine patterns from all agent types, not just Claude |\n| **Confidence decay** | Apply similar decay to skill effectiveness |\n| **Anti-pattern detection** | Link to skill pitfall sections |\n| **Scientific validation** | Validate skill rules against CASS evidence |\n| **Playbook rules** | Seed skill generation with existing rules |\n\n## Integration Architecture\n\n```rust\n/// CM client for querying playbook rules\nstruct CmClient {\n    /// Path to cm binary\n    cm_path: PathBuf,\n    /// Default flags\n    default_flags: Vec\u003cString\u003e,\n}\n\nimpl CmClient {\n    /// Get relevant context for skill mining\n    async fn get_context(\u0026self, task: \u0026str) -\u003e Result\u003cCmContext\u003e {\n        // Call: cm context \"\u003ctask\u003e\" --json\n    }\n    \n    /// Get playbook rules by category\n    async fn get_rules(\u0026self, category: \u0026str) -\u003e Result\u003cVec\u003cPlaybookRule\u003e\u003e {\n        // Call: cm playbook list --category \u003ccat\u003e --json\n    }\n    \n    /// Check if a rule already exists\n    async fn rule_exists(\u0026self, rule: \u0026str) -\u003e Result\u003cbool\u003e {\n        // Search playbook for similar rules\n    }\n}\n\n/// Context returned by cm\nstruct CmContext {\n    /// Rules that may help with the task\n    relevant_bullets: Vec\u003cPlaybookRule\u003e,\n    /// Pitfalls to avoid\n    anti_patterns: Vec\u003cAntiPattern\u003e,\n    /// Past sessions that solved similar problems\n    history_snippets: Vec\u003cHistorySnippet\u003e,\n    /// Suggested CASS queries for deeper investigation\n    suggested_cass_queries: Vec\u003cString\u003e,\n}\n\nstruct PlaybookRule {\n    id: String,\n    content: String,\n    category: String,\n    confidence: f32,\n    maturity: RuleMaturity,\n    helpful_count: u32,\n    harmful_count: u32,\n}\n\nenum RuleMaturity {\n    Candidate,\n    Established,\n    Proven,\n}\n```\n\n## Skill Mining Enhancements\n\n### 1. Pre-seed with Playbook Rules\n\nBefore mining CASS sessions, query CM for relevant rules:\n\n```rust\nimpl SkillBuilder {\n    async fn build_with_cm(\u0026self, topic: \u0026str) -\u003e Result\u003cSkillSpec\u003e {\n        // Get CM context first\n        let cm_context = self.cm_client.get_context(topic).await?;\n        \n        // Use relevant rules as seed patterns\n        let seed_patterns: Vec\u003cPattern\u003e = cm_context.relevant_bullets\n            .iter()\n            .map(|rule| Pattern::from_cm_rule(rule))\n            .collect();\n        \n        // Use anti-patterns for Pitfalls section\n        let pitfalls: Vec\u003cPitfall\u003e = cm_context.anti_patterns\n            .iter()\n            .map(|ap| Pitfall::from_cm_antipattern(ap))\n            .collect();\n        \n        // Mine CASS with enhanced queries\n        let cass_patterns = self.mine_cass(topic, \u0026cm_context.suggested_cass_queries).await?;\n        \n        // Merge and deduplicate\n        self.merge_patterns(seed_patterns, cass_patterns, pitfalls)\n    }\n}\n```\n\n### 2. Validate Extracted Patterns\n\nUse CM's scientific validation approach:\n\n```rust\nimpl PatternValidator {\n    /// Validate pattern against CASS evidence (CM-style)\n    async fn validate(\u0026self, pattern: \u0026ExtractedPattern) -\u003e ValidationResult {\n        // Search CASS for sessions where this pattern applied\n        let evidence = self.cass_client.search(\u0026pattern.evidence_query()).await?;\n        \n        if evidence.len() \u003c 3 {\n            return ValidationResult::InsufficientEvidence {\n                found: evidence.len(),\n                required: 3,\n            };\n        }\n        \n        // Check outcomes\n        let positive = evidence.iter().filter(|e| e.outcome.is_success()).count();\n        let negative = evidence.iter().filter(|e| e.outcome.is_failure()).count();\n        \n        // Apply CM's 4x harmful multiplier\n        let weighted_score = positive as f32 - (negative as f32 * 4.0);\n        \n        if weighted_score \u003e 0.0 {\n            ValidationResult::Validated { confidence: weighted_score / evidence.len() as f32 }\n        } else {\n            ValidationResult::Rejected { reason: \"More harmful than helpful\" }\n        }\n    }\n}\n```\n\n### 3. Bidirectional Sync\n\nSkills can generate CM playbook rules, and CM rules can seed skills:\n\n```rust\n/// Sync skills to CM playbook\nasync fn sync_skill_to_cm(skill: \u0026SkillSpec, cm: \u0026CmClient) -\u003e Result\u003c()\u003e {\n    for rule in skill.critical_rules() {\n        if !cm.rule_exists(\u0026rule.content).await? {\n            cm.add_rule(\u0026rule.content, \u0026skill.category()).await?;\n        }\n    }\n    Ok(())\n}\n\n/// Generate skill from CM rules\nasync fn skill_from_cm_rules(category: \u0026str, cm: \u0026CmClient) -\u003e Result\u003cSkillSpec\u003e {\n    let rules = cm.get_rules(category).await?;\n    // Convert to skill format\n    SkillSpec::from_cm_rules(rules)\n}\n```\n\n## CLI Commands\n\n```bash\n# Query CM before building\nms build --topic \"react auth\" --with-cm\n\n# Sync skill to CM playbook\nms sync-to-cm \u003cskill\u003e\n\n# Generate skill from CM rules\nms from-cm --category \"debugging\" --output debugging-workflow.skill.yaml\n\n# Show CM context for skill\nms context --cm \"react authentication\"\n```\n\n## Tasks\n\n1. [ ] Implement CmClient wrapper\n2. [ ] Add --with-cm flag to ms build\n3. [ ] Implement pattern seeding from CM rules\n4. [ ] Add validation using CM's evidence gate\n5. [ ] Implement bidirectional sync (skills \u003c-\u003e playbook)\n6. [ ] Add from-cm command for skill generation\n7. [ ] Handle graceful degradation when CM unavailable\n\n## Testing Requirements\n\n- CM integration tests (context, rules, sync)\n- Pattern seeding correctness\n- Validation with 4x harmful multiplier\n- Bidirectional sync tests\n- Graceful degradation when CM unavailable\n\n## Acceptance Criteria\n\n- CM detected and integrated\n- Build command can use CM context\n- Patterns validated against evidence\n- Skills can sync to CM playbook\n- Graceful fallback when CM unavailable\n\n## References\n\n- CM repository: /data/projects/cass_memory_system\n- CM README: /data/projects/cass_memory_system/README.md\n- CM SKILL.md: /data/projects/cass_memory_system/SKILL.md","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T23:09:26.577580416-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:09:26.577580416-05:00","labels":["phase-4 memory cross-agent learning"],"dependencies":[{"issue_id":"meta_skill-mc3","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T23:09:32.054827536-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-mh8","title":"[P2] Tantivy BM25 Full-Text Search","description":"# Tantivy BM25 Full-Text Search\n\nImplement BM25 ranking using Tantivy.\n\n## Tasks\n1. Define Tantivy schema for skills\n2. Create index on skill content, metadata\n3. Implement indexing pipeline (skill  Tantivy doc)\n4. Query parser with field weighting\n5. Rank aggregation\n\n## Schema Fields (from Section 6.1)\n- id (STORED, not indexed)\n- name (TEXT, boost 3.0)\n- description (TEXT, boost 2.0)\n- body (TEXT, boost 1.0)\n- tags (TEXT, boost 1.5)\n- layer (KEYWORD)\n- tech_stack (KEYWORD)\n\n## Query Modes\n- Simple: single query string\n- Advanced: field:value syntax\n- Phrase: \"exact phrase\"\n\n## Performance Target\n- 1000 skills/second indexing\n- \u003c50ms p99 search latency\n\n## Acceptance Criteria\n- Index builds on `ms index`\n- Full-text queries return ranked results\n- Field boosting affects ranking","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:23:02.169398245-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:23:02.169398245-05:00","labels":["phase-2","search","tantivy"],"dependencies":[{"issue_id":"meta_skill-mh8","depends_on_id":"meta_skill-14h","type":"blocks","created_at":"2026-01-13T22:23:13.459954471-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-n9r","title":"[Cross-Cutting] Security Hardening","description":"# Security Hardening (CrossCutting)\n\n## Overview\n\nApply security best practices across **all** ms subsystems: input validation, secret handling, redaction, command execution, dependency auditing, and errormessage hygiene. This bead is the umbrella security checklist that ensures individual features dont regress the global threat posture.\n\n---\n\n## Core Security Areas\n\n1. **Input Validation \u0026 Canonicalization**\n   - Normalize + validate all file paths (prevent traversal, symlink escapes).\n   - Reject unexpected path roots and disallow `..` unless explicitly allowed.\n2. **Secret Management**\n   - No secrets written to disk or logs.\n   - Only load secrets from env vars or secure store.\n3. **Command Execution Guarding**\n   - All commands pass through Safety Invariant Layer (DCG).\n4. **Redaction \u0026 Privacy**\n   - PII/secret redaction before storage or display.\n   - Taint propagation for untrusted sources.\n5. **Supply Chain Security**\n   - `cargo audit` + RUSTSEC on CI.\n   - Dependabot / Renovate update policy.\n6. **Error Hygiene**\n   - No error message should leak sensitive content.\n\n---\n\n## Implementation Tasks\n\n1. **Input Validation Utilities**\n   - Add `PathPolicy` utilities: `canonicalize_with_root`, `deny_symlink_escape`.\n2. **Secret Scanning**\n   - Regex + entropy detectors; integrate with redaction pipeline.\n3. **Command Safety Integration**\n   - Ensure all command paths route through DCG guard.\n4. **Redaction Enforcement**\n   - Enforce redaction on all evidence, logs, and skill outputs.\n5. **Dependency Auditing**\n   - Add `cargo audit` (CI) + dependency check policy.\n6. **Security Gate in Doctor**\n   - `ms doctor --check=security` verifies all safety invariants.\n\n---\n\n## Testing Requirements\n\n- Unit tests for path traversal + canonicalization.\n- Unit tests for secret detection + redaction.\n- Integration tests: ensure unsafe paths are rejected in CLI.\n- E2E: run `cargo audit` and ensure CI fails on known advisory.\n- Regression tests for error messages (no secret leakage).\n\n---\n\n## Acceptance Criteria\n\n- All usersupplied paths are validated and canonicalized.\n- No secrets appear in persisted data or logs.\n- DCG gate enforced for all commands.\n- Redaction runs on all evidence and outputs.\n- CI enforces dependency scanning.\n\n---\n\n## Dependencies\n\n- `meta_skill-qox` Safety Invariant Layer (DCG)\n- `meta_skill-ans` Redaction Pipeline\n- `meta_skill-628` CI/CD Pipeline","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:29:10.040491573-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:45:03.362934676-05:00","labels":["cross-cutting","hardening","security"],"dependencies":[{"issue_id":"meta_skill-n9r","depends_on_id":"meta_skill-qox","type":"blocks","created_at":"2026-01-13T23:45:10.908403648-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-n9r","depends_on_id":"meta_skill-ans","type":"blocks","created_at":"2026-01-13T23:45:19.493750298-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-n9r","depends_on_id":"meta_skill-628","type":"blocks","created_at":"2026-01-13T23:45:28.093272495-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-nf3","title":"[P5] Backup System","description":"# Backup System\n\nAutomatic backups with retention policy.\n\n## Tasks\n1. Automatic backup before updates\n2. Backup retention (N backups, N days)\n3. Manual backup creation\n4. Restore from backup\n5. Backup location configuration\n\n## Backup Triggers\n- Before bundle update\n- Before major ms upgrade\n- Manual: `ms backup create`\n\n## Backup Format\n```\n.ms/backups/\n 2026-01-13T12:00:00/\n    config.yaml\n    skills/\n    beads.db\n 2026-01-12T12:00:00/\n     ...\n```\n\n## Retention Policy\n- Default: Keep 5 backups\n- Configurable: `backup.retention = 10`\n- Age-based: `backup.max_age_days = 30`\n\n## Restore\n- `ms backup list` - Show available\n- `ms backup restore \u003ctimestamp\u003e` - Full restore\n- `ms backup restore \u003ctimestamp\u003e --skill \u003cid\u003e` - Single skill\n\n## Acceptance Criteria\n- Automatic backups on updates\n- Retention policy enforced\n- Restore works correctly","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:27:07.224390191-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:27:07.224390191-05:00","labels":["backup","phase-5","safety"],"dependencies":[{"issue_id":"meta_skill-nf3","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.539017091-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-nht","title":"[P6] Auto-Update System","description":"# Auto-Update System\n\nSelf-updating binary via GitHub Releases.\n\n## Tasks\n1. Check for updates (background)\n2. Download new binary\n3. Verify checksum\n4. Atomic replace\n5. Post-update migrations\n\n## Update Check\n- On startup, async check GitHub releases\n- Compare version with current\n- Cache result (check once per day)\n- Notify user if update available\n\n## Update Flow\n1. `ms update` or `ms update --check`\n2. Download binary to temp file\n3. Verify SHA256 checksum\n4. Atomic rename to replace binary\n5. Run any post-update migrations\n\n## Platform Support\n- Linux x86_64\n- Linux aarch64\n- macOS x86_64\n- macOS aarch64\n- Windows x86_64\n\n## Security\n- HTTPS only\n- Checksum verification mandatory\n- Signed releases (future)\n\n## CLI\n- `ms update` - Check and update\n- `ms update --check` - Just check\n- `ms update --force` - Force reinstall\n\n## Acceptance Criteria\n- Updates work seamlessly\n- Checksums verified\n- Rollback possible on failure","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:23.234322584-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:28:23.234322584-05:00","labels":["distribution","phase-6","update"],"dependencies":[{"issue_id":"meta_skill-nht","depends_on_id":"meta_skill-08m","type":"blocks","created_at":"2026-01-13T22:28:37.006068433-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-o8o","title":"[P3] Context-Aware Suggestions","description":"# Context-Aware Suggestions\n\nSuggest skills based on cwd, files, recent commands.\n\n## Tasks\n1. Define ContextSignals struct (cwd, files, history, errors)\n2. Implement signal extraction\n3. Match signals to skill triggers\n4. Rank suggestions by relevance\n5. Deduplicate and cooldown\n\n## Context Signals (from Section 7.1)\n- cwd: Current working directory\n- files: Recently accessed files\n- commands: Recent shell history\n- errors: Error messages in output\n- project: Project type detection (package.json, Cargo.toml, etc.)\n\n## Suggestion Triggers\nEach skill can define:\n```yaml\ntriggers:\n  files: [\"*.tsx\", \"*.jsx\"]\n  commands: [\"npm run build\", \"pnpm build\"]\n  errors: [\"cannot find module\", \"import error\"]\n```\n\n## Ranking\n- Exact file match: high score\n- Command prefix match: medium score\n- Error substring match: high score\n- Recency weighted\n\n## Acceptance Criteria\n- `ms suggest` returns relevant skills\n- Suggestions based on actual context\n- No spam (cooldown works)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:16.074120543-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:24:16.074120543-05:00","labels":["context","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-o8o","depends_on_id":"meta_skill-0ki","type":"blocks","created_at":"2026-01-13T22:24:25.953462594-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-obj","title":"Brenner Method / ms mine --guided","description":"## Section Reference\nSection 28 - Brenner Method / Guided Mining Wizard\n\n## Overview\nInteractive TUI wizard guiding from \"some sessions\" to \"skill + tests\" in one flow. Based on Brenner methodology: do not summarize - extract the generative grammar.\n\n## Core Concept\n**The Brenner Principle**: Do not summarize examples. Extract the underlying generative grammar that PRODUCES those examples. A good skill specification should let you generate new valid instances, not just recognize existing ones.\n\n## The Brenner Loop\n\n```\nSession Selection\n      \nMove Extraction (what cognitive moves were made?)\n      \nHypothesis Slate (multiple competing hypotheses)\n      \nThird Alternative Guard (force beyond binary)\n      \nSkill Formalization (write the grammar)\n      \nMaterialization Test (can it generate new instances?)\n      \nIterate (refine until materialization works)\n```\n\n## Skill Tags (Operator Algebra)\n\nThe Brenner method identifies specific cognitive operators:\n\n| Tag | Description | Detection Signal |\n|-----|-------------|------------------|\n| ProblemSelection | Choosing what to work on | Task prioritization, scope decisions |\n| HypothesisSlate | Generating multiple hypotheses | \"Could be X, could be Y, could be Z\" |\n| ThirdAlternative | Breaking binary thinking | \"What if neither?\" |\n| IterativeRefinement | Progressive improvement | Multiple rounds of edits |\n| RuthlessKill | Abandoning bad paths | \"Actually, scratch that\" |\n| Quickie | Fast tactical decisions | Rapid-fire small choices |\n| MaterializationInstinct | Knowing when abstract becomes concrete | \"Let me try that\" |\n| InnerTruth | Core insight recognition | \"The real issue is...\" |\n\n## Data Structures\n\n```rust\n/// The Brenner wizard state machine\nstruct BrennerWizard {\n    /// Current state in the wizard\n    state: WizardState,\n    /// Sessions being analyzed\n    sessions: Vec\u003cSessionRef\u003e,\n    /// Extracted cognitive moves\n    moves: Vec\u003cCognitiveMove\u003e,\n    /// Current skill draft\n    skill_draft: SkillSpec,\n    /// Results of materialization tests\n    test_results: Vec\u003cTestResult\u003e,\n    /// History for undo\n    history: Vec\u003cWizardSnapshot\u003e,\n}\n\nenum WizardState {\n    /// Selecting sessions to analyze\n    SessionSelection,\n    /// Extracting cognitive moves from sessions\n    MoveExtraction,\n    /// Third alternative guard - forcing beyond binary\n    ThirdAlternativeGuard,\n    /// Formalizing into skill specification\n    SkillFormalization,\n    /// Testing if skill can materialize new instances\n    MaterializationTest,\n    /// Wizard complete\n    Complete,\n}\n\n/// A cognitive move extracted from a session\nstruct CognitiveMove {\n    /// Type of move (from operator algebra)\n    move_type: MoveType,\n    /// Where in session this occurred\n    location: SessionLocation,\n    /// The actual content/action\n    content: String,\n    /// Why this move was made (if determinable)\n    rationale: Option\u003cString\u003e,\n    /// Preconditions for this move\n    preconditions: Vec\u003cCondition\u003e,\n    /// Effects of this move\n    effects: Vec\u003cEffect\u003e,\n}\n\nenum MoveType {\n    ProblemSelection,\n    HypothesisSlate,\n    ThirdAlternative,\n    IterativeRefinement,\n    RuthlessKill,\n    Quickie,\n    MaterializationInstinct,\n    InnerTruth,\n    /// Custom move type\n    Custom(String),\n}\n\n/// A hypothesis about what pattern underlies the sessions\nstruct Hypothesis {\n    id: HypothesisId,\n    /// Statement of the hypothesis\n    statement: String,\n    /// Evidence supporting this hypothesis\n    supporting_evidence: Vec\u003cEvidence\u003e,\n    /// Evidence against this hypothesis\n    counter_evidence: Vec\u003cEvidence\u003e,\n    /// Confidence score\n    confidence: f32,\n    /// Whether this was generated as third alternative\n    is_third_alternative: bool,\n}\n\n/// Skill specification being built\nstruct SkillSpec {\n    /// Name of the skill\n    name: String,\n    /// The generative grammar\n    grammar: GenerativeGrammar,\n    /// Preconditions for applying this skill\n    preconditions: Vec\u003cPredicate\u003e,\n    /// Expected outcomes\n    outcomes: Vec\u003cOutcome\u003e,\n    /// Anti-patterns (when NOT to use)\n    anti_patterns: Vec\u003cAntiPattern\u003e,\n    /// Example applications\n    examples: Vec\u003cExample\u003e,\n}\n\n/// The generative grammar - core of Brenner method\nstruct GenerativeGrammar {\n    /// Production rules\n    rules: Vec\u003cProductionRule\u003e,\n    /// Terminal symbols\n    terminals: Vec\u003cTerminal\u003e,\n    /// Non-terminal symbols\n    non_terminals: Vec\u003cNonTerminal\u003e,\n    /// Start symbol\n    start: NonTerminal,\n}\n\nstruct ProductionRule {\n    /// Left-hand side (what gets expanded)\n    lhs: NonTerminal,\n    /// Right-hand side (expansion)\n    rhs: Vec\u003cSymbol\u003e,\n    /// Conditions for this rule to apply\n    conditions: Vec\u003cCondition\u003e,\n    /// Probability/weight of this rule\n    weight: f32,\n}\n\n/// Materialization test result\nstruct TestResult {\n    /// The generated instance\n    generated_instance: String,\n    /// Whether it is valid\n    is_valid: bool,\n    /// Validation details\n    validation: ValidationDetails,\n    /// Which rules were used\n    rules_used: Vec\u003cRuleId\u003e,\n}\n```\n\n## TUI Interface\n\n```\n\n BRENNER WIZARD - Step 2/6: Move Extraction                                  \n\n                                                                             \n Session: abc123 (debugging memory leak)                                     \n                                    \n                                                                             \n [14:23] User: \"The app is using too much memory\"                           \n          Move: ProblemSelection (scope: memory, not CPU)                   \n                                                                             \n [14:25] User: \"Could be a leak, could be cache, could be data structure\"   \n          Move: HypothesisSlate (3 hypotheses generated)                    \n                                                                             \n [14:28] User: \"Actually, let me check if it is even our code\"               \n          Move: ThirdAlternative (external dependency hypothesis)           \n                                                                             \n Extracted Moves: 7                                                          \n Coverage: 85%                                                               \n                                                                             \n [a] Add move  [e] Edit move  [d] Delete move  [n] Next  [b] Back  [?] Help \n\n```\n\n## Third Alternative Guard\n\nThe wizard enforces third alternative thinking:\n\n```rust\nstruct ThirdAlternativeGuard {\n    /// Minimum number of alternatives required\n    min_alternatives: usize, // Default: 3\n    /// Current hypotheses\n    hypotheses: Vec\u003cHypothesis\u003e,\n}\n\nimpl ThirdAlternativeGuard {\n    fn check(\u0026self) -\u003e GuardResult {\n        if self.hypotheses.len() \u003c self.min_alternatives {\n            return GuardResult::NeedMore {\n                have: self.hypotheses.len(),\n                need: self.min_alternatives,\n                prompt: \"What is a third possibility you have not considered?\".into(),\n            };\n        }\n        \n        // Check if hypotheses are truly distinct\n        if self.hypotheses_too_similar() {\n            return GuardResult::TooSimilar {\n                prompt: \"These hypotheses seem like variations of the same idea. What is a fundamentally different explanation?\".into(),\n            };\n        }\n        \n        GuardResult::Pass\n    }\n}\n```\n\n## Materialization Test\n\n```rust\n/// Test if the skill grammar can generate valid new instances\nfn materialization_test(skill: \u0026SkillSpec, count: usize) -\u003e Vec\u003cTestResult\u003e {\n    let mut results = Vec::new();\n    \n    for _ in 0..count {\n        // Generate instance from grammar\n        let instance = skill.grammar.generate();\n        \n        // Validate against known patterns\n        let validation = validate_instance(\u0026instance, \u0026skill);\n        \n        results.push(TestResult {\n            generated_instance: instance,\n            is_valid: validation.is_valid,\n            validation,\n            rules_used: skill.grammar.last_rules_used(),\n        });\n    }\n    \n    results\n}\n\n/// Validation checks for generated instances\nfn validate_instance(instance: \u0026str, skill: \u0026SkillSpec) -\u003e ValidationDetails {\n    ValidationDetails {\n        // Does it look like the examples?\n        similarity_to_examples: compute_similarity(instance, \u0026skill.examples),\n        // Does it satisfy preconditions?\n        preconditions_met: check_preconditions(instance, \u0026skill.preconditions),\n        // Does it avoid anti-patterns?\n        avoids_anti_patterns: check_anti_patterns(instance, \u0026skill.anti_patterns),\n        // Human judgment needed?\n        needs_human_review: similarity_score \u003c 0.7,\n    }\n}\n```\n\n## CLI Commands\n\n```bash\n# Start guided mining wizard\nms mine --guided\n\n# Start with specific sessions\nms mine --guided --sessions abc123,def456,ghi789\n\n# Resume wizard from saved state\nms mine --guided --resume\n\n# Run wizard in non-interactive mode (for testing)\nms mine --guided --non-interactive --config wizard-config.yaml\n\n# Export wizard state\nms wizard export --output wizard-state.json\n\n# Import and continue wizard\nms wizard import --input wizard-state.json --continue\n```\n\n## Wizard Flow Example\n\n```\n$ ms mine --guided\n\nBRENNER WIZARD\n==============\n\nStep 1: Session Selection\n-------------------------\nFound 47 sessions in the last 30 days.\n\nSuggested clusters:\n  [1] Debugging sessions (12 sessions)\n  [2] Refactoring sessions (8 sessions)  \n  [3] Feature development (15 sessions)\n  [4] Code review (7 sessions)\n  [5] Other (5 sessions)\n\nSelect cluster or enter session IDs: 1\n\nSelected 12 debugging sessions.\n\nStep 2: Move Extraction\n-----------------------\nAnalyzing sessions for cognitive moves...\n\nFound 47 moves across 12 sessions:\n  - ProblemSelection: 12\n  - HypothesisSlate: 8\n  - IterativeRefinement: 15\n  - RuthlessKill: 4\n  - MaterializationInstinct: 6\n  - Other: 2\n\nReview moves? [y/n]: y\n\n[Interactive move review TUI...]\n\nStep 3: Third Alternative Guard\n-------------------------------\nCurrent hypotheses about the underlying skill:\n\n  1. \"Systematic elimination debugging\"\n  2. \"Hypothesis-driven investigation\"\n\n Third Alternative Required\n\nYou have 2 hypotheses. The Brenner method requires at least 3.\nWhat is a fundamentally different explanation?\n\n\u003e \"Intuition-guided exploration - following hunches before systematic search\"\n\n Third alternative accepted.\n\nStep 4: Skill Formalization\n---------------------------\nBased on your moves and hypotheses, here is a draft skill grammar:\n\n  DEBUGGING_SKILL ::= SCOPE_DECISION HYPOTHESIS_GENERATION INVESTIGATION\n  SCOPE_DECISION ::= \"identify\" SYMPTOM \"in\" CONTEXT\n  HYPOTHESIS_GENERATION ::= HYPOTHESIS (\"|\" HYPOTHESIS)*\n  ...\n\nEdit grammar? [y/n]: y\n\n[Grammar editor TUI...]\n\nStep 5: Materialization Test\n----------------------------\nTesting if grammar can generate valid debugging scenarios...\n\nGenerated 5 instances:\n  [1]  Valid - \"identify slow response in API endpoint\"\n  [2]  Valid - \"identify crash in background worker\"  \n  [3]  Invalid - \"identify null in morning\" (malformed)\n  [4]  Valid - \"identify memory growth in cache layer\"\n  [5] ? Needs review - \"identify flaky in CI pipeline\"\n\n3/5 valid, 1 needs review, 1 invalid.\n\nRefine grammar to fix invalid generations? [y/n]: y\n\n[Iterate...]\n\nStep 6: Complete\n----------------\nSkill \"debugging-methodology\" created successfully.\n\n  - 47 moves extracted\n  - 3 hypotheses considered\n  - Grammar with 12 production rules\n  - 4/5 materialization test pass rate\n\nView skill: ms skill show debugging-methodology\n```\n\n## Integration Points\n\n- **Interactive Build TUI** (meta_skill-330): Wizard uses TUI infrastructure\n- **Pattern Extraction Pipeline**: Moves feed extraction\n- **Uncertainty Queue**: Low-confidence moves create uncertainties\n- **Skill Packer**: Final output is packed skill\n\n## Testing Requirements\n\n- Unit tests for each wizard state transition\n- Move extraction accuracy tests\n- Third alternative guard enforcement tests\n- Grammar generation tests\n- Materialization test validation\n- End-to-end wizard flow tests\n- TUI rendering tests","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:56:54.26584897-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:56:54.26584897-05:00","labels":["brenner","guided","mining","phase-4"],"dependencies":[{"issue_id":"meta_skill-obj","depends_on_id":"meta_skill-330","type":"blocks","created_at":"2026-01-13T22:57:38.516488682-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-on7","title":"[P6] Error Recovery \u0026 Resilience","description":"# Error Recovery \u0026 Resilience\n\nRobust error handling for autonomous operation.\n\n## Tasks\n1. Error taxonomy with retryability\n2. Retry with exponential backoff\n3. Rate limit handling\n4. Graceful degradation\n5. Checkpoint recovery\n\n## Error Taxonomy (from Section 24.2)\n- Retryable: Network timeout, rate limit\n- Non-retryable: Invalid input, not found\n- Unknown: Treat as retryable with limits\n\n## Retry Strategy\n```rust\nstruct RetryConfig {\n    max_retries: u32,           // 3\n    initial_delay: Duration,     // 1s\n    max_delay: Duration,         // 30s\n    backoff_multiplier: f32,     // 2.0\n    jitter: bool,                // true\n}\n```\n\n## Rate Limit Handling\n- Parse Retry-After headers\n- Token bucket for local rate limiting\n- Queue requests during cooldown\n\n## Graceful Degradation\n- CASS unavailable? Use cached patterns\n- LLM unavailable? Skip transformation\n- Network down? Work offline\n\n## Checkpoint Recovery\n- Resume from last checkpoint\n- Partial results preserved\n- doctor --fix for corrupt checkpoints\n\n## Acceptance Criteria\n- Retries work correctly\n- Rate limits respected\n- Degradation is graceful","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:25.711087479-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:28:25.711087479-05:00","labels":["errors","phase-6","resilience"],"dependencies":[{"issue_id":"meta_skill-on7","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:28:37.146732879-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-q3l","title":"[P6] Doctor Command","description":"# Doctor Command\n\nComprehensive health checks with auto-fix.\n\n## Tasks\n1. Define health check categories\n2. Implement individual checks\n3. Auto-fix for common issues\n4. Summary reporting\n5. Robot mode output\n\n## Check Categories (from Section 10.1)\n1. **Database**: Schema version, integrity, corruption\n2. **Indices**: Tantivy index health, rebuild if needed\n3. **Git**: Repository state, uncommitted changes\n4. **Config**: Valid YAML, required fields present\n5. **Skills**: Parse all skills, report errors\n6. **Network**: CASS availability, GitHub access\n\n## Check Results\n- PASS: All good\n- WARN: Non-critical issue\n- FAIL: Requires attention\n- FIXED: Auto-fixed\n\n## Auto-Fix Capabilities\n- Rebuild corrupted indices\n- Repair database schema\n- Fix config syntax errors\n- Re-commit orphaned changes\n\n## CLI\n- `ms doctor` - Run all checks\n- `ms doctor --fix` - Auto-fix where possible\n- `ms doctor --category db` - Single category\n- `ms doctor --robot` - JSON output\n\n## Acceptance Criteria\n- All checks implemented\n- Auto-fix works safely\n- Clear reporting","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:21.122225821-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:28:21.122225821-05:00","labels":["doctor","health","phase-6"],"dependencies":[{"issue_id":"meta_skill-q3l","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:28:36.860741281-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-q3l","depends_on_id":"meta_skill-mh8","type":"blocks","created_at":"2026-01-13T22:28:36.89503469-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-q5x","title":"Suggestion Signal Bandit","description":"# Suggestion Signal Bandit\n\n## Overview\n\nThe Suggestion Signal Bandit is a contextual multi-armed bandit that learns per-project weightings over different suggestion signals. Instead of using fixed weights for BM25, embeddings, triggers, freshness, and project match scores, the bandit adaptively learns which signals are most predictive of useful suggestions for each project.\n\n**Why a Bandit?**\n\nDifferent projects have different characteristics:\n- A greenfield project might benefit more from freshness signals (new skills)\n- A mature codebase might benefit more from BM25 (established patterns)\n- A TypeScript project might weight tech stack matching higher\n- A mono-repo might weight project path triggers higher\n\nA contextual bandit learns these preferences from user feedback without requiring explicit configuration.\n\n## Background \u0026 Rationale\n\n### Section 7.2 Reference\n\nFrom the plan Section 7.2:\n\u003e \"A contextual bandit learns per-project weighting over signals (bm25, embeddings, triggers, freshness, project match) using usage/outcome rewards.\"\n\n### Multi-Armed Bandit Basics\n\nThe multi-armed bandit problem models exploration vs exploitation:\n- **Arms**: Different signal weighting strategies\n- **Rewards**: User acceptance/rejection of suggestions\n- **Goal**: Maximize cumulative reward (useful suggestions)\n\nWe use Thompson Sampling with Beta priors for:\n- Efficient exploration of uncertain arms\n- Natural handling of binary rewards (accept/reject)\n- Convergence to optimal arm selection\n\n### Contextual Extension\n\nOur bandit is \"contextual\" because arm selection depends on:\n- Project type (detected tech stack)\n- Task context (recent commands, open files)\n- Time of day / session patterns\n\n## Core Data Structures\n\n### SignalBandit Struct\n\n```rust\nuse std::collections::HashMap;\nuse rand::distributions::Distribution;\nuse rand_distr::Beta;\nuse serde::{Deserialize, Serialize};\n\n/// Type of signal used for suggestion scoring\n#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum SignalType {\n    /// BM25 text matching score\n    Bm25,\n    /// Semantic embedding similarity\n    Embedding,\n    /// Explicit trigger pattern match\n    Trigger,\n    /// How recently the skill was updated/used\n    Freshness,\n    /// Match with detected project tech stack\n    ProjectMatch,\n    /// Match with current file types being edited\n    FileTypeMatch,\n    /// Match with recent command patterns\n    CommandPattern,\n    /// User's historical acceptance rate for this skill\n    UserHistory,\n}\n\n/// A contextual bandit for learning signal weights\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SignalBandit {\n    /// Map from signal type to its bandit arm\n    pub arms: HashMap\u003cSignalType, BanditArm\u003e,\n    \n    /// Prior distribution for new arms (Beta distribution parameters)\n    pub prior: BetaDistribution,\n    \n    /// Context-specific arm adjustments\n    pub context_modifiers: HashMap\u003cContextKey, ContextModifier\u003e,\n    \n    /// Total number of selections made\n    pub total_selections: u64,\n    \n    /// Configuration for bandit behavior\n    pub config: BanditConfig,\n}\n\n/// A single arm in the multi-armed bandit\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BanditArm {\n    /// Signal type this arm represents\n    pub signal_type: SignalType,\n    \n    /// Number of successes (user accepted suggestion)\n    pub successes: u64,\n    \n    /// Number of failures (user rejected/ignored suggestion)\n    pub failures: u64,\n    \n    /// Current estimated probability of success\n    pub estimated_prob: f64,\n    \n    /// Upper confidence bound for exploration\n    pub ucb: f64,\n    \n    /// Last time this arm was selected\n    pub last_selected: Option\u003cchrono::DateTime\u003cchrono::Utc\u003e\u003e,\n    \n    /// Decay factor for older observations\n    pub decay_factor: f64,\n}\n\n/// Beta distribution parameters for Thompson Sampling\n#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\npub struct BetaDistribution {\n    /// Alpha parameter (prior successes + 1)\n    pub alpha: f64,\n    /// Beta parameter (prior failures + 1)\n    pub beta: f64,\n}\n\nimpl Default for BetaDistribution {\n    fn default() -\u003e Self {\n        // Uniform prior: Beta(1, 1)\n        Self { alpha: 1.0, beta: 1.0 }\n    }\n}\n\nimpl BetaDistribution {\n    /// Create an optimistic prior (expects success)\n    pub fn optimistic() -\u003e Self {\n        Self { alpha: 2.0, beta: 1.0 }\n    }\n    \n    /// Create a pessimistic prior (expects failure)\n    pub fn pessimistic() -\u003e Self {\n        Self { alpha: 1.0, beta: 2.0 }\n    }\n    \n    /// Sample from the distribution\n    pub fn sample(\u0026self, rng: \u0026mut impl rand::Rng) -\u003e f64 {\n        let beta = Beta::new(self.alpha, self.beta).unwrap();\n        beta.sample(rng)\n    }\n}\n```\n\n### Context Modifiers\n\n```rust\n/// Key for context-specific modifications\n#[derive(Debug, Clone, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum ContextKey {\n    /// Tech stack (e.g., \"rust\", \"typescript\")\n    TechStack(String),\n    /// Time of day bucket (morning, afternoon, evening)\n    TimeOfDay(TimeOfDay),\n    /// Project size category\n    ProjectSize(ProjectSize),\n    /// Recent activity pattern\n    ActivityPattern(String),\n}\n\n#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum TimeOfDay {\n    Morning,    // 6am - 12pm\n    Afternoon,  // 12pm - 6pm\n    Evening,    // 6pm - 12am\n    Night,      // 12am - 6am\n}\n\n#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq, Serialize, Deserialize)]\npub enum ProjectSize {\n    Small,      // \u003c 1000 files\n    Medium,     // 1000 - 10000 files\n    Large,      // 10000 - 100000 files\n    Massive,    // \u003e 100000 files\n}\n\n/// Modifier applied to arm selection based on context\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ContextModifier {\n    /// Additive bonus to arm's estimated probability\n    pub probability_bonus: HashMap\u003cSignalType, f64\u003e,\n    \n    /// Multiplicative factor for arm's weight\n    pub weight_multiplier: HashMap\u003cSignalType, f64\u003e,\n    \n    /// Number of observations in this context\n    pub observation_count: u64,\n}\n```\n\n### Bandit Configuration\n\n```rust\n/// Configuration for bandit behavior\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BanditConfig {\n    /// Exploration factor (higher = more exploration)\n    pub exploration_factor: f64,\n    \n    /// Decay rate for older observations (0-1, 1 = no decay)\n    pub observation_decay: f64,\n    \n    /// Minimum observations before trusting an arm\n    pub min_observations: u64,\n    \n    /// Whether to use context modifiers\n    pub use_context: bool,\n    \n    /// How often to persist bandit state (in selections)\n    pub persist_frequency: u64,\n    \n    /// Path to persist state\n    pub persistence_path: Option\u003cPathBuf\u003e,\n}\n\nimpl Default for BanditConfig {\n    fn default() -\u003e Self {\n        Self {\n            exploration_factor: 0.1,\n            observation_decay: 0.99,\n            min_observations: 10,\n            use_context: true,\n            persist_frequency: 10,\n            persistence_path: None,\n        }\n    }\n}\n```\n\n## Bandit Implementation\n\n```rust\nimpl SignalBandit {\n    /// Create a new bandit with default arms\n    pub fn new() -\u003e Self {\n        let mut arms = HashMap::new();\n        \n        for signal_type in SignalType::all() {\n            arms.insert(signal_type, BanditArm::new(signal_type));\n        }\n        \n        Self {\n            arms,\n            prior: BetaDistribution::default(),\n            context_modifiers: HashMap::new(),\n            total_selections: 0,\n            config: BanditConfig::default(),\n        }\n    }\n    \n    /// Select signal weights using Thompson Sampling\n    pub fn select_weights(\u0026mut self, context: \u0026SuggestionContext) -\u003e SignalWeights {\n        let mut rng = rand::thread_rng();\n        let mut weights = HashMap::new();\n        \n        for (signal_type, arm) in \u0026self.arms {\n            // Sample from posterior Beta distribution\n            let alpha = self.prior.alpha + arm.successes as f64;\n            let beta = self.prior.beta + arm.failures as f64;\n            let sampled = BetaDistribution { alpha, beta }.sample(\u0026mut rng);\n            \n            // Apply context modifier if enabled\n            let modified = if self.config.use_context {\n                self.apply_context_modifier(sampled, *signal_type, context)\n            } else {\n                sampled\n            };\n            \n            weights.insert(*signal_type, modified);\n        }\n        \n        // Normalize weights to sum to 1\n        let total: f64 = weights.values().sum();\n        for weight in weights.values_mut() {\n            *weight /= total;\n        }\n        \n        self.total_selections += 1;\n        \n        SignalWeights { weights }\n    }\n    \n    /// Apply context-specific modifier to weight\n    fn apply_context_modifier(\n        \u0026self,\n        base_weight: f64,\n        signal_type: SignalType,\n        context: \u0026SuggestionContext,\n    ) -\u003e f64 {\n        let mut weight = base_weight;\n        \n        // Apply tech stack modifier\n        if let Some(stack) = \u0026context.tech_stack {\n            let key = ContextKey::TechStack(stack.clone());\n            if let Some(modifier) = self.context_modifiers.get(\u0026key) {\n                if let Some(bonus) = modifier.probability_bonus.get(\u0026signal_type) {\n                    weight += bonus;\n                }\n                if let Some(mult) = modifier.weight_multiplier.get(\u0026signal_type) {\n                    weight *= mult;\n                }\n            }\n        }\n        \n        // Apply time of day modifier\n        let time_key = ContextKey::TimeOfDay(context.time_of_day());\n        if let Some(modifier) = self.context_modifiers.get(\u0026time_key) {\n            if let Some(mult) = modifier.weight_multiplier.get(\u0026signal_type) {\n                weight *= mult;\n            }\n        }\n        \n        weight.clamp(0.0, 1.0)\n    }\n    \n    /// Update arm based on reward (user feedback)\n    pub fn update(\n        \u0026mut self,\n        signal_type: SignalType,\n        reward: Reward,\n        context: \u0026SuggestionContext,\n    ) {\n        let arm = self.arms.get_mut(\u0026signal_type)\n            .expect(\"Unknown signal type\");\n        \n        // Apply observation decay to existing counts\n        arm.successes = (arm.successes as f64 * self.config.observation_decay) as u64;\n        arm.failures = (arm.failures as f64 * self.config.observation_decay) as u64;\n        \n        // Update counts based on reward\n        match reward {\n            Reward::Success =\u003e arm.successes += 1,\n            Reward::Failure =\u003e arm.failures += 1,\n            Reward::Partial(p) =\u003e {\n                // Fractional reward (e.g., 0.5 for \"used but not immediately\")\n                arm.successes += (p * 100.0) as u64;\n                arm.failures += ((1.0 - p) * 100.0) as u64;\n            }\n        }\n        \n        // Update estimated probability\n        let total = arm.successes + arm.failures;\n        arm.estimated_prob = if total \u003e 0 {\n            arm.successes as f64 / total as f64\n        } else {\n            0.5\n        };\n        \n        arm.last_selected = Some(chrono::Utc::now());\n        \n        // Update context modifier\n        if self.config.use_context {\n            self.update_context_modifier(signal_type, reward, context);\n        }\n        \n        // Persist if needed\n        if self.total_selections % self.config.persist_frequency == 0 {\n            if let Some(path) = \u0026self.config.persistence_path {\n                let _ = self.save(path);\n            }\n        }\n    }\n    \n    /// Update context-specific modifier based on observation\n    fn update_context_modifier(\n        \u0026mut self,\n        signal_type: SignalType,\n        reward: Reward,\n        context: \u0026SuggestionContext,\n    ) {\n        // Update tech stack modifier\n        if let Some(stack) = \u0026context.tech_stack {\n            let key = ContextKey::TechStack(stack.clone());\n            let modifier = self.context_modifiers\n                .entry(key)\n                .or_insert_with(ContextModifier::default);\n            \n            modifier.observation_count += 1;\n            \n            // Adjust probability bonus based on reward\n            let bonus = modifier.probability_bonus\n                .entry(signal_type)\n                .or_insert(0.0);\n            \n            let reward_value = match reward {\n                Reward::Success =\u003e 0.01,\n                Reward::Failure =\u003e -0.01,\n                Reward::Partial(p) =\u003e (p - 0.5) * 0.02,\n            };\n            \n            *bonus = (*bonus + reward_value).clamp(-0.2, 0.2);\n        }\n    }\n    \n    /// Get current arm statistics for debugging/display\n    pub fn get_stats(\u0026self) -\u003e BanditStats {\n        let arm_stats: Vec\u003cArmStats\u003e = self.arms\n            .iter()\n            .map(|(signal_type, arm)| ArmStats {\n                signal_type: *signal_type,\n                successes: arm.successes,\n                failures: arm.failures,\n                estimated_prob: arm.estimated_prob,\n                total_pulls: arm.successes + arm.failures,\n            })\n            .collect();\n        \n        BanditStats {\n            total_selections: self.total_selections,\n            arm_stats,\n            context_modifier_count: self.context_modifiers.len(),\n        }\n    }\n}\n\nimpl BanditArm {\n    pub fn new(signal_type: SignalType) -\u003e Self {\n        Self {\n            signal_type,\n            successes: 0,\n            failures: 0,\n            estimated_prob: 0.5,\n            ucb: 1.0,\n            last_selected: None,\n            decay_factor: 0.99,\n        }\n    }\n}\n\n/// Reward signal from user interaction\n#[derive(Debug, Clone, Copy)]\npub enum Reward {\n    /// User accepted and used the suggestion\n    Success,\n    /// User rejected or ignored the suggestion\n    Failure,\n    /// Partial success (e.g., used later, used partially)\n    Partial(f64),\n}\n\nimpl SignalType {\n    pub fn all() -\u003e Vec\u003cSignalType\u003e {\n        vec![\n            SignalType::Bm25,\n            SignalType::Embedding,\n            SignalType::Trigger,\n            SignalType::Freshness,\n            SignalType::ProjectMatch,\n            SignalType::FileTypeMatch,\n            SignalType::CommandPattern,\n            SignalType::UserHistory,\n        ]\n    }\n}\n```\n\n### Signal Weights Output\n\n```rust\n/// Computed weights for each signal type\n#[derive(Debug, Clone)]\npub struct SignalWeights {\n    pub weights: HashMap\u003cSignalType, f64\u003e,\n}\n\nimpl SignalWeights {\n    /// Get weight for a specific signal type\n    pub fn get(\u0026self, signal_type: SignalType) -\u003e f64 {\n        *self.weights.get(\u0026signal_type).unwrap_or(\u00260.0)\n    }\n    \n    /// Compute weighted score from individual signal scores\n    pub fn compute_score(\u0026self, scores: \u0026SignalScores) -\u003e f64 {\n        let mut total = 0.0;\n        \n        for (signal_type, score) in \u0026scores.scores {\n            let weight = self.get(*signal_type);\n            total += weight * score;\n        }\n        \n        total\n    }\n    \n    /// Format weights for logging\n    pub fn format_for_log(\u0026self) -\u003e String {\n        let mut pairs: Vec\u003c_\u003e = self.weights.iter().collect();\n        pairs.sort_by(|a, b| b.1.partial_cmp(a.1).unwrap());\n        \n        pairs\n            .iter()\n            .map(|(t, w)| format!(\"{:?}={:.3}\", t, w))\n            .collect::\u003cVec\u003c_\u003e\u003e()\n            .join(\", \")\n    }\n}\n\n/// Individual signal scores for a suggestion\n#[derive(Debug, Clone)]\npub struct SignalScores {\n    pub scores: HashMap\u003cSignalType, f64\u003e,\n}\n```\n\n## Integration with Suggestion Engine\n\n```rust\n/// Suggestion engine with bandit-based weighting\npub struct BanditSuggestionEngine {\n    /// The signal bandit\n    bandit: SignalBandit,\n    \n    /// Individual signal scorers\n    scorers: HashMap\u003cSignalType, Box\u003cdyn SignalScorer\u003e\u003e,\n    \n    /// Logger\n    logger: Arc\u003cdyn SuggestionLogger\u003e,\n}\n\nimpl BanditSuggestionEngine {\n    /// Score a skill using bandit-selected weights\n    pub fn score_skill(\n        \u0026mut self,\n        skill: \u0026Skill,\n        context: \u0026SuggestionContext,\n    ) -\u003e ScoredSkill {\n        // Select weights from bandit\n        let weights = self.bandit.select_weights(context);\n        \n        self.logger.log_weights_selected(\u0026weights);\n        \n        // Compute individual signal scores\n        let mut scores = SignalScores { scores: HashMap::new() };\n        \n        for (signal_type, scorer) in \u0026self.scorers {\n            let score = scorer.score(skill, context);\n            scores.scores.insert(*signal_type, score);\n            \n            self.logger.log_signal_score(*signal_type, score);\n        }\n        \n        // Compute weighted total\n        let total_score = weights.compute_score(\u0026scores);\n        \n        self.logger.log_total_score(skill.id(), total_score);\n        \n        ScoredSkill {\n            skill: skill.clone(),\n            score: total_score,\n            signal_scores: scores,\n            weights_used: weights,\n        }\n    }\n    \n    /// Record feedback for learning\n    pub fn record_feedback(\n        \u0026mut self,\n        skill_id: \u0026str,\n        accepted: bool,\n        context: \u0026SuggestionContext,\n        signal_scores: \u0026SignalScores,\n    ) {\n        let reward = if accepted { Reward::Success } else { Reward::Failure };\n        \n        // Update bandit for each signal based on contribution\n        for (signal_type, score) in \u0026signal_scores.scores {\n            // Weight the reward by how much this signal contributed\n            if *score \u003e 0.5 {\n                // This signal was influential\n                self.bandit.update(*signal_type, reward, context);\n            }\n        }\n        \n        self.logger.log_feedback_recorded(skill_id, accepted);\n    }\n}\n\n/// Trait for individual signal scorers\npub trait SignalScorer: Send + Sync {\n    fn score(\u0026self, skill: \u0026Skill, context: \u0026SuggestionContext) -\u003e f64;\n}\n```\n\n## Persistence\n\n```rust\nimpl SignalBandit {\n    /// Save bandit state to disk\n    pub fn save(\u0026self, path: \u0026Path) -\u003e Result\u003c(), BanditError\u003e {\n        if let Some(parent) = path.parent() {\n            std::fs::create_dir_all(parent)?;\n        }\n        \n        let json = serde_json::to_string_pretty(self)?;\n        \n        // Atomic write\n        let temp_path = path.with_extension(\"tmp\");\n        std::fs::write(\u0026temp_path, \u0026json)?;\n        std::fs::rename(\u0026temp_path, path)?;\n        \n        Ok(())\n    }\n    \n    /// Load bandit state from disk\n    pub fn load(path: \u0026Path) -\u003e Result\u003cSelf, BanditError\u003e {\n        if !path.exists() {\n            return Ok(Self::new());\n        }\n        \n        let json = std::fs::read_to_string(path)?;\n        let bandit: Self = serde_json::from_str(\u0026json)?;\n        \n        Ok(bandit)\n    }\n}\n```\n\n## Tasks\n\n### Task 1: Implement Core Bandit Types\n- [ ] Create `src/suggestions/bandit/types.rs`\n- [ ] Implement SignalType enum with all signal types\n- [ ] Implement BanditArm with success/failure tracking\n- [ ] Implement BetaDistribution with sampling\n\n### Task 2: Implement SignalBandit\n- [ ] Create `src/suggestions/bandit/bandit.rs`\n- [ ] Implement Thompson Sampling selection\n- [ ] Implement observation decay\n- [ ] Implement arm update logic\n\n### Task 3: Implement Context Modifiers\n- [ ] Create `src/suggestions/bandit/context.rs`\n- [ ] Implement ContextKey types\n- [ ] Implement ContextModifier application\n- [ ] Implement context modifier learning\n\n### Task 4: Implement Signal Scorers\n- [ ] Create `src/suggestions/bandit/scorers.rs`\n- [ ] Implement BM25 scorer\n- [ ] Implement embedding scorer (with placeholder)\n- [ ] Implement trigger pattern scorer\n- [ ] Implement freshness scorer\n- [ ] Implement project match scorer\n\n### Task 5: Integrate with Suggestion Engine\n- [ ] Modify SuggestionEngine to use bandit\n- [ ] Wire up feedback collection\n- [ ] Add bandit state persistence\n- [ ] Add bandit stats to diagnostic output\n\n### Task 6: Add CLI Commands\n- [ ] Add `ms bandit stats` command\n- [ ] Add `ms bandit reset` command\n- [ ] Add `--no-bandit` flag for fixed weights\n- [ ] Add `--bandit-exploration` flag\n\n## Acceptance Criteria\n\n1. **Learning**: Bandit learns from user feedback over time\n2. **Exploration**: Initial period explores all signals fairly\n3. **Exploitation**: Converges to best signals for each context\n4. **Persistence**: State survives restarts\n5. **Context Sensitivity**: Different contexts produce different weights\n6. **Decay**: Old observations have less influence than recent ones\n7. **Diagnostics**: Stats available for debugging\n\n## Testing Requirements\n\n### Unit Tests\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_beta_sampling_in_range() {\n        let beta = BetaDistribution { alpha: 1.0, beta: 1.0 };\n        let mut rng = rand::thread_rng();\n        \n        for _ in 0..1000 {\n            let sample = beta.sample(\u0026mut rng);\n            assert!(sample \u003e= 0.0 \u0026\u0026 sample \u003c= 1.0);\n        }\n    }\n    \n    #[test]\n    fn test_arm_update_increases_prob() {\n        let mut bandit = SignalBandit::new();\n        let context = SuggestionContext::default();\n        \n        // Update with many successes\n        for _ in 0..100 {\n            bandit.update(SignalType::Bm25, Reward::Success, \u0026context);\n        }\n        \n        let arm = bandit.arms.get(\u0026SignalType::Bm25).unwrap();\n        assert!(arm.estimated_prob \u003e 0.9);\n    }\n    \n    #[test]\n    fn test_weights_sum_to_one() {\n        let mut bandit = SignalBandit::new();\n        let context = SuggestionContext::default();\n        \n        let weights = bandit.select_weights(\u0026context);\n        let sum: f64 = weights.weights.values().sum();\n        \n        assert!((sum - 1.0).abs() \u003c 0.001);\n    }\n    \n    #[test]\n    fn test_context_modifier_application() {\n        // Test that context modifiers affect weights\n    }\n    \n    #[test]\n    fn test_observation_decay() {\n        // Test that old observations decay over time\n    }\n}\n```\n\n### Integration Tests\n\n```rust\n#[tokio::test]\nasync fn test_bandit_learns_from_feedback() {\n    let mut engine = BanditSuggestionEngine::new();\n    let context = SuggestionContext::default();\n    \n    // Simulate many interactions where BM25 is good\n    for _ in 0..100 {\n        engine.record_feedback(\"skill-1\", true, \u0026context, \u0026bm25_high_scores());\n    }\n    \n    // BM25 weight should be higher now\n    let weights = engine.bandit.select_weights(\u0026context);\n    assert!(weights.get(SignalType::Bm25) \u003e 0.2);\n}\n\n#[tokio::test]\nasync fn test_bandit_persistence() {\n    let mut bandit = SignalBandit::new();\n    // Add some observations\n    // Save\n    // Load\n    // Verify state restored\n}\n```\n\n### Logging Requirements\n\n```rust\n// DEBUG level\nlog::debug!(\"Selecting weights with Thompson Sampling\");\nlog::debug!(\"Arm {} sampled: {}\", signal_type, sampled_value);\nlog::debug!(\"Context modifier applied: {:?}\", modifier);\n\n// INFO level\nlog::info!(\"Selected weights: {}\", weights.format_for_log());\nlog::info!(\"Updated arm {} with {:?}\", signal_type, reward);\n\n// WARN level\nlog::warn!(\"Bandit has few observations, weights may be unstable\");\n\n// ERROR level\nlog::error!(\"Failed to persist bandit state: {}\", error);\n```\n\n## Dependencies\n\n- **Depends on**: `meta_skill-o8o` (Context-Aware Suggestions) - Core suggestion infrastructure\n\n## Mathematical Background\n\n### Thompson Sampling\n\nFor each arm $i$ with $s_i$ successes and $f_i$ failures:\n\n1. Sample $\\theta_i \\sim \\text{Beta}(\\alpha + s_i, \\beta + f_i)$\n2. Select arm $i^* = \\arg\\max_i \\theta_i$\n\nThe Beta distribution naturally balances:\n- **Exploration**: Arms with few observations have high variance samples\n- **Exploitation**: Arms with many successes have high mean samples\n\n### Observation Decay\n\nTo adapt to changing preferences, we decay old observations:\n\n$$s_i(t+1) = \\gamma \\cdot s_i(t) + \\mathbb{1}[\\text{success}]$$\n$$f_i(t+1) = \\gamma \\cdot f_i(t) + \\mathbb{1}[\\text{failure}]$$\n\nWhere $\\gamma \\in (0, 1)$ is the decay factor (default 0.99).\n\n## References\n\n- Plan Section 7.2: Context-aware suggestions\n- Thompson Sampling: https://en.wikipedia.org/wiki/Thompson_sampling\n- Contextual Bandits: https://arxiv.org/abs/1003.0146","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:57:49.064226866-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:57:49.064226866-05:00","labels":["bandit","ml","phase-3","suggestions"],"dependencies":[{"issue_id":"meta_skill-q5x","depends_on_id":"meta_skill-o8o","type":"blocks","created_at":"2026-01-13T23:00:23.20630478-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-qox","title":"Safety Invariant Layer","description":"# Safety Invariant Layer (DCG-Backed)\n\n## Overview\n\nHardenforce the global safety invariant: **no destructive filesystem or git operation executes without explicit, verbatim approval**. This is enforced at runtime via **DCG (Destructive Command Guard)** from `/data/projects/destructive_command_guard`, not via adhoc regexes. This layer gates *all* command execution paths (CLI, skill scripts, automation) and records auditable safety events.\n\nThis bead must be fully selfcontained so future work never needs to consult the main plan or external docs.\n\n---\n\n## Why This Matters\n\n- Aligns with AGENTS.md Rule 1 and irreversible action constraints.\n- Prevents catastrophic operations from automation or malformed skills.\n- Provides consistent, explainable safety behavior across ms features (build, prune, sync, bundle, simulate).\n\n---\n\n## Scope\n\n**In scope:**\n- Any command execution initiated by ms (skill scripts, maintenance, sync, build helpers).\n- Classification into safety tiers (Safe/Caution/Danger/Critical).\n- Mandatory **verbatim approval** for destructive ops.\n- Tombstone deletes (never rm) in msmanaged directories.\n- Auditable safety events stored in SQLite.\n\n**Out of scope:**\n- OSlevel sandboxing or kernel enforcement (external responsibility).\n- Arbitrary thirdparty process supervision beyond ms command execution.\n\n---\n\n## Core Concepts \u0026 Data Model\n\n### DCG Wrapper\n\n```rust\npub struct DcgGuard {\n    pub dcg_bin: PathBuf,\n    pub packs: Vec\u003cString\u003e,\n}\n\npub struct DcgDecision {\n    pub allowed: bool,\n    pub reason: String,\n    pub remediation: Option\u003cString\u003e,\n    pub rule_id: Option\u003cString\u003e,\n    pub pack: Option\u003cString\u003e,\n    pub tier: SafetyTier,\n}\n```\n\n### Safety Event (Audit)\n\n```rust\npub struct CommandSafetyEvent {\n    pub session_id: Option\u003cString\u003e,\n    pub command: String,\n    pub dcg_version: Option\u003cString\u003e,\n    pub dcg_pack: Option\u003cString\u003e,\n    pub decision: DcgDecision,\n    pub created_at: DateTime\u003cUtc\u003e,\n}\n```\n\n### SQLite Tables\n\n- `command_safety_events` stores DCG decisions + context.\n- Use `ms doctor --check=safety` to surface failures.\n\n---\n\n## Behavioral Rules\n\n1. **Default deny** for destructive tiers unless exact approval is present.\n2. **Verbatim approval** required for `Dangerous` + `Critical` tiers.\n3. **Tombstone deletes** inside msmanaged dirs (no actual delete).\n4. **Failopen only for observation** (log + warning) when DCG is unavailable.\n5. **Policy slices** must always be included in packs; packer fails closed if omitted.\n\n---\n\n## Implementation Tasks\n\n1. **DCG Integration**\n   - Implement `DcgGuard::evaluate_command` using `dcg explain` / scan mode.\n   - Map DCG decision to `SafetyTier` and ms policies.\n2. **Command Gate**\n   - Wrap all command execution paths with a `SafetyGate` that consults DCG.\n   - Return structured `approval_required` responses in robot mode.\n3. **Tombstone Deletes**\n   - Replace deletes with tombstone markers in `.ms/tombstones/`.\n   - Add `ms prune --approve` flow for explicit cleanup.\n4. **Audit Logging**\n   - Persist `CommandSafetyEvent` in SQLite for every gated command.\n   - Expose via `ms doctor --check=safety` and `ms safety log`.\n5. **Config Wiring**\n   - `[safety] dcg_bin`, `dcg_packs`, `dcg_explain_mode`, `require_verbatim_approval`.\n6. **Policy Slice Enforcement**\n   - Mark critical policies as `Policy` slices with `MandatoryPredicate::Always`.\n   - Validate in packer before emitting any content.\n\n---\n\n## Testing Requirements\n\n### Unit Tests\n- Decision mapping (DCG output  SafetyTier  approval requirement).\n- Tombstone creation for delete operations.\n- Mandatory policy slices enforced in packer.\n\n### Integration Tests\n- Run a blocked command and assert `approval_required` in robot output.\n- Run allowed command and ensure it executes normally.\n- Verify `command_safety_events` rows are written.\n\n### E2E\n- Simulate `ms prune` without approval  blocked.\n- Provide exact approval string  allowed and logged.\n\n### Logging\n- **DEBUG**: DCG decision payload\n- **INFO**: approval required events\n- **WARN**: DCG unavailable fallback\n- **ERROR**: attempted destructive command without approval\n\n---\n\n## Acceptance Criteria\n\n- No destructive command executes without explicit approval.\n- DCG decisions logged for every executed command.\n- Tombstone deletes are used consistently in msmanaged dirs.\n- Policy slices are mandatory and cannot be packed away.\n- Robot mode returns `approval_required` with exact approve hint.\n\n---\n\n## Dependencies\n\n- `meta_skill-vqr` Robot Mode Infrastructure (for structured approval output)\n- `meta_skill-qs1` SQLite Database Layer (for audit logging)","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:55:50.573156935-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:44:30.771072081-05:00","labels":["destructive","invariants","phase-4","safety"],"dependencies":[{"issue_id":"meta_skill-qox","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:57:37.95673377-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-qox","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:44:39.595541554-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-qs1","title":"[P1] SQLite Database Layer","description":"## Overview\n\nImplement the SQLite database layer following xf patterns exactly. This provides the structured data persistence for fast queries, skill metadata, embeddings storage, and usage tracking. Combined with the Git archive layer (dual persistence), this forms the foundation for all data operations in ms.\n\n## Background \u0026 Rationale\n\n### Why SQLite\n\n1. **Zero Dependencies**: No external database server required\n2. **Battle-Tested**: Powers billions of devices, extremely reliable\n3. **WAL Mode**: Concurrent readers with single writer\n4. **FTS5**: Built-in full-text search with BM25 ranking\n5. **Local-First**: Works offline, syncs when available\n6. **Performance**: Sub-millisecond queries for most operations\n\n### Why WAL Mode\n\nWrite-Ahead Logging (WAL) mode provides:\n- **Concurrent Reads**: Multiple readers don't block each other\n- **Faster Writes**: Writes append to WAL, not main DB file\n- **Crash Safety**: WAL provides atomic commit guarantees\n- **Checkpoint Control**: Can control when WAL merges with main DB\n\n### PRAGMA Tuning Philosophy\n\nSQLite defaults are conservative. For a local CLI tool, we tune for:\n- **journal_mode=WAL**: Concurrent access\n- **synchronous=NORMAL**: Balance of safety and speed\n- **cache_size=-64000**: 64MB cache (negative = KB)\n- **mmap_size=268435456**: 256MB memory-mapped I/O\n- **temp_store=MEMORY**: Temp tables in RAM\n- **foreign_keys=ON**: Enforce referential integrity\n\n## Key Data Structures (from Plan Section 3.2)\n\n```rust\nuse rusqlite::{Connection, params};\nuse std::path::Path;\n\n/// Database connection wrapper with schema management\npub struct Database {\n    /// The underlying SQLite connection\n    conn: Connection,\n    /// Path to the database file\n    path: PathBuf,\n    /// Current schema version\n    schema_version: u32,\n}\n\nimpl Database {\n    /// Current schema version (bump on breaking changes)\n    pub const SCHEMA_VERSION: u32 = 1;\n    \n    /// Open or create database at path\n    pub fn open(path: impl AsRef\u003cPath\u003e) -\u003e Result\u003cSelf\u003e {\n        let path = path.as_ref().to_path_buf();\n        let conn = Connection::open(\u0026path)?;\n        \n        // Apply performance tuning\n        Self::apply_pragmas(\u0026conn)?;\n        \n        let mut db = Self {\n            conn,\n            path,\n            schema_version: 0,\n        };\n        \n        // Run migrations\n        db.migrate()?;\n        \n        Ok(db)\n    }\n    \n    /// Apply performance-tuned PRAGMAs\n    fn apply_pragmas(conn: \u0026Connection) -\u003e Result\u003c()\u003e {\n        conn.execute_batch(r#\"\n            PRAGMA journal_mode = WAL;\n            PRAGMA synchronous = NORMAL;\n            PRAGMA cache_size = -64000;\n            PRAGMA mmap_size = 268435456;\n            PRAGMA temp_store = MEMORY;\n            PRAGMA foreign_keys = ON;\n            PRAGMA auto_vacuum = INCREMENTAL;\n            PRAGMA page_size = 4096;\n        \"#)?;\n        Ok(())\n    }\n    \n    /// Run all pending migrations\n    fn migrate(\u0026mut self) -\u003e Result\u003c()\u003e {\n        // Create migrations table if not exists\n        self.conn.execute(\n            \"CREATE TABLE IF NOT EXISTS _ms_migrations (\n                version INTEGER PRIMARY KEY,\n                applied_at TEXT NOT NULL DEFAULT (datetime('now'))\n            )\",\n            [],\n        )?;\n        \n        // Get current version\n        let current: u32 = self.conn\n            .query_row(\n                \"SELECT COALESCE(MAX(version), 0) FROM _ms_migrations\",\n                [],\n                |row| row.get(0),\n            )?;\n        \n        self.schema_version = current;\n        \n        // Apply pending migrations\n        for (version, migration) in MIGRATIONS.iter().enumerate() {\n            let v = (version + 1) as u32;\n            if v \u003e current {\n                tracing::info!(\"Applying migration v{}\", v);\n                self.conn.execute_batch(migration)?;\n                self.conn.execute(\n                    \"INSERT INTO _ms_migrations (version) VALUES (?)\",\n                    [v],\n                )?;\n                self.schema_version = v;\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n/// Embedded migrations (compile-time)\nconst MIGRATIONS: \u0026[\u0026str] = \u0026[\n    // Migration 1: Core tables\n    r#\"\n        -- Skills registry\n        CREATE TABLE skills (\n            id TEXT PRIMARY KEY,\n            path TEXT NOT NULL UNIQUE,\n            layer TEXT NOT NULL CHECK (layer IN ('system', 'global', 'project', 'session')),\n            name TEXT NOT NULL,\n            description TEXT,\n            version TEXT,\n            content_hash TEXT NOT NULL,\n            metadata_json TEXT NOT NULL DEFAULT '{}',\n            indexed_at TEXT NOT NULL DEFAULT (datetime('now')),\n            updated_at TEXT NOT NULL DEFAULT (datetime('now'))\n        );\n        \n        CREATE INDEX idx_skills_layer ON skills(layer);\n        CREATE INDEX idx_skills_name ON skills(name);\n        CREATE INDEX idx_skills_content_hash ON skills(content_hash);\n        \n        -- Triggers for updated_at\n        CREATE TRIGGER skills_update_timestamp\n        AFTER UPDATE ON skills\n        BEGIN\n            UPDATE skills SET updated_at = datetime('now') WHERE id = NEW.id;\n        END;\n        \n        -- Skill aliases (for renames and deprecations)\n        CREATE TABLE skill_aliases (\n            alias TEXT PRIMARY KEY,\n            canonical_id TEXT NOT NULL REFERENCES skills(id) ON DELETE CASCADE,\n            alias_type TEXT NOT NULL CHECK (alias_type IN ('rename', 'deprecated', 'abbreviation')),\n            deprecated_at TEXT,\n            replaced_by TEXT,\n            created_at TEXT NOT NULL DEFAULT (datetime('now'))\n        );\n        \n        CREATE INDEX idx_skill_aliases_canonical ON skill_aliases(canonical_id);\n        \n        -- Hash embeddings for semantic search\n        CREATE TABLE skill_embeddings (\n            skill_id TEXT PRIMARY KEY REFERENCES skills(id) ON DELETE CASCADE,\n            embedding BLOB NOT NULL,\n            embedding_version INTEGER NOT NULL DEFAULT 1,\n            created_at TEXT NOT NULL DEFAULT (datetime('now'))\n        );\n        \n        -- Pre-computed slice bundles\n        CREATE TABLE skill_packs (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            skill_id TEXT NOT NULL REFERENCES skills(id) ON DELETE CASCADE,\n            pack_name TEXT NOT NULL,\n            disclosure_level TEXT NOT NULL CHECK (disclosure_level IN ('minimal', 'overview', 'standard', 'full', 'complete')),\n            token_count INTEGER NOT NULL,\n            content_hash TEXT NOT NULL,\n            pack_data BLOB NOT NULL,\n            created_at TEXT NOT NULL DEFAULT (datetime('now')),\n            UNIQUE(skill_id, disclosure_level)\n        );\n        \n        CREATE INDEX idx_skill_packs_skill ON skill_packs(skill_id);\n        \n        -- Atomic content blocks (for micro-slicing)\n        CREATE TABLE skill_slices (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            skill_id TEXT NOT NULL REFERENCES skills(id) ON DELETE CASCADE,\n            block_id TEXT NOT NULL,\n            block_type TEXT NOT NULL,\n            content TEXT NOT NULL,\n            token_count INTEGER NOT NULL,\n            utility_score REAL NOT NULL DEFAULT 1.0,\n            byte_start INTEGER NOT NULL,\n            byte_end INTEGER NOT NULL,\n            created_at TEXT NOT NULL DEFAULT (datetime('now')),\n            UNIQUE(skill_id, block_id)\n        );\n        \n        CREATE INDEX idx_skill_slices_skill ON skill_slices(skill_id);\n        CREATE INDEX idx_skill_slices_utility ON skill_slices(utility_score DESC);\n        \n        -- Evidence linking skills to CASS sessions\n        CREATE TABLE skill_evidence (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            skill_id TEXT NOT NULL REFERENCES skills(id) ON DELETE CASCADE,\n            session_path TEXT NOT NULL,\n            turn_number INTEGER NOT NULL,\n            pattern_type TEXT NOT NULL,\n            confidence REAL NOT NULL CHECK (confidence \u003e= 0 AND confidence \u003c= 1),\n            extracted_at TEXT NOT NULL DEFAULT (datetime('now'))\n        );\n        \n        CREATE INDEX idx_skill_evidence_skill ON skill_evidence(skill_id);\n        CREATE INDEX idx_skill_evidence_session ON skill_evidence(session_path);\n        \n        -- Full-text search using FTS5\n        CREATE VIRTUAL TABLE skills_fts USING fts5(\n            id,\n            name,\n            description,\n            content,\n            tags,\n            content='skills',\n            content_rowid='rowid',\n            tokenize='porter unicode61'\n        );\n        \n        -- FTS sync triggers\n        CREATE TRIGGER skills_ai AFTER INSERT ON skills BEGIN\n            INSERT INTO skills_fts(rowid, id, name, description, content, tags)\n            SELECT rowid, id, name, description, \n                   json_extract(metadata_json, '$.body'),\n                   json_extract(metadata_json, '$.tags')\n            FROM skills WHERE id = NEW.id;\n        END;\n        \n        CREATE TRIGGER skills_ad AFTER DELETE ON skills BEGIN\n            INSERT INTO skills_fts(skills_fts, rowid, id, name, description, content, tags)\n            VALUES('delete', OLD.rowid, OLD.id, OLD.name, OLD.description, \n                   json_extract(OLD.metadata_json, '$.body'),\n                   json_extract(OLD.metadata_json, '$.tags'));\n        END;\n        \n        CREATE TRIGGER skills_au AFTER UPDATE ON skills BEGIN\n            INSERT INTO skills_fts(skills_fts, rowid, id, name, description, content, tags)\n            VALUES('delete', OLD.rowid, OLD.id, OLD.name, OLD.description,\n                   json_extract(OLD.metadata_json, '$.body'),\n                   json_extract(OLD.metadata_json, '$.tags'));\n            INSERT INTO skills_fts(rowid, id, name, description, content, tags)\n            SELECT rowid, id, name, description,\n                   json_extract(metadata_json, '$.body'),\n                   json_extract(metadata_json, '$.tags')\n            FROM skills WHERE id = NEW.id;\n        END;\n    \"#,\n];\n```\n\n## Tasks\n\n### Task 1: Database Initialization\n- [ ] Create `src/db/mod.rs` module\n- [ ] Implement `Database::open()` with path handling\n- [ ] Create database directory if not exists\n- [ ] Handle database file permissions\n- [ ] Support both file and in-memory databases (for tests)\n\n### Task 2: PRAGMA Tuning\n- [ ] Enable WAL mode for concurrent access\n- [ ] Set synchronous=NORMAL for performance\n- [ ] Configure 64MB cache size\n- [ ] Enable 256MB memory-mapped I/O\n- [ ] Set temp_store to MEMORY\n- [ ] Enable foreign key constraints\n- [ ] Configure auto_vacuum=INCREMENTAL\n\n### Task 3: Migration System\n- [ ] Create _ms_migrations tracking table\n- [ ] Embed migrations at compile time\n- [ ] Run migrations on database open\n- [ ] Track schema version in Database struct\n- [ ] Support migration rollback (for development)\n- [ ] Log migration progress\n\n### Task 4: Core Tables (Migration 1)\n- [ ] Create skills table with all columns\n- [ ] Create skill_aliases table\n- [ ] Create skill_embeddings table\n- [ ] Create skill_packs table\n- [ ] Create skill_slices table\n- [ ] Create skill_evidence table\n- [ ] Add all indexes\n\n### Task 5: FTS5 Full-Text Search\n- [ ] Create skills_fts virtual table\n- [ ] Configure porter stemmer + unicode tokenizer\n- [ ] Create insert/update/delete triggers\n- [ ] Implement FTS5 query execution\n- [ ] Support phrase queries and boolean operators\n- [ ] Add field weighting (name \u003e description \u003e content)\n\n### Task 6: Query Methods\n- [ ] Implement `get_skill(id)` - single skill lookup\n- [ ] Implement `list_skills(filter)` - filtered listing\n- [ ] Implement `search_fts(query)` - full-text search\n- [ ] Implement `upsert_skill(skill)` - insert or update\n- [ ] Implement `delete_skill(id)` - removal\n- [ ] Implement `get_by_alias(alias)` - alias resolution\n\n### Task 7: Connection Pooling (Optional)\n- [ ] Evaluate need for connection pooling\n- [ ] If needed, implement simple pool with parking_lot\n- [ ] Ensure WAL mode concurrent access works\n- [ ] Add connection health checks\n\n### Task 8: Embedding Storage\n- [ ] Store embeddings as BLOB (384-dim float32 = 1536 bytes)\n- [ ] Implement `set_embedding(skill_id, embedding)`\n- [ ] Implement `get_embedding(skill_id)` \n- [ ] Implement `batch_get_embeddings(ids)` for similarity search\n- [ ] Track embedding version for recomputation\n\n## Acceptance Criteria\n\n1. **Database Creates**: Database file created on first run\n2. **Migrations Run**: All migrations applied automatically\n3. **WAL Mode**: PRAGMA journal_mode returns 'wal'\n4. **FTS5 Works**: Full-text queries return ranked results\n5. **Triggers Fire**: updated_at timestamps update correctly\n6. **Foreign Keys**: Invalid references rejected\n7. **Concurrency**: Multiple readers work simultaneously\n8. **Performance**: Simple queries complete in \u003c1ms\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::tempdir;\n    \n    #[test]\n    fn test_database_creation() {\n        let dir = tempdir().unwrap();\n        let db_path = dir.path().join(\"test.db\");\n        \n        let db = Database::open(\u0026db_path).unwrap();\n        assert!(db_path.exists());\n        assert_eq!(db.schema_version, Database::SCHEMA_VERSION);\n    }\n    \n    #[test]\n    fn test_wal_mode_enabled() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        let mode: String = db.conn.query_row(\n            \"PRAGMA journal_mode\",\n            [],\n            |row| row.get(0),\n        ).unwrap();\n        \n        assert_eq!(mode.to_lowercase(), \"wal\");\n    }\n    \n    #[test]\n    fn test_foreign_keys_enabled() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        let enabled: i32 = db.conn.query_row(\n            \"PRAGMA foreign_keys\",\n            [],\n            |row| row.get(0),\n        ).unwrap();\n        \n        assert_eq!(enabled, 1);\n    }\n    \n    #[test]\n    fn test_skill_crud() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        // Insert\n        db.conn.execute(\n            \"INSERT INTO skills (id, path, layer, name, description, content_hash, metadata_json)\n             VALUES (?, ?, ?, ?, ?, ?, ?)\",\n            params![\n                \"test-skill\",\n                \"/path/to/skill\",\n                \"global\",\n                \"Test Skill\",\n                \"A test skill\",\n                \"abc123\",\n                \"{}\"\n            ],\n        ).unwrap();\n        \n        // Read\n        let name: String = db.conn.query_row(\n            \"SELECT name FROM skills WHERE id = ?\",\n            [\"test-skill\"],\n            |row| row.get(0),\n        ).unwrap();\n        assert_eq!(name, \"Test Skill\");\n        \n        // Update\n        db.conn.execute(\n            \"UPDATE skills SET name = ? WHERE id = ?\",\n            [\"Updated Name\", \"test-skill\"],\n        ).unwrap();\n        \n        // Verify updated_at trigger fired\n        let name: String = db.conn.query_row(\n            \"SELECT name FROM skills WHERE id = ?\",\n            [\"test-skill\"],\n            |row| row.get(0),\n        ).unwrap();\n        assert_eq!(name, \"Updated Name\");\n        \n        // Delete\n        db.conn.execute(\"DELETE FROM skills WHERE id = ?\", [\"test-skill\"]).unwrap();\n        \n        let count: i32 = db.conn.query_row(\n            \"SELECT COUNT(*) FROM skills\",\n            [],\n            |row| row.get(0),\n        ).unwrap();\n        assert_eq!(count, 0);\n    }\n    \n    #[test]\n    fn test_fts5_search() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        // Insert a skill\n        db.conn.execute(\n            \"INSERT INTO skills (id, path, layer, name, description, content_hash, metadata_json)\n             VALUES (?, ?, ?, ?, ?, ?, ?)\",\n            params![\n                \"git-commit\",\n                \"/skills/git-commit\",\n                \"system\",\n                \"Git Commit Best Practices\",\n                \"How to write good commit messages\",\n                \"def456\",\n                r#\"{\"body\": \"Always write descriptive commit messages\", \"tags\": \"git,workflow\"}\"#\n            ],\n        ).unwrap();\n        \n        // Search by name\n        let results: Vec\u003cString\u003e = db.conn\n            .prepare(\"SELECT id FROM skills_fts WHERE skills_fts MATCH ? ORDER BY rank\")?\n            .query_map([\"commit\"], |row| row.get(0))?\n            .collect::\u003cResult\u003cVec\u003c_\u003e, _\u003e\u003e()?;\n        \n        assert_eq!(results.len(), 1);\n        assert_eq!(results[0], \"git-commit\");\n    }\n    \n    #[test]\n    fn test_alias_resolution() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        // Create canonical skill\n        db.conn.execute(\n            \"INSERT INTO skills (id, path, layer, name, content_hash) VALUES (?, ?, ?, ?, ?)\",\n            [\"new-name\", \"/path\", \"global\", \"Skill\", \"hash\"],\n        ).unwrap();\n        \n        // Create alias\n        db.conn.execute(\n            \"INSERT INTO skill_aliases (alias, canonical_id, alias_type) VALUES (?, ?, ?)\",\n            [\"old-name\", \"new-name\", \"rename\"],\n        ).unwrap();\n        \n        // Resolve alias\n        let canonical: String = db.conn.query_row(\n            \"SELECT canonical_id FROM skill_aliases WHERE alias = ?\",\n            [\"old-name\"],\n            |row| row.get(0),\n        ).unwrap();\n        \n        assert_eq!(canonical, \"new-name\");\n    }\n    \n    #[test]\n    fn test_foreign_key_constraint() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        // Try to insert alias for nonexistent skill\n        let result = db.conn.execute(\n            \"INSERT INTO skill_aliases (alias, canonical_id, alias_type) VALUES (?, ?, ?)\",\n            [\"alias\", \"nonexistent\", \"rename\"],\n        );\n        \n        assert!(result.is_err());\n    }\n    \n    #[test]\n    fn test_embedding_storage() {\n        let dir = tempdir().unwrap();\n        let db = Database::open(dir.path().join(\"test.db\")).unwrap();\n        \n        // Create skill first\n        db.conn.execute(\n            \"INSERT INTO skills (id, path, layer, name, content_hash) VALUES (?, ?, ?, ?, ?)\",\n            [\"emb-skill\", \"/path\", \"global\", \"Skill\", \"hash\"],\n        ).unwrap();\n        \n        // Create 384-dim embedding\n        let embedding: Vec\u003cf32\u003e = (0..384).map(|i| i as f32 / 384.0).collect();\n        let embedding_bytes: Vec\u003cu8\u003e = embedding.iter()\n            .flat_map(|f| f.to_le_bytes())\n            .collect();\n        \n        // Store embedding\n        db.conn.execute(\n            \"INSERT INTO skill_embeddings (skill_id, embedding) VALUES (?, ?)\",\n            params![\"emb-skill\", embedding_bytes],\n        ).unwrap();\n        \n        // Retrieve embedding\n        let stored: Vec\u003cu8\u003e = db.conn.query_row(\n            \"SELECT embedding FROM skill_embeddings WHERE skill_id = ?\",\n            [\"emb-skill\"],\n            |row| row.get(0),\n        ).unwrap();\n        \n        assert_eq!(stored.len(), 384 * 4); // 384 floats * 4 bytes each\n    }\n}\n```\n\n### Logging Requirements\nAll database operations must log:\n- `DEBUG`: SQL queries, parameter values, execution times\n- `INFO`: Database opened, migrations applied, schema version\n- `WARN`: Slow queries (\u003e100ms), constraint violations\n- `ERROR`: Database corruption, migration failures\n\nExample log output:\n```\n[INFO] Opening database at /home/user/.local/share/ms/ms.db\n[DEBUG] Applying PRAGMA settings\n[DEBUG] PRAGMA journal_mode = WAL (was: delete)\n[INFO] Database schema version: 1\n[DEBUG] Executing query: SELECT * FROM skills WHERE layer = ? [params: \"global\"]\n[DEBUG] Query completed in 0.3ms, returned 15 rows\n```\n\n## References\n\n- Plan Section 3.2: SQLite Schema\n- Plan Section 3.7: Two-Phase Commit for Dual Persistence\n- xf database implementation: /data/projects/xf/src/db/mod.rs\n- Depends on: meta_skill-5s0 (Rust Project Scaffolding)\n- Blocks: meta_skill-14h (CLI Commands), meta_skill-ch6 (Hash Embeddings), meta_skill-fus (2PC)\n\nLabels: [database phase-1 sqlite]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:21:59.808662035-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:39:19.750869027-05:00","labels":["database","phase-1","sqlite"],"dependencies":[{"issue_id":"meta_skill-qs1","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.795926271-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-r6k","title":"[P2] Skill Alias System","description":"# Skill Alias System\n\nHandle skill renames and deprecations transparently.\n\n## Tasks\n1. Define skill_aliases table\n2. Resolve aliases in search\n3. Resolve aliases in load/show commands\n4. Emit deprecation warnings\n5. Migration support for old skill names\n\n## Database Schema (from Section 3.2)\n```sql\nCREATE TABLE skill_aliases (\n    alias TEXT PRIMARY KEY,\n    canonical_id TEXT NOT NULL REFERENCES skills(id),\n    deprecated_at TIMESTAMP,\n    reason TEXT\n);\n```\n\n## Behavior\n- Search for \"old-name\" returns skill with canonical ID\n- Load with old name emits warning: \"Skill 'old-name' renamed to 'new-name'\"\n- Robot mode includes alias_resolution field\n\n## Use Cases\n- Skill renames (git-commit-guide  git/commit)\n- Typo tolerance (postgres  postgresql)\n- Deprecated skills point to replacements\n\n## Acceptance Criteria\n- Aliases resolve transparently\n- Deprecation warnings shown\n- Robot mode includes resolution info","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:23:04.728286559-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:23:04.728286559-05:00","labels":["aliases","phase-2","search"],"dependencies":[{"issue_id":"meta_skill-r6k","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:23:13.569822748-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-red","title":"[Cross-Cutting] Performance Optimization","description":"# Performance Optimization\n\nPerformance targets and optimization patterns (Section 30, 31).\n\n## Performance Targets (from Section 15)\n| Metric | Target |\n|--------|--------|\n| Indexing speed | 1000 skills/second |\n| Search latency | \u003c50ms p99 |\n| Memory usage | \u003c100MB idle |\n| Binary size | \u003c20MB stripped |\n| Build session start | \u003c2 seconds |\n\n## Optimization Patterns\n1. **SIMD**: Vectorized embedding comparisons\n2. **Parallel Processing**: Rayon for batch operations\n3. **Memory-Mapped Files**: Large session processing\n4. **LRU Cache**: Parsed sessions, rendered templates\n5. **String Interning**: Deduplicate skill names/tags\n\n## Profiling Workflow (from Section 30.6)\n1. cargo build --profile profiling\n2. perf record / flamegraph\n3. Identify hot paths\n4. Apply targeted optimizations\n5. Verify with criterion benchmarks\n\n## Key Optimizations\n- Hash embeddings (zero ML dependency)\n- SoA layout for SIMD-friendly vectors\n- Pre-computed slice tokens\n\n## Acceptance Criteria\n- Meets all performance targets\n- Benchmarks in CI\n- No regression between releases","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:29:08.882398138-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:29:08.882398138-05:00","labels":["cross-cutting","optimization","performance"]}
{"id":"meta_skill-rvd","title":"[P6] Skill Effectiveness Tracking","description":"# Skill Effectiveness Tracking\n\nTrack whether skills actually help agents.\n\n## Tasks\n1. Record skill loads with context\n2. Track session outcomes\n3. Infer effectiveness from outcomes\n4. Update quality scores\n5. A/B experiment framework\n\n## Tracking Events (from Section 22)\n- skill_loaded: When skill loaded into context\n- session_completed: Session reached goal\n- session_failed: Session failed\n- explicit_feedback: User thumbs up/down\n\n## Inference Logic\n- Skill loaded + session success = positive signal\n- Skill loaded + session failure = negative signal\n- Weight by recency\n\n## A/B Experiments (from Section 22.4.1)\n- Create variants of same skill\n- Randomly assign to sessions\n- Compare outcomes\n- Promote winning variant\n\n## Storage\n```sql\nCREATE TABLE skill_usage (\n    id TEXT PRIMARY KEY,\n    skill_id TEXT,\n    session_id TEXT,\n    outcome TEXT,  -- success, failure, unknown\n    loaded_at TIMESTAMP,\n    feedback TEXT  -- positive, negative, null\n);\n```\n\n## Acceptance Criteria\n- Usage tracked automatically\n- Effectiveness scores computed\n- A/B framework functional","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:28:24.171197012-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:42:21.925060629-05:00","closed_at":"2026-01-13T23:42:21.925060629-05:00","close_reason":"Duplicate of meta_skill-iim (Skill Effectiveness Feedback Loop)","labels":["analytics","effectiveness","phase-6"],"dependencies":[{"issue_id":"meta_skill-rvd","depends_on_id":"meta_skill-7va","type":"blocks","created_at":"2026-01-13T22:28:37.033158307-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-rvd","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T22:28:37.060620812-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-sqh","title":"[P3] Disclosure Levels System","description":"# Disclosure Levels System\n\nProgressive disclosure from minimal (~100 tokens) to full.\n\n## Tasks\n1. Define DisclosureLevel enum (minimal, overview, operational, reference, full)\n2. Implement level selection in ms load command\n3. Implement level-based content filtering\n4. Token counting for each level\n5. Level recommendation based on context\n\n## Disclosure Levels (from Section 5.1)\n| Level | Tokens | Use Case |\n|-------|--------|----------|\n| minimal | ~100 | Trigger words, quick reminder |\n| overview | ~300 | High-level understanding |\n| operational | ~800 | Enough to execute |\n| reference | ~1500 | Detailed with examples |\n| full | unlimited | Complete with edge cases |\n\n## Level Selection Algorithm\n- Start with minimal\n- If query suggests execution, escalate to operational\n- If troubleshooting, escalate to reference\n- Agent can request higher levels\n\n## Acceptance Criteria\n- `ms load skill --level overview` works\n- Token counts approximately match targets\n- Level escalation works","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:24:12.151427753-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:24:12.151427753-05:00","labels":["disclosure","phase-3","ux"],"dependencies":[{"issue_id":"meta_skill-sqh","depends_on_id":"meta_skill-ik6","type":"blocks","created_at":"2026-01-13T22:24:25.816771637-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-swe","title":"[P5] Local Modification Safety","description":"# Local Modification Safety\n\nPreserve user customizations on bundle updates.\n\n## Tasks\n1. Three-tier storage: upstream, local, merged\n2. Detect local modifications\n3. Generate patches for local changes\n4. Apply patches after update\n5. Conflict detection and resolution UI\n\n## Storage Tiers (from Section 9.3)\n```\n.ms/skills/\n upstream/     # Original from bundle\n local/        # User modifications\n merged/       # Effective content\n```\n\n## Modification Tracking\n- Store original hash from bundle\n- Compare current to original\n- Generate unified diff for changes\n\n## Update Flow\n1. Download new upstream version\n2. Check for local modifications\n3. If no mods: replace directly\n4. If mods: three-way merge\n5. Conflict? Show diff and ask user\n\n## Conflict Resolution\n- Show side-by-side diff\n- Options: keep local, take upstream, merge\n- Save resolution for future\n\n## Acceptance Criteria\n- Local changes survive updates\n- Conflicts detected and surfaced\n- Resolution is clear and safe","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:27:05.252577245-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:27:05.252577245-05:00","labels":["bundles","phase-5","safety"],"dependencies":[{"issue_id":"meta_skill-swe","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T22:27:15.401857842-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-swe","depends_on_id":"meta_skill-b98","type":"blocks","created_at":"2026-01-13T22:27:15.428882372-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-tdj","title":"[P3] Meta-Skills (Composed Bundles)","description":"# Meta-Skills (Composed Bundles)\n\nSkills that compose slices from multiple skills.\n\n## Tasks\n1. Define MetaSkill spec format\n2. Cross-skill slice references\n3. Deduplication of overlapping content\n4. Coherent ordering of composed slices\n5. CLI for meta-skill creation\n\n## Meta-Skill Spec (from Section 5.5)\n```yaml\nname: react-debugging\ntype: meta-skill\ncompose:\n  - skill: react/hooks\n    slices: [pitfall-1, pitfall-2]\n  - skill: typescript/strict\n    slices: [rule-1, rule-2]\n  - skill: chrome-devtools\n    slices: [command-inspect]\n```\n\n## Use Cases\n- Combine debugging skills for specific stack\n- Create role-specific skill bundles\n- Curate onboarding sets\n\n## Coherence\n- Slices ordered by type (rules  commands  examples)\n- Duplicates removed (same content from multiple sources)\n- Transitions smoothed\n\n## Acceptance Criteria\n- Meta-skills compose correctly\n- Duplicates deduplicated\n- Order is logical","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:24:18.563378802-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:41:43.399656607-05:00","closed_at":"2026-01-13T23:41:43.399656607-05:00","close_reason":"Duplicate of meta_skill-7ws (Meta-Skills)","labels":["composition","meta-skill","phase-3"],"dependencies":[{"issue_id":"meta_skill-tdj","depends_on_id":"meta_skill-0an","type":"blocks","created_at":"2026-01-13T22:24:26.061331463-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-tun","title":"Anti-Pattern Mining","description":"## Section Reference\nSection 5.14 - Anti-Pattern Extraction and Presentation\n\n## Overview\nExtract anti-patterns from failure signals, marked anti-pattern sessions, and explicit \"wrong\" fixes. Present these as \"Avoid / When NOT to use\" sections in generated skills.\n\n## Core Concept\n**Counterexamples are first-class patterns** with the same pipeline as positive patterns:\n- Extraction  Clustering  Synthesis  Packing\n\nEach anti-pattern MUST link to the positive rule it constrains. Anti-patterns without positive counterparts are orphaned and flagged for review.\n\n## Data Structures\n\n```rust\n/// A negative pattern extracted from failure evidence\nstruct AntiPattern {\n    id: AntiPatternId,\n    /// The positive pattern this anti-pattern constrains\n    constrains: PatternId,\n    /// Extracted from failure/rollback evidence\n    evidence: Vec\u003cAntiPatternEvidence\u003e,\n    /// Synthesized \"do not\" rule\n    rule: NegativeRule,\n    /// When this anti-pattern applies\n    trigger_conditions: Vec\u003cCondition\u003e,\n    /// What goes wrong when violated\n    failure_modes: Vec\u003cFailureMode\u003e,\n    /// Confidence based on evidence strength\n    confidence: f32,\n}\n\nstruct AntiPatternEvidence {\n    source: AntiPatternSource,\n    session_id: SessionId,\n    /// The specific failure or correction\n    incident: FailureIncident,\n    /// User-provided context if marked explicitly\n    user_annotation: Option\u003cString\u003e,\n}\n\nenum AntiPatternSource {\n    /// Session explicitly marked as anti-pattern example\n    MarkedAntiPattern { marker: String },\n    /// Detected from rollback or undo sequence\n    RollbackDetected { rollback_type: RollbackType },\n    /// Explicit \"wrong\" fix with correction\n    WrongFix { original: String, correction: String },\n    /// Failure signal in session\n    FailureSignal { signal_type: FailureSignalType },\n    /// Counter-example surfaced during uncertainty resolution\n    CounterExample { uncertainty_id: UncertaintyId },\n}\n\nenum RollbackType {\n    GitReset,\n    GitRevert,\n    FileRestore,\n    ManualUndo,\n    ExplicitCorrection,\n}\n\nenum FailureSignalType {\n    TestFailure,\n    BuildError,\n    RuntimeException,\n    UserRejection,\n    ExplicitNo,\n    Frustration,\n}\n\nstruct NegativeRule {\n    /// \"NEVER do X when Y\"\n    statement: String,\n    /// Formal predicate for rule matching\n    predicate: Predicate,\n    /// Severity if violated\n    severity: AntiPatternSeverity,\n}\n\nenum AntiPatternSeverity {\n    /// Suggestion to avoid\n    Advisory,\n    /// Strong recommendation against\n    Warning,\n    /// Must not do - blocks action\n    Blocking,\n}\n\nstruct FailureMode {\n    description: String,\n    observed_count: u32,\n    example_session: Option\u003cSessionId\u003e,\n}\n```\n\n## Extraction Pipeline\n\n### Phase 1: Signal Detection\n```rust\ntrait AntiPatternDetector {\n    /// Scan session for anti-pattern signals\n    fn detect_signals(\u0026self, session: \u0026Session) -\u003e Vec\u003cAntiPatternSignal\u003e;\n    \n    /// Check for explicit anti-pattern markers\n    fn check_markers(\u0026self, session: \u0026Session) -\u003e Option\u003cMarkedAntiPattern\u003e;\n    \n    /// Detect rollback/undo sequences\n    fn detect_rollbacks(\u0026self, session: \u0026Session) -\u003e Vec\u003cRollbackSequence\u003e;\n    \n    /// Find explicit corrections (\"No, do X instead\")\n    fn find_corrections(\u0026self, session: \u0026Session) -\u003e Vec\u003cCorrection\u003e;\n}\n\nstruct AntiPatternSignal {\n    signal_type: FailureSignalType,\n    location: MessageIndex,\n    context: ContextWindow,\n    /// What action preceded the failure\n    preceding_action: Option\u003cActionSummary\u003e,\n}\n```\n\n### Phase 2: Context Extraction\n```rust\n/// Extract the \"what went wrong\" context\nstruct AntiPatternContext {\n    /// The action that failed\n    failed_action: ActionDescription,\n    /// Why it failed (if determinable)\n    failure_reason: Option\u003cString\u003e,\n    /// What conditions made it wrong\n    conditions: Vec\u003cCondition\u003e,\n    /// The correction applied (if any)\n    correction: Option\u003cActionDescription\u003e,\n}\n\nfn extract_anti_pattern_context(\n    signal: \u0026AntiPatternSignal,\n    session: \u0026Session,\n) -\u003e Result\u003cAntiPatternContext, ExtractionError\u003e;\n```\n\n### Phase 3: Clustering\n```rust\n/// Cluster similar anti-patterns across sessions\nfn cluster_anti_patterns(\n    patterns: Vec\u003cAntiPatternContext\u003e,\n    similarity_threshold: f32,\n) -\u003e Vec\u003cAntiPatternCluster\u003e;\n\nstruct AntiPatternCluster {\n    id: ClusterId,\n    /// Representative pattern for this cluster\n    centroid: AntiPatternContext,\n    /// All patterns in cluster\n    members: Vec\u003cAntiPatternContext\u003e,\n    /// Derived conditions when this anti-pattern applies\n    synthesized_conditions: Vec\u003cCondition\u003e,\n}\n```\n\n### Phase 4: Synthesis\n```rust\n/// Synthesize cluster into formal anti-pattern\nfn synthesize_anti_pattern(\n    cluster: \u0026AntiPatternCluster,\n    positive_patterns: \u0026[Pattern],\n) -\u003e Result\u003cAntiPattern, SynthesisError\u003e;\n\n/// Link anti-pattern to the positive rule it constrains\nfn find_constrained_pattern(\n    anti: \u0026AntiPatternContext,\n    patterns: \u0026[Pattern],\n) -\u003e Option\u003cPatternId\u003e;\n```\n\n### Phase 5: Packing\n```rust\n/// Pack anti-patterns into skill output\nstruct AntiPatternSection {\n    header: String, // \"## Avoid / When NOT to use\"\n    patterns: Vec\u003cFormattedAntiPattern\u003e,\n}\n\nstruct FormattedAntiPattern {\n    /// \"NEVER X when Y\"\n    rule: String,\n    /// Why this is wrong\n    rationale: String,\n    /// What to do instead (link to positive pattern)\n    instead: String,\n    /// Example from evidence\n    example: Option\u003cString\u003e,\n}\n```\n\n## CLI Commands\n\n```bash\n# Extract anti-patterns from sessions\nms mine --anti-patterns\n\n# Mine specific failure sessions\nms mine --failures-only\n\n# Show anti-patterns for a skill\nms skill show \u003cskill-id\u003e --anti-patterns\n\n# List orphaned anti-patterns (no positive counterpart)\nms anti-patterns --orphaned\n\n# Link anti-pattern to positive rule manually\nms anti-patterns link \u003canti-id\u003e \u003cpattern-id\u003e\n\n# Mark session as anti-pattern example\nms session mark \u003csession-id\u003e --anti-pattern --note \"Shows wrong approach to X\"\n```\n\n## Output Format\n\nIn generated skills, anti-patterns appear as:\n\n```markdown\n## Avoid / When NOT to use\n\n### NEVER force-push to shared branches without coordination\n**Severity**: Blocking\n**Conditions**: Branch has upstream, other contributors active\n**Failure mode**: Overwrites others work, causes merge conflicts\n**Instead**: Use git push --force-with-lease or coordinate first\n**Evidence**: 3 sessions, 2 explicit corrections\n\n### AVOID using recursive delete in scripts without path validation\n**Severity**: Warning  \n**Conditions**: Path comes from variable, script runs unattended\n**Failure mode**: Variable expansion to root or home, catastrophic deletion\n**Instead**: Validate path exists and is expected location first\n**Evidence**: 1 rollback, 1 explicit wrong fix\n```\n\n## Integration Points\n\n- **Pattern Extraction Pipeline** (meta_skill-237): Anti-patterns use same extraction infrastructure\n- **Uncertainty Queue**: Counter-examples feed anti-pattern mining\n- **Skill Packer**: Include anti-pattern section in output\n- **Linter**: Check for anti-pattern violations in new patterns\n\n## Validation\n\n- Every anti-pattern MUST link to a positive pattern (or be flagged as orphaned)\n- Anti-patterns MUST have at least 2 evidence sources for Warning severity\n- Blocking severity requires explicit user confirmation\n- Evidence must be traceable to source sessions\n\n## Testing Requirements\n\n- Unit tests for each detector type (rollback, correction, marker)\n- Integration test: full pipeline from session to packed anti-pattern\n- Test orphaned anti-pattern detection\n- Test severity escalation based on evidence count","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:52:28.403401172-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:52:28.403401172-05:00","labels":["antipatterns","mining","phase-4"],"dependencies":[{"issue_id":"meta_skill-tun","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T22:57:35.568728177-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-tzu","title":"Agent Mail Integration","description":"# Agent Mail Integration\n\n**Phase 6 - Section 20**\n\nIntegrate with Agent Mail MCP server for multi-agent skill coordination. This enables agents to share patterns, coordinate skill generation, and request skills from other agents working on related projects.\n\n---\n\n## Overview\n\nWhen multiple agents work on related projects or within the same organization, they can benefit from coordination:\n\n1. **Pattern Sharing**: Share discovered patterns with other agents\n2. **Skill Requests**: Request skills on topics you need but don't have\n3. **Generation Coordination**: Avoid duplicate work when building skills\n4. **Knowledge Distribution**: Broadcast useful skills to interested agents\n\nAgent Mail provides a message-passing infrastructure between agents. This integration makes meta_skill a first-class participant in multi-agent workflows.\n\n---\n\n## Core Data Structures\n\n### Agent Mail Client\n\n```rust\nuse std::collections::HashMap;\n\n/// Client for Agent Mail MCP server communication\npub struct AgentMailClient {\n    /// Project identifier (for message routing)\n    pub project_key: String,\n    \n    /// This agent's name/identifier\n    pub agent_name: String,\n    \n    /// MCP server endpoint\n    pub mcp_endpoint: String,\n    \n    /// Connection state\n    state: ConnectionState,\n    \n    /// Message handlers\n    handlers: HashMap\u003cMessageType, Box\u003cdyn MessageHandler\u003e\u003e,\n    \n    /// Subscriptions\n    subscriptions: Vec\u003cSubscription\u003e,\n}\n\n#[derive(Debug, Clone)]\npub enum ConnectionState {\n    Disconnected,\n    Connecting,\n    Connected { session_id: String },\n    Reconnecting { attempts: u32 },\n    Failed { error: String },\n}\n\nimpl AgentMailClient {\n    /// Create a new Agent Mail client\n    pub fn new(project_key: \u0026str, agent_name: \u0026str, endpoint: \u0026str) -\u003e Self {\n        Self {\n            project_key: project_key.to_string(),\n            agent_name: agent_name.to_string(),\n            mcp_endpoint: endpoint.to_string(),\n            state: ConnectionState::Disconnected,\n            handlers: HashMap::new(),\n            subscriptions: Vec::new(),\n        }\n    }\n    \n    /// Connect to Agent Mail server\n    pub async fn connect(\u0026mut self) -\u003e Result\u003c(), AgentMailError\u003e {\n        self.state = ConnectionState::Connecting;\n        \n        // Initialize MCP connection\n        let mcp_client = McpClient::connect(\u0026self.mcp_endpoint).await?;\n        \n        // Register as agent\n        let session_id = mcp_client.call(\n            \"agent_mail/register\",\n            json!({\n                \"project_key\": self.project_key,\n                \"agent_name\": self.agent_name,\n                \"capabilities\": [\"skill_sharing\", \"pattern_discovery\", \"skill_requests\"]\n            })\n        ).await?;\n        \n        self.state = ConnectionState::Connected { session_id };\n        \n        // Subscribe to skill-related topics\n        self.subscribe_to_defaults().await?;\n        \n        Ok(())\n    }\n    \n    /// Subscribe to default skill topics\n    async fn subscribe_to_defaults(\u0026mut self) -\u003e Result\u003c(), AgentMailError\u003e {\n        let default_topics = vec![\n            \"skills/new\",\n            \"skills/requests\",\n            \"patterns/discovered\",\n            format!(\"projects/{}/skills\", self.project_key),\n        ];\n        \n        for topic in default_topics {\n            self.subscribe(\u0026topic).await?;\n        }\n        \n        Ok(())\n    }\n    \n    /// Subscribe to a topic\n    pub async fn subscribe(\u0026mut self, topic: \u0026str) -\u003e Result\u003c(), AgentMailError\u003e {\n        if !matches!(self.state, ConnectionState::Connected { .. }) {\n            return Err(AgentMailError::NotConnected);\n        }\n        \n        let subscription = Subscription {\n            topic: topic.to_string(),\n            subscribed_at: Utc::now(),\n            message_count: 0,\n        };\n        \n        // Register subscription with server\n        self.call_mcp(\"agent_mail/subscribe\", json!({\n            \"topic\": topic,\n            \"agent\": self.agent_name\n        })).await?;\n        \n        self.subscriptions.push(subscription);\n        Ok(())\n    }\n    \n    /// Send a message to a topic\n    pub async fn publish(\u0026self, topic: \u0026str, message: Message) -\u003e Result\u003c(), AgentMailError\u003e {\n        if !matches!(self.state, ConnectionState::Connected { .. }) {\n            return Err(AgentMailError::NotConnected);\n        }\n        \n        self.call_mcp(\"agent_mail/publish\", json!({\n            \"topic\": topic,\n            \"message\": message,\n            \"sender\": self.agent_name\n        })).await?;\n        \n        Ok(())\n    }\n    \n    /// Send a direct message to another agent\n    pub async fn send_direct(\u0026self, recipient: \u0026str, message: Message) -\u003e Result\u003c(), AgentMailError\u003e {\n        self.call_mcp(\"agent_mail/send\", json!({\n            \"to\": recipient,\n            \"message\": message,\n            \"from\": self.agent_name\n        })).await?;\n        \n        Ok(())\n    }\n    \n    /// Check inbox for new messages\n    pub async fn check_inbox(\u0026self) -\u003e Result\u003cVec\u003cInboxMessage\u003e, AgentMailError\u003e {\n        let response = self.call_mcp(\"agent_mail/inbox\", json!({\n            \"agent\": self.agent_name,\n            \"limit\": 50\n        })).await?;\n        \n        let messages: Vec\u003cInboxMessage\u003e = serde_json::from_value(response)?;\n        Ok(messages)\n    }\n    \n    /// Acknowledge message receipt\n    pub async fn acknowledge(\u0026self, message_id: \u0026str) -\u003e Result\u003c(), AgentMailError\u003e {\n        self.call_mcp(\"agent_mail/ack\", json!({\n            \"message_id\": message_id,\n            \"agent\": self.agent_name\n        })).await?;\n        \n        Ok(())\n    }\n    \n    async fn call_mcp(\u0026self, method: \u0026str, params: serde_json::Value) -\u003e Result\u003cserde_json::Value, AgentMailError\u003e {\n        // MCP call implementation\n        let client = McpClient::connect(\u0026self.mcp_endpoint).await?;\n        let response = client.call(method, params).await?;\n        Ok(response)\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Message {\n    pub id: String,\n    pub message_type: MessageType,\n    pub content: MessageContent,\n    pub metadata: MessageMetadata,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, Hash, Eq, PartialEq)]\npub enum MessageType {\n    SkillShared,\n    PatternDiscovered,\n    SkillRequest,\n    SkillRequestResponse,\n    GenerationStarted,\n    GenerationCompleted,\n    Ping,\n    Pong,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum MessageContent {\n    Skill(SharedSkill),\n    Pattern(SharedPattern),\n    Request(SkillRequest),\n    Response(SkillRequestResponse),\n    Generation(GenerationNotification),\n    Status(AgentStatus),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MessageMetadata {\n    pub sender: String,\n    pub timestamp: DateTime\u003cUtc\u003e,\n    pub priority: Priority,\n    pub ttl_seconds: Option\u003cu64\u003e,\n    pub correlation_id: Option\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct InboxMessage {\n    pub message: Message,\n    pub received_at: DateTime\u003cUtc\u003e,\n    pub read: bool,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Subscription {\n    pub topic: String,\n    pub subscribed_at: DateTime\u003cUtc\u003e,\n    pub message_count: u64,\n}\n```\n\n### Skill Request System\n\n```rust\n/// A request for a skill on a topic (bounty system)\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillRequestBounty {\n    /// Unique request identifier\n    pub id: String,\n    \n    /// Topic being requested\n    pub topic: String,\n    \n    /// Detailed description of what's needed\n    pub description: String,\n    \n    /// Urgency level\n    pub urgency: SkillRequestUrgency,\n    \n    /// Context about why this skill is needed\n    pub context: RequestContext,\n    \n    /// Who created the request\n    pub requester: String,\n    \n    /// When the request was created\n    pub created_at: DateTime\u003cUtc\u003e,\n    \n    /// When the request expires\n    pub expires_at: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// Current status\n    pub status: RequestStatus,\n    \n    /// Responses received\n    pub responses: Vec\u003cRequestResponse\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum SkillRequestUrgency {\n    /// Nice to have, no rush\n    Low,\n    \n    /// Would help current work\n    Medium,\n    \n    /// Blocking current work\n    High,\n    \n    /// Critical blocker\n    Critical,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RequestContext {\n    /// What the requester is trying to accomplish\n    pub goal: String,\n    \n    /// Technologies involved\n    pub technologies: Vec\u003cString\u003e,\n    \n    /// Specific aspects needed\n    pub aspects: Vec\u003cString\u003e,\n    \n    /// Example use cases\n    pub use_cases: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RequestStatus {\n    /// Request is open\n    Open,\n    \n    /// Someone is working on it\n    InProgress { assignee: String },\n    \n    /// Skill has been provided\n    Fulfilled { skill_id: SkillId },\n    \n    /// Request expired\n    Expired,\n    \n    /// Request cancelled\n    Cancelled,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RequestResponse {\n    /// Who responded\n    pub responder: String,\n    \n    /// Response type\n    pub response_type: ResponseType,\n    \n    /// When response was sent\n    pub responded_at: DateTime\u003cUtc\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ResponseType {\n    /// Will work on this\n    WillFulfill { eta: Option\u003cDateTime\u003cUtc\u003e\u003e },\n    \n    /// Have an existing skill that might help\n    ExistingSkill { skill_id: SkillId, relevance: f64 },\n    \n    /// Created new skill\n    NewSkill { skill: SharedSkill },\n    \n    /// Can't help\n    CannotFulfill { reason: String },\n}\n\nimpl SkillRequestBounty {\n    /// Create a new skill request\n    pub fn new(topic: \u0026str, description: \u0026str, urgency: SkillRequestUrgency) -\u003e Self {\n        Self {\n            id: Uuid::new_v4().to_string(),\n            topic: topic.to_string(),\n            description: description.to_string(),\n            urgency,\n            context: RequestContext {\n                goal: String::new(),\n                technologies: Vec::new(),\n                aspects: Vec::new(),\n                use_cases: Vec::new(),\n            },\n            requester: String::new(),\n            created_at: Utc::now(),\n            expires_at: None,\n            status: RequestStatus::Open,\n            responses: Vec::new(),\n        }\n    }\n    \n    /// Set context for the request\n    pub fn with_context(mut self, context: RequestContext) -\u003e Self {\n        self.context = context;\n        self\n    }\n    \n    /// Set expiration\n    pub fn expires_in(mut self, duration: chrono::Duration) -\u003e Self {\n        self.expires_at = Some(Utc::now() + duration);\n        self\n    }\n}\n```\n\n### Pattern Sharing\n\n```rust\n/// Shares discovered patterns with other agents\npub struct PatternSharer {\n    /// Agent mail client\n    mail_client: AgentMailClient,\n    \n    /// Local patterns pending share\n    local_patterns: Vec\u003cExtractedPattern\u003e,\n    \n    /// Patterns received from others\n    received_patterns: Vec\u003cSharedPattern\u003e,\n    \n    /// Sharing policy\n    policy: SharingPolicy,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SharedPattern {\n    /// Unique pattern identifier\n    pub id: String,\n    \n    /// The pattern itself\n    pub pattern: ExtractedPattern,\n    \n    /// Who discovered it\n    pub discovered_by: String,\n    \n    /// Projects where it was observed\n    pub source_projects: Vec\u003cString\u003e,\n    \n    /// How many times it's been observed\n    pub observation_count: u32,\n    \n    /// Confidence score\n    pub confidence: f64,\n    \n    /// When it was shared\n    pub shared_at: DateTime\u003cUtc\u003e,\n    \n    /// How many agents have used it\n    pub adoption_count: u32,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SharingPolicy {\n    /// Minimum confidence to share\n    pub min_confidence: f64,\n    \n    /// Minimum observations before sharing\n    pub min_observations: u32,\n    \n    /// Whether to auto-share new patterns\n    pub auto_share: bool,\n    \n    /// Topics to share (empty = all)\n    pub share_topics: Vec\u003cString\u003e,\n    \n    /// Topics to exclude from sharing\n    pub exclude_topics: Vec\u003cString\u003e,\n}\n\nimpl PatternSharer {\n    pub fn new(mail_client: AgentMailClient) -\u003e Self {\n        Self {\n            mail_client,\n            local_patterns: Vec::new(),\n            received_patterns: Vec::new(),\n            policy: SharingPolicy::default(),\n        }\n    }\n    \n    /// Share a pattern with other agents\n    pub async fn share_pattern(\u0026mut self, pattern: ExtractedPattern) -\u003e Result\u003c(), AgentMailError\u003e {\n        // Check policy\n        if pattern.confidence \u003c self.policy.min_confidence {\n            return Ok(()); // Don't share low-confidence patterns\n        }\n        \n        let shared = SharedPattern {\n            id: Uuid::new_v4().to_string(),\n            pattern: pattern.clone(),\n            discovered_by: self.mail_client.agent_name.clone(),\n            source_projects: vec![self.mail_client.project_key.clone()],\n            observation_count: 1,\n            confidence: pattern.confidence,\n            shared_at: Utc::now(),\n            adoption_count: 0,\n        };\n        \n        let message = Message {\n            id: Uuid::new_v4().to_string(),\n            message_type: MessageType::PatternDiscovered,\n            content: MessageContent::Pattern(shared),\n            metadata: MessageMetadata {\n                sender: self.mail_client.agent_name.clone(),\n                timestamp: Utc::now(),\n                priority: Priority::Normal,\n                ttl_seconds: Some(86400 * 7), // 7 days\n                correlation_id: None,\n            },\n        };\n        \n        self.mail_client.publish(\"patterns/discovered\", message).await?;\n        \n        Ok(())\n    }\n    \n    /// Process received pattern\n    pub fn receive_pattern(\u0026mut self, pattern: SharedPattern) {\n        // Check if we already have this pattern\n        let existing = self.received_patterns.iter_mut()\n            .find(|p| p.pattern.signature() == pattern.pattern.signature());\n        \n        if let Some(existing) = existing {\n            // Merge observations\n            existing.observation_count += pattern.observation_count;\n            existing.confidence = (existing.confidence + pattern.confidence) / 2.0;\n            for project in pattern.source_projects {\n                if !existing.source_projects.contains(\u0026project) {\n                    existing.source_projects.push(project);\n                }\n            }\n        } else {\n            self.received_patterns.push(pattern);\n        }\n    }\n    \n    /// Get patterns relevant to a topic\n    pub fn get_relevant_patterns(\u0026self, topic: \u0026str) -\u003e Vec\u003c\u0026SharedPattern\u003e {\n        self.received_patterns\n            .iter()\n            .filter(|p| p.pattern.relates_to(topic))\n            .collect()\n    }\n}\n\nimpl Default for SharingPolicy {\n    fn default() -\u003e Self {\n        Self {\n            min_confidence: 0.7,\n            min_observations: 3,\n            auto_share: true,\n            share_topics: Vec::new(),\n            exclude_topics: Vec::new(),\n        }\n    }\n}\n```\n\n### Skill Sharing\n\n```rust\n/// Shared skill representation (for transmission)\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SharedSkill {\n    /// Skill identifier\n    pub id: SkillId,\n    \n    /// Skill name\n    pub name: String,\n    \n    /// Brief description\n    pub description: String,\n    \n    /// Skill content (serialized)\n    pub content: String,\n    \n    /// Format version\n    pub format_version: String,\n    \n    /// Who created/shared it\n    pub shared_by: String,\n    \n    /// When it was shared\n    pub shared_at: DateTime\u003cUtc\u003e,\n    \n    /// Effectiveness score if known\n    pub effectiveness_score: Option\u003cf64\u003e,\n    \n    /// Topics covered\n    pub topics: Vec\u003cString\u003e,\n    \n    /// Checksum for integrity\n    pub checksum: String,\n}\n\nimpl SharedSkill {\n    /// Create from a local skill\n    pub fn from_skill(skill: \u0026Skill, sharer: \u0026str) -\u003e Self {\n        let content = serde_json::to_string(skill).unwrap_or_default();\n        let checksum = Self::compute_checksum(\u0026content);\n        \n        Self {\n            id: skill.id.clone(),\n            name: skill.name.clone(),\n            description: skill.description.clone(),\n            content,\n            format_version: \"1.0\".to_string(),\n            shared_by: sharer.to_string(),\n            shared_at: Utc::now(),\n            effectiveness_score: skill.effectiveness_score,\n            topics: skill.topics.clone(),\n            checksum,\n        }\n    }\n    \n    /// Convert back to a Skill\n    pub fn to_skill(\u0026self) -\u003e Result\u003cSkill, serde_json::Error\u003e {\n        // Verify checksum\n        let computed = Self::compute_checksum(\u0026self.content);\n        if computed != self.checksum {\n            // Log warning but continue (could be version difference)\n            tracing::warn!(\"Checksum mismatch for shared skill {}\", self.id.0);\n        }\n        \n        serde_json::from_str(\u0026self.content)\n    }\n    \n    fn compute_checksum(content: \u0026str) -\u003e String {\n        use sha2::{Sha256, Digest};\n        let mut hasher = Sha256::new();\n        hasher.update(content.as_bytes());\n        format!(\"{:x}\", hasher.finalize())\n    }\n}\n\n/// Skill sharing coordinator\npub struct SkillShareCoordinator {\n    mail_client: AgentMailClient,\n    skill_registry: Arc\u003cSkillRegistry\u003e,\n    pending_shares: Vec\u003cSharedSkill\u003e,\n}\n\nimpl SkillShareCoordinator {\n    /// Share a skill with other agents\n    pub async fn share_skill(\u0026mut self, skill: \u0026Skill) -\u003e Result\u003c(), AgentMailError\u003e {\n        let shared = SharedSkill::from_skill(skill, \u0026self.mail_client.agent_name);\n        \n        let message = Message {\n            id: Uuid::new_v4().to_string(),\n            message_type: MessageType::SkillShared,\n            content: MessageContent::Skill(shared),\n            metadata: MessageMetadata {\n                sender: self.mail_client.agent_name.clone(),\n                timestamp: Utc::now(),\n                priority: Priority::Normal,\n                ttl_seconds: None, // Persistent\n                correlation_id: None,\n            },\n        };\n        \n        self.mail_client.publish(\"skills/new\", message).await?;\n        \n        Ok(())\n    }\n    \n    /// Import a received skill\n    pub fn import_skill(\u0026self, shared: \u0026SharedSkill) -\u003e Result\u003c(), ShareError\u003e {\n        let skill = shared.to_skill()?;\n        \n        // Check if we already have this skill\n        if self.skill_registry.exists(\u0026skill.id)? {\n            // Merge or skip based on effectiveness\n            let existing = self.skill_registry.get(\u0026skill.id)?;\n            if let (Some(new_score), Some(old_score)) = (skill.effectiveness_score, existing.effectiveness_score) {\n                if new_score \u003c= old_score {\n                    return Ok(()); // Keep existing, it's better\n                }\n            }\n        }\n        \n        // Import the skill\n        self.skill_registry.save(\u0026skill)?;\n        \n        tracing::info!(\"Imported skill {} from {}\", skill.name, shared.shared_by);\n        \n        Ok(())\n    }\n}\n```\n\n---\n\n## Reservation-Aware Editing\n\nWhen Agent Mail is unavailable, use a local reservation mechanism with compatible semantics to prevent conflicts during skill editing.\n\n```rust\n/// Local reservation mechanism (fallback when Agent Mail unavailable)\npub struct LocalReservationManager {\n    /// Path to reservation lock file\n    lock_dir: PathBuf,\n    \n    /// Active reservations\n    reservations: HashMap\u003cSkillId, Reservation\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Reservation {\n    /// Skill being reserved\n    pub skill_id: SkillId,\n    \n    /// Who holds the reservation\n    pub holder: String,\n    \n    /// When reservation was acquired\n    pub acquired_at: DateTime\u003cUtc\u003e,\n    \n    /// When reservation expires\n    pub expires_at: DateTime\u003cUtc\u003e,\n    \n    /// Purpose of reservation\n    pub purpose: ReservationPurpose,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum ReservationPurpose {\n    Editing,\n    Generation,\n    Sync,\n}\n\nimpl LocalReservationManager {\n    /// Try to acquire a reservation\n    pub fn acquire(\u0026mut self, skill_id: \u0026SkillId, purpose: ReservationPurpose) -\u003e Result\u003cReservation, ReservationError\u003e {\n        // Check for existing reservation\n        if let Some(existing) = self.reservations.get(skill_id) {\n            if existing.expires_at \u003e Utc::now() {\n                return Err(ReservationError::AlreadyReserved {\n                    skill_id: skill_id.clone(),\n                    holder: existing.holder.clone(),\n                    expires_at: existing.expires_at,\n                });\n            }\n        }\n        \n        let reservation = Reservation {\n            skill_id: skill_id.clone(),\n            holder: self.get_local_identity(),\n            acquired_at: Utc::now(),\n            expires_at: Utc::now() + chrono::Duration::minutes(30),\n            purpose,\n        };\n        \n        // Write lock file\n        self.write_lock_file(\u0026reservation)?;\n        \n        self.reservations.insert(skill_id.clone(), reservation.clone());\n        \n        Ok(reservation)\n    }\n    \n    /// Release a reservation\n    pub fn release(\u0026mut self, skill_id: \u0026SkillId) -\u003e Result\u003c(), ReservationError\u003e {\n        if let Some(reservation) = self.reservations.remove(skill_id) {\n            if reservation.holder == self.get_local_identity() {\n                self.remove_lock_file(skill_id)?;\n            }\n        }\n        Ok(())\n    }\n    \n    /// Extend reservation\n    pub fn extend(\u0026mut self, skill_id: \u0026SkillId, duration: chrono::Duration) -\u003e Result\u003c(), ReservationError\u003e {\n        if let Some(reservation) = self.reservations.get_mut(skill_id) {\n            if reservation.holder != self.get_local_identity() {\n                return Err(ReservationError::NotOwner);\n            }\n            reservation.expires_at = Utc::now() + duration;\n            self.write_lock_file(reservation)?;\n        }\n        Ok(())\n    }\n    \n    fn write_lock_file(\u0026self, reservation: \u0026Reservation) -\u003e Result\u003c(), ReservationError\u003e {\n        let lock_path = self.lock_dir.join(format!(\"{}.lock\", reservation.skill_id.0));\n        let content = serde_json::to_string_pretty(reservation)?;\n        std::fs::write(\u0026lock_path, content)?;\n        Ok(())\n    }\n    \n    fn remove_lock_file(\u0026self, skill_id: \u0026SkillId) -\u003e Result\u003c(), ReservationError\u003e {\n        let lock_path = self.lock_dir.join(format!(\"{}.lock\", skill_id.0));\n        if lock_path.exists() {\n            std::fs::remove_file(\u0026lock_path)?;\n        }\n        Ok(())\n    }\n    \n    fn get_local_identity(\u0026self) -\u003e String {\n        hostname::get()\n            .map(|h| h.to_string_lossy().to_string())\n            .unwrap_or_else(|_| \"unknown\".to_string())\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ReservationError {\n    #[error(\"Skill {skill_id:?} already reserved by {holder} until {expires_at}\")]\n    AlreadyReserved {\n        skill_id: SkillId,\n        holder: String,\n        expires_at: DateTime\u003cUtc\u003e,\n    },\n    \n    #[error(\"Not the owner of this reservation\")]\n    NotOwner,\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Serialization error: {0}\")]\n    SerializationError(#[from] serde_json::Error),\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms inbox`\n\n```\nCheck Agent Mail inbox\n\nUSAGE:\n    ms inbox [OPTIONS]\n\nOPTIONS:\n    --unread            Only show unread messages\n    --type \u003cTYPE\u003e       Filter by message type: skill, pattern, request\n    --since \u003cDATE\u003e      Messages since date\n    --limit \u003cN\u003e         Maximum messages to show [default: 20]\n    --ack \u003cID\u003e          Acknowledge a message\n    --ack-all           Acknowledge all messages\n\nOUTPUT EXAMPLE:\n    Inbox (5 unread, 23 total)\n    ==========================\n    \n    [NEW] skill-request from proj-analytics-agent (2 hours ago)\n          Topic: \"Time Series Analysis in Rust\"\n          Urgency: High\n          \n    [NEW] pattern from data-pipeline-agent (5 hours ago)\n          Pattern: \"Streaming CSV Processing\"\n          Confidence: 0.89\n          \n    [NEW] skill-shared from ml-team-agent (1 day ago)\n          Skill: \"python-pandas-optimization\"\n          Effectiveness: 0.85\n          \n    Actions:\n      ms inbox --ack msg-123    Acknowledge message\n      ms request respond 456    Respond to request\n```\n\n### `ms request`\n\n```\nRequest skills from other agents\n\nUSAGE:\n    ms request \u003cSUBCOMMAND\u003e\n\nSUBCOMMANDS:\n    create \u003cTOPIC\u003e      Create a new skill request\n    list                List open requests\n    respond \u003cID\u003e        Respond to a request\n    cancel \u003cID\u003e         Cancel your request\n    status \u003cID\u003e         Check request status\n\nEXAMPLES:\n    # Create a request\n    ms request create \"Kubernetes StatefulSets\" \\\n        --description \"Need guidance on StatefulSet patterns for databases\" \\\n        --urgency high \\\n        --technologies kubernetes,databases \\\n        --expires 7d\n    \n    # List open requests\n    ms request list --topic kubernetes\n    \n    # Respond to a request\n    ms request respond req-456 --skill k8s-statefulsets\n    ms request respond req-456 --will-create --eta \"2 hours\"\n\nOUTPUT (create):\n    Created skill request: req-789\n    Topic: Kubernetes StatefulSets\n    Urgency: High\n    Expires: 2024-01-22\n    \n    Published to: skills/requests\n    Subscribed agents: 12\n```\n\n### `ms subscribe`\n\n```\nSubscribe to skill topics\n\nUSAGE:\n    ms subscribe \u003cTOPIC\u003e\n    ms subscribe --list\n    ms subscribe --unsubscribe \u003cTOPIC\u003e\n\nTOPICS:\n    skills/new              All new skills\n    skills/\u003clanguage\u003e       Skills for a language\n    patterns/discovered     New patterns\n    projects/\u003ckey\u003e/skills   Skills for a project\n\nEXAMPLES:\n    ms subscribe skills/rust\n    ms subscribe patterns/discovered\n    ms subscribe projects/my-team/skills\n    \n    ms subscribe --list\n    \nOUTPUT (list):\n    Active Subscriptions\n    ====================\n    \n    skills/new              (subscribed 30 days ago, 45 messages)\n    skills/rust             (subscribed 7 days ago, 12 messages)\n    patterns/discovered     (subscribed 30 days ago, 23 messages)\n```\n\n### `ms build --broadcast-start`\n\n```\nBroadcast skill generation start to avoid duplicate work\n\nUSAGE:\n    ms build \u003cTOPIC\u003e --broadcast-start [OPTIONS]\n\nOPTIONS:\n    --broadcast-start       Announce generation start\n    --broadcast-complete    Announce generation complete (with skill)\n    --check-in-progress     Check if someone is already building this\n\nBEHAVIOR:\n    When --broadcast-start is used:\n    1. Publishes \"generation-started\" message to skills/generation topic\n    2. Other agents see this and can skip generating the same skill\n    3. On completion, publishes \"generation-completed\" with the skill\n    4. Other agents can import the skill instead of building\n\nEXAMPLE:\n    $ ms build \"Rust async patterns\" --broadcast-start\n    \n    Broadcasting generation start...\n    Checking for in-progress generation... none found\n    \n    Generating skill: rust-async-patterns\n    [=========\u003e          ] 45%\n    \n    Generation complete!\n    Broadcasting completion with skill...\n    \n    Skill published to: skills/new\n    Agents notified: 8\n```\n\n---\n\n## Connection Management\n\n```rust\n/// Manages Agent Mail connection lifecycle\npub struct AgentMailManager {\n    client: AgentMailClient,\n    config: AgentMailConfig,\n    reconnect_task: Option\u003ctokio::task::JoinHandle\u003c()\u003e\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct AgentMailConfig {\n    /// MCP endpoint URL\n    pub endpoint: String,\n    \n    /// Project key\n    pub project_key: String,\n    \n    /// Agent name\n    pub agent_name: String,\n    \n    /// Auto-reconnect on disconnect\n    pub auto_reconnect: bool,\n    \n    /// Reconnect interval (seconds)\n    pub reconnect_interval_secs: u64,\n    \n    /// Maximum reconnect attempts\n    pub max_reconnect_attempts: u32,\n    \n    /// Whether to use local fallback\n    pub use_local_fallback: bool,\n}\n\nimpl AgentMailManager {\n    /// Initialize from configuration\n    pub fn from_config(config: AgentMailConfig) -\u003e Self {\n        let client = AgentMailClient::new(\n            \u0026config.project_key,\n            \u0026config.agent_name,\n            \u0026config.endpoint,\n        );\n        \n        Self {\n            client,\n            config,\n            reconnect_task: None,\n        }\n    }\n    \n    /// Start connection with auto-reconnect\n    pub async fn start(\u0026mut self) -\u003e Result\u003c(), AgentMailError\u003e {\n        match self.client.connect().await {\n            Ok(()) =\u003e {\n                tracing::info!(\"Connected to Agent Mail\");\n                Ok(())\n            }\n            Err(e) if self.config.use_local_fallback =\u003e {\n                tracing::warn!(\"Agent Mail unavailable, using local fallback: {}\", e);\n                Ok(())\n            }\n            Err(e) =\u003e Err(e),\n        }\n    }\n    \n    /// Check if connected (or using fallback)\n    pub fn is_available(\u0026self) -\u003e bool {\n        matches!(self.client.state, ConnectionState::Connected { .. })\n            || self.config.use_local_fallback\n    }\n    \n    /// Get client reference\n    pub fn client(\u0026self) -\u003e \u0026AgentMailClient {\n        \u0026self.client\n    }\n}\n\nimpl Default for AgentMailConfig {\n    fn default() -\u003e Self {\n        Self {\n            endpoint: \"http://localhost:3000/mcp\".to_string(),\n            project_key: \"default\".to_string(),\n            agent_name: hostname::get()\n                .map(|h| h.to_string_lossy().to_string())\n                .unwrap_or_else(|_| \"unknown-agent\".to_string()),\n            auto_reconnect: true,\n            reconnect_interval_secs: 30,\n            max_reconnect_attempts: 10,\n            use_local_fallback: true,\n        }\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum AgentMailError {\n    #[error(\"Not connected to Agent Mail\")]\n    NotConnected,\n    \n    #[error(\"Connection failed: {0}\")]\n    ConnectionFailed(String),\n    \n    #[error(\"MCP error: {0}\")]\n    McpError(String),\n    \n    #[error(\"Serialization error: {0}\")]\n    SerializationError(#[from] serde_json::Error),\n    \n    #[error(\"Message expired\")]\n    MessageExpired,\n    \n    #[error(\"Topic not found: {0}\")]\n    TopicNotFound(String),\n    \n    #[error(\"Permission denied: {0}\")]\n    PermissionDenied(String),\n    \n    #[error(\"Rate limited\")]\n    RateLimited,\n}\n\n#[derive(Debug, thiserror::Error)]\npub enum ShareError {\n    #[error(\"Skill parsing error: {0}\")]\n    ParseError(#[from] serde_json::Error),\n    \n    #[error(\"Registry error: {0}\")]\n    RegistryError(String),\n    \n    #[error(\"Checksum mismatch\")]\n    ChecksumMismatch,\n}\n```\n\n---\n\n## Configuration\n\nConfiguration in `~/.config/meta_skill/agent_mail.toml`:\n\n```toml\n[connection]\nendpoint = \"http://agent-mail.internal:3000/mcp\"\nproject_key = \"my-project\"\nagent_name = \"dev-laptop\"\nauto_reconnect = true\nreconnect_interval_secs = 30\nmax_reconnect_attempts = 10\nuse_local_fallback = true\n\n[sharing]\nauto_share_skills = false\nauto_share_patterns = true\nmin_pattern_confidence = 0.7\nmin_pattern_observations = 3\n\n[subscriptions]\ndefault_topics = [\n    \"skills/new\",\n    \"skills/rust\",\n    \"patterns/discovered\"\n]\n\n[notifications]\non_skill_request = true\non_pattern_discovered = true\non_skill_shared = false  # Can be noisy\n```\n\n---\n\n## Dependencies\n\n- **MCP Server Mode** (meta_skill-ugf): MCP protocol support for Agent Mail communication\n- `tokio`: Async runtime\n- `serde`, `serde_json`: Message serialization\n- `chrono`: Timestamps\n- `uuid`: Message IDs\n- `sha2`: Checksums","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-13T23:00:03.594409474-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:07.69132757-05:00","labels":["agent-mail","coordination","phase-6"],"dependencies":[{"issue_id":"meta_skill-tzu","depends_on_id":"meta_skill-ugf","type":"blocks","created_at":"2026-01-13T23:04:15.23692171-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-tzu","depends_on_id":"meta_skill-237","type":"blocks","created_at":"2026-01-13T23:43:23.919324587-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ugf","title":"[P6] MCP Server Mode","description":"# MCP Server Mode\n\nNative agent tool-use integration via MCP protocol.\n\n## Tasks\n1. Implement MCP server using mcp-rust-sdk\n2. Define tool schemas for all commands\n3. Handle tool calls\n4. Streaming responses\n5. Resource exposure for skills\n\n## MCP Tools Exposed\n- ms_search: Search skills\n- ms_load: Load skill content\n- ms_suggest: Get suggestions\n- ms_list: List skills\n- ms_show: Show skill details\n\n## Tool Schemas\n```json\n{\n  \"name\": \"ms_search\",\n  \"description\": \"Search for skills matching a query\",\n  \"inputSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"query\": {\"type\": \"string\"},\n      \"limit\": {\"type\": \"integer\", \"default\": 10}\n    },\n    \"required\": [\"query\"]\n  }\n}\n```\n\n## Benefits over Subprocess\n- No process spawn overhead\n- Persistent connection\n- Streaming support\n- Type-safe schemas\n\n## CLI\n- `ms mcp serve` - Start MCP server\n- Listens on stdin/stdout (standard MCP transport)\n\n## Acceptance Criteria\n- MCP server implements full protocol\n- All commands exposed as tools\n- Works with Claude desktop, etc.","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:28:22.510815207-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:28:22.510815207-05:00","labels":["integration","mcp","phase-6"],"dependencies":[{"issue_id":"meta_skill-ugf","depends_on_id":"meta_skill-0ki","type":"blocks","created_at":"2026-01-13T22:28:36.950862753-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ugf","depends_on_id":"meta_skill-7va","type":"blocks","created_at":"2026-01-13T22:28:36.978593985-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ujr","title":"Multi-Machine Synchronization","description":"# Multi-Machine Synchronization\n\n**Phase 5 - Section 8.5**\n\nSynchronize skills across multiple development machines. This feature enables developers to maintain consistent skill libraries across workstations, laptops, and cloud environments with robust conflict resolution.\n\n---\n\n## Overview\n\nMulti-machine sync solves the problem of skill fragmentation when developers work across multiple environments. Skills created on a laptop should be available on a workstation, and vice versa. The system tracks machine identity, maintains sync state, and resolves conflicts when the same skill is modified on different machines.\n\n---\n\n## Core Data Structures\n\n### Machine Identity\n\n```rust\nuse std::collections::HashMap;\nuse chrono::{DateTime, Utc};\nuse serde::{Deserialize, Serialize};\nuse uuid::Uuid;\n\n/// Unique identity for a development machine participating in sync\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MachineIdentity {\n    /// Globally unique machine identifier (generated on first sync setup)\n    pub machine_id: String,\n    \n    /// Human-readable machine name (e.g., \"work-laptop\", \"home-desktop\")\n    pub machine_name: String,\n    \n    /// Last sync timestamp per remote (remote_url -\u003e last_sync_time)\n    pub sync_timestamps: HashMap\u003cString, DateTime\u003cUtc\u003e\u003e,\n    \n    /// Machine-specific metadata\n    pub metadata: MachineMetadata,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MachineMetadata {\n    /// Operating system (linux, macos, windows)\n    pub os: String,\n    \n    /// Hostname at registration time\n    pub hostname: String,\n    \n    /// When this machine was first registered\n    pub registered_at: DateTime\u003cUtc\u003e,\n    \n    /// Optional user-provided description\n    pub description: Option\u003cString\u003e,\n}\n\nimpl MachineIdentity {\n    /// Generate a new machine identity\n    pub fn generate(machine_name: String) -\u003e Self {\n        Self {\n            machine_id: Uuid::new_v4().to_string(),\n            machine_name,\n            sync_timestamps: HashMap::new(),\n            metadata: MachineMetadata {\n                os: std::env::consts::OS.to_string(),\n                hostname: hostname::get()\n                    .map(|h| h.to_string_lossy().to_string())\n                    .unwrap_or_else(|_| \"unknown\".to_string()),\n                registered_at: Utc::now(),\n                description: None,\n            },\n        }\n    }\n    \n    /// Update sync timestamp for a remote\n    pub fn record_sync(\u0026mut self, remote_url: \u0026str) {\n        self.sync_timestamps.insert(remote_url.to_string(), Utc::now());\n    }\n    \n    /// Get last sync time for a remote\n    pub fn last_sync(\u0026self, remote_url: \u0026str) -\u003e Option\u003cDateTime\u003cUtc\u003e\u003e {\n        self.sync_timestamps.get(remote_url).copied()\n    }\n    \n    /// Path to machine identity file\n    pub fn identity_path() -\u003e PathBuf {\n        dirs::data_local_dir()\n            .unwrap_or_else(|| PathBuf::from(\".\"))\n            .join(\"meta_skill\")\n            .join(\"machine_identity.json\")\n    }\n    \n    /// Load or generate machine identity\n    pub fn load_or_generate() -\u003e Result\u003cSelf, SyncError\u003e {\n        let path = Self::identity_path();\n        if path.exists() {\n            let content = std::fs::read_to_string(\u0026path)?;\n            Ok(serde_json::from_str(\u0026content)?)\n        } else {\n            let identity = Self::generate(Self::default_machine_name());\n            identity.save()?;\n            Ok(identity)\n        }\n    }\n    \n    fn default_machine_name() -\u003e String {\n        hostname::get()\n            .map(|h| h.to_string_lossy().to_string())\n            .unwrap_or_else(|_| \"default-machine\".to_string())\n    }\n    \n    pub fn save(\u0026self) -\u003e Result\u003c(), SyncError\u003e {\n        let path = Self::identity_path();\n        if let Some(parent) = path.parent() {\n            std::fs::create_dir_all(parent)?;\n        }\n        let content = serde_json::to_string_pretty(self)?;\n        std::fs::write(\u0026path, content)?;\n        Ok(())\n    }\n}\n```\n\n### Sync State\n\n```rust\nuse std::collections::HashMap;\n\n/// Per-skill sync state\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillSyncState {\n    /// Skill identifier\n    pub skill_id: SkillId,\n    \n    /// Local version hash (content-addressable)\n    pub local_hash: String,\n    \n    /// Known remote hashes (remote_url -\u003e hash)\n    pub remote_hashes: HashMap\u003cString, String\u003e,\n    \n    /// Sync status\n    pub status: SkillSyncStatus,\n    \n    /// Last modification time locally\n    pub local_modified: DateTime\u003cUtc\u003e,\n    \n    /// Machine that last modified this skill\n    pub last_modified_by: String,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum SkillSyncStatus {\n    /// Local matches all remotes\n    Synced,\n    \n    /// Local has changes not pushed\n    LocalAhead,\n    \n    /// Remote has changes not pulled\n    RemoteAhead,\n    \n    /// Both local and remote have diverged\n    Diverged,\n    \n    /// Skill only exists locally\n    LocalOnly,\n    \n    /// Skill only exists on remote\n    RemoteOnly,\n    \n    /// Conflict detected and unresolved\n    Conflict(ConflictInfo),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub struct ConflictInfo {\n    /// Machine IDs involved in conflict\n    pub machines: Vec\u003cString\u003e,\n    \n    /// When conflict was detected\n    pub detected_at: DateTime\u003cUtc\u003e,\n    \n    /// Brief description of conflict\n    pub description: String,\n}\n\n/// Global sync state for this machine\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SyncState {\n    /// Per-skill sync states\n    pub skill_states: HashMap\u003cSkillId, SkillSyncState\u003e,\n    \n    /// Configured remotes\n    pub remotes: Vec\u003cRemoteConfig\u003e,\n    \n    /// Last time a full sync was performed\n    pub last_full_sync: Option\u003cDateTime\u003cUtc\u003e\u003e,\n    \n    /// This machine's identity\n    pub machine: MachineIdentity,\n    \n    /// Pending operations (for resume after interruption)\n    pub pending_ops: Vec\u003cPendingOperation\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum PendingOperation {\n    Push { skill_id: SkillId, remote: String },\n    Pull { skill_id: SkillId, remote: String },\n    Resolve { skill_id: SkillId, strategy: ConflictStrategy },\n}\n\nimpl SyncState {\n    pub fn new(machine: MachineIdentity) -\u003e Self {\n        Self {\n            skill_states: HashMap::new(),\n            remotes: Vec::new(),\n            last_full_sync: None,\n            machine,\n            pending_ops: Vec::new(),\n        }\n    }\n    \n    /// Get skills that need pushing\n    pub fn needs_push(\u0026self) -\u003e Vec\u003c\u0026SkillSyncState\u003e {\n        self.skill_states\n            .values()\n            .filter(|s| matches!(s.status, SkillSyncStatus::LocalAhead | SkillSyncStatus::LocalOnly))\n            .collect()\n    }\n    \n    /// Get skills that need pulling\n    pub fn needs_pull(\u0026self) -\u003e Vec\u003c\u0026SkillSyncState\u003e {\n        self.skill_states\n            .values()\n            .filter(|s| matches!(s.status, SkillSyncStatus::RemoteAhead | SkillSyncStatus::RemoteOnly))\n            .collect()\n    }\n    \n    /// Get skills with conflicts\n    pub fn conflicts(\u0026self) -\u003e Vec\u003c\u0026SkillSyncState\u003e {\n        self.skill_states\n            .values()\n            .filter(|s| matches!(s.status, SkillSyncStatus::Conflict(_) | SkillSyncStatus::Diverged))\n            .collect()\n    }\n    \n    /// Calculate content hash for a skill\n    pub fn hash_skill(skill: \u0026Skill) -\u003e String {\n        use sha2::{Sha256, Digest};\n        let mut hasher = Sha256::new();\n        \n        // Hash skill content deterministically\n        hasher.update(skill.name.as_bytes());\n        hasher.update(skill.description.as_bytes());\n        \n        // Sort sections for deterministic hashing\n        let mut sections: Vec\u003c_\u003e = skill.sections.iter().collect();\n        sections.sort_by_key(|(name, _)| *name);\n        \n        for (name, section) in sections {\n            hasher.update(name.as_bytes());\n            hasher.update(section.content.as_bytes());\n        }\n        \n        format!(\"{:x}\", hasher.finalize())\n    }\n}\n```\n\n### Remote Configuration\n\n```rust\n/// Type of remote storage\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum RemoteType {\n    /// Git repository (GitHub, GitLab, etc.)\n    Git,\n    \n    /// Amazon S3 or compatible (MinIO, DigitalOcean Spaces)\n    S3,\n    \n    /// Custom HTTP API\n    Custom,\n}\n\n/// Configuration for a sync remote\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RemoteConfig {\n    /// Unique name for this remote (e.g., \"origin\", \"backup\")\n    pub name: String,\n    \n    /// Remote type\n    pub remote_type: RemoteType,\n    \n    /// Connection URL\n    pub url: String,\n    \n    /// Authentication configuration\n    pub auth: RemoteAuth,\n    \n    /// Whether this remote is enabled\n    pub enabled: bool,\n    \n    /// Push/pull settings\n    pub settings: RemoteSettings,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum RemoteAuth {\n    /// No authentication (public remote)\n    None,\n    \n    /// SSH key authentication (for Git)\n    SshKey { key_path: PathBuf },\n    \n    /// Token-based authentication\n    Token { token_env_var: String },\n    \n    /// AWS credentials (for S3)\n    AwsCredentials {\n        access_key_env: String,\n        secret_key_env: String,\n        region: String,\n    },\n    \n    /// Basic HTTP authentication\n    Basic { username: String, password_env: String },\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RemoteSettings {\n    /// Auto-push on skill save\n    pub auto_push: bool,\n    \n    /// Auto-pull on sync check\n    pub auto_pull: bool,\n    \n    /// Sync frequency for background sync (in minutes, 0 = disabled)\n    pub sync_interval_minutes: u32,\n    \n    /// Skills to exclude from this remote (glob patterns)\n    pub exclude_patterns: Vec\u003cString\u003e,\n    \n    /// Only sync skills matching these patterns (empty = all)\n    pub include_patterns: Vec\u003cString\u003e,\n}\n\nimpl RemoteConfig {\n    /// Create a Git remote configuration\n    pub fn git(name: \u0026str, url: \u0026str) -\u003e Self {\n        Self {\n            name: name.to_string(),\n            remote_type: RemoteType::Git,\n            url: url.to_string(),\n            auth: RemoteAuth::SshKey {\n                key_path: dirs::home_dir()\n                    .unwrap_or_default()\n                    .join(\".ssh\")\n                    .join(\"id_rsa\"),\n            },\n            enabled: true,\n            settings: RemoteSettings::default(),\n        }\n    }\n    \n    /// Create an S3 remote configuration\n    pub fn s3(name: \u0026str, bucket: \u0026str, region: \u0026str) -\u003e Self {\n        Self {\n            name: name.to_string(),\n            remote_type: RemoteType::S3,\n            url: format!(\"s3://{}\", bucket),\n            auth: RemoteAuth::AwsCredentials {\n                access_key_env: \"AWS_ACCESS_KEY_ID\".to_string(),\n                secret_key_env: \"AWS_SECRET_ACCESS_KEY\".to_string(),\n                region: region.to_string(),\n            },\n            enabled: true,\n            settings: RemoteSettings::default(),\n        }\n    }\n}\n\nimpl Default for RemoteSettings {\n    fn default() -\u003e Self {\n        Self {\n            auto_push: false,\n            auto_pull: true,\n            sync_interval_minutes: 0,\n            exclude_patterns: vec![],\n            include_patterns: vec![],\n        }\n    }\n}\n```\n\n### Conflict Resolution\n\n```rust\n/// Strategy for resolving sync conflicts\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum ConflictStrategy {\n    /// Always prefer local version\n    KeepLocal,\n    \n    /// Always prefer remote version\n    KeepRemote,\n    \n    /// Prefer version from specific machine\n    PreferMachine(String),\n    \n    /// Keep most recently modified\n    LatestWins,\n    \n    /// Merge changes (section-level)\n    Merge,\n    \n    /// Create both versions with suffixes\n    Fork,\n    \n    /// Prompt user interactively\n    Manual,\n}\n\n/// Conflict resolver with configurable strategies\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ConflictResolver {\n    /// Default strategy for all skills\n    pub default_strategy: ConflictStrategy,\n    \n    /// Per-skill strategy overrides\n    pub skill_strategies: HashMap\u003cSkillId, ConflictStrategy\u003e,\n    \n    /// Machine priority for PreferMachine strategy\n    pub machine_priority: Vec\u003cString\u003e,\n}\n\nimpl ConflictResolver {\n    pub fn new(default_strategy: ConflictStrategy) -\u003e Self {\n        Self {\n            default_strategy,\n            skill_strategies: HashMap::new(),\n            machine_priority: Vec::new(),\n        }\n    }\n    \n    /// Get strategy for a specific skill\n    pub fn strategy_for(\u0026self, skill_id: \u0026SkillId) -\u003e \u0026ConflictStrategy {\n        self.skill_strategies\n            .get(skill_id)\n            .unwrap_or(\u0026self.default_strategy)\n    }\n    \n    /// Resolve a conflict between local and remote versions\n    pub fn resolve(\n        \u0026self,\n        skill_id: \u0026SkillId,\n        local: \u0026Skill,\n        remote: \u0026Skill,\n        local_machine: \u0026str,\n        remote_machine: \u0026str,\n    ) -\u003e Result\u003cConflictResolution, SyncError\u003e {\n        let strategy = self.strategy_for(skill_id);\n        \n        match strategy {\n            ConflictStrategy::KeepLocal =\u003e Ok(ConflictResolution::UseLocal),\n            \n            ConflictStrategy::KeepRemote =\u003e Ok(ConflictResolution::UseRemote),\n            \n            ConflictStrategy::PreferMachine(machine_id) =\u003e {\n                if machine_id == local_machine {\n                    Ok(ConflictResolution::UseLocal)\n                } else if machine_id == remote_machine {\n                    Ok(ConflictResolution::UseRemote)\n                } else {\n                    // Fallback to latest wins\n                    self.resolve_by_timestamp(local, remote)\n                }\n            }\n            \n            ConflictStrategy::LatestWins =\u003e self.resolve_by_timestamp(local, remote),\n            \n            ConflictStrategy::Merge =\u003e self.merge_skills(local, remote),\n            \n            ConflictStrategy::Fork =\u003e Ok(ConflictResolution::Fork {\n                local_suffix: format!(\"-{}\", local_machine),\n                remote_suffix: format!(\"-{}\", remote_machine),\n            }),\n            \n            ConflictStrategy::Manual =\u003e Ok(ConflictResolution::RequiresManual),\n        }\n    }\n    \n    fn resolve_by_timestamp(\n        \u0026self,\n        local: \u0026Skill,\n        remote: \u0026Skill,\n    ) -\u003e Result\u003cConflictResolution, SyncError\u003e {\n        if local.modified_at \u003e= remote.modified_at {\n            Ok(ConflictResolution::UseLocal)\n        } else {\n            Ok(ConflictResolution::UseRemote)\n        }\n    }\n    \n    fn merge_skills(\u0026self, local: \u0026Skill, remote: \u0026Skill) -\u003e Result\u003cConflictResolution, SyncError\u003e {\n        let mut merged_sections = HashMap::new();\n        let mut conflicts = Vec::new();\n        \n        // Collect all section names\n        let all_sections: HashSet\u003c_\u003e = local.sections.keys()\n            .chain(remote.sections.keys())\n            .collect();\n        \n        for section_name in all_sections {\n            match (local.sections.get(section_name), remote.sections.get(section_name)) {\n                (Some(l), Some(r)) if l.content != r.content =\u003e {\n                    // Section differs - record conflict\n                    conflicts.push(SectionConflict {\n                        section: section_name.clone(),\n                        local_content: l.content.clone(),\n                        remote_content: r.content.clone(),\n                    });\n                }\n                (Some(l), Some(_)) =\u003e {\n                    // Same content\n                    merged_sections.insert(section_name.clone(), l.clone());\n                }\n                (Some(l), None) =\u003e {\n                    // Only in local\n                    merged_sections.insert(section_name.clone(), l.clone());\n                }\n                (None, Some(r)) =\u003e {\n                    // Only in remote\n                    merged_sections.insert(section_name.clone(), r.clone());\n                }\n                (None, None) =\u003e unreachable!(),\n            }\n        }\n        \n        if conflicts.is_empty() {\n            Ok(ConflictResolution::Merged { sections: merged_sections })\n        } else {\n            Ok(ConflictResolution::PartialMerge {\n                merged_sections,\n                conflicts,\n            })\n        }\n    }\n}\n\n#[derive(Debug)]\npub enum ConflictResolution {\n    UseLocal,\n    UseRemote,\n    Merged { sections: HashMap\u003cString, Section\u003e },\n    PartialMerge {\n        merged_sections: HashMap\u003cString, Section\u003e,\n        conflicts: Vec\u003cSectionConflict\u003e,\n    },\n    Fork { local_suffix: String, remote_suffix: String },\n    RequiresManual,\n}\n\n#[derive(Debug)]\npub struct SectionConflict {\n    pub section: String,\n    pub local_content: String,\n    pub remote_content: String,\n}\n```\n\n---\n\n## Sync Engine\n\n```rust\n/// Main synchronization engine\npub struct SyncEngine {\n    /// Current sync state\n    state: SyncState,\n    \n    /// Conflict resolver\n    resolver: ConflictResolver,\n    \n    /// Skill registry for local operations\n    registry: Arc\u003cSkillRegistry\u003e,\n    \n    /// Remote clients\n    clients: HashMap\u003cString, Box\u003cdyn RemoteClient\u003e\u003e,\n}\n\n#[async_trait]\npub trait RemoteClient: Send + Sync {\n    /// List all skills on remote\n    async fn list_skills(\u0026self) -\u003e Result\u003cVec\u003cRemoteSkillInfo\u003e, SyncError\u003e;\n    \n    /// Get skill content by ID\n    async fn get_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cSkill\u003e, SyncError\u003e;\n    \n    /// Push skill to remote\n    async fn push_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003c(), SyncError\u003e;\n    \n    /// Delete skill from remote\n    async fn delete_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003c(), SyncError\u003e;\n    \n    /// Get skill hash without downloading content\n    async fn get_hash(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cString\u003e, SyncError\u003e;\n}\n\n#[derive(Debug)]\npub struct RemoteSkillInfo {\n    pub id: SkillId,\n    pub hash: String,\n    pub modified_at: DateTime\u003cUtc\u003e,\n    pub modified_by: String,\n}\n\nimpl SyncEngine {\n    /// Perform full sync with all remotes\n    pub async fn sync_all(\u0026mut self) -\u003e Result\u003cSyncReport, SyncError\u003e {\n        let mut report = SyncReport::new();\n        \n        for remote in \u0026self.state.remotes.clone() {\n            if !remote.enabled {\n                continue;\n            }\n            \n            match self.sync_remote(\u0026remote.name).await {\n                Ok(remote_report) =\u003e report.merge(remote_report),\n                Err(e) =\u003e report.add_error(\u0026remote.name, e),\n            }\n        }\n        \n        self.state.last_full_sync = Some(Utc::now());\n        self.save_state()?;\n        \n        Ok(report)\n    }\n    \n    /// Sync with a specific remote\n    pub async fn sync_remote(\u0026mut self, remote_name: \u0026str) -\u003e Result\u003cSyncReport, SyncError\u003e {\n        let client = self.clients.get(remote_name)\n            .ok_or_else(|| SyncError::UnknownRemote(remote_name.to_string()))?;\n        \n        let mut report = SyncReport::new();\n        \n        // Get remote skill list\n        let remote_skills = client.list_skills().await?;\n        let remote_map: HashMap\u003c_, _\u003e = remote_skills.iter()\n            .map(|s| (s.id.clone(), s))\n            .collect();\n        \n        // Get local skills\n        let local_skills = self.registry.list_all()?;\n        \n        // Process each local skill\n        for skill in \u0026local_skills {\n            let local_hash = SyncState::hash_skill(skill);\n            \n            if let Some(remote_info) = remote_map.get(\u0026skill.id) {\n                if local_hash != remote_info.hash {\n                    // Diverged - need to resolve\n                    self.handle_divergence(skill, remote_info, client.as_ref(), \u0026mut report).await?;\n                } else {\n                    report.add_synced(\u0026skill.id);\n                }\n            } else {\n                // Local only - push\n                client.push_skill(skill).await?;\n                report.add_pushed(\u0026skill.id);\n            }\n        }\n        \n        // Process remote-only skills\n        let local_ids: HashSet\u003c_\u003e = local_skills.iter().map(|s| \u0026s.id).collect();\n        for remote_info in \u0026remote_skills {\n            if !local_ids.contains(\u0026remote_info.id) {\n                // Remote only - pull\n                if let Some(skill) = client.get_skill(\u0026remote_info.id).await? {\n                    self.registry.save(\u0026skill)?;\n                    report.add_pulled(\u0026remote_info.id);\n                }\n            }\n        }\n        \n        // Update sync timestamp\n        self.state.machine.record_sync(\u0026self.get_remote_url(remote_name)?);\n        \n        Ok(report)\n    }\n    \n    async fn handle_divergence(\n        \u0026mut self,\n        local: \u0026Skill,\n        remote_info: \u0026RemoteSkillInfo,\n        client: \u0026dyn RemoteClient,\n        report: \u0026mut SyncReport,\n    ) -\u003e Result\u003c(), SyncError\u003e {\n        let remote = client.get_skill(\u0026local.id).await?\n            .ok_or_else(|| SyncError::SkillNotFound(local.id.clone()))?;\n        \n        let resolution = self.resolver.resolve(\n            \u0026local.id,\n            local,\n            \u0026remote,\n            \u0026self.state.machine.machine_id,\n            \u0026remote_info.modified_by,\n        )?;\n        \n        match resolution {\n            ConflictResolution::UseLocal =\u003e {\n                client.push_skill(local).await?;\n                report.add_pushed(\u0026local.id);\n            }\n            ConflictResolution::UseRemote =\u003e {\n                self.registry.save(\u0026remote)?;\n                report.add_pulled(\u0026local.id);\n            }\n            ConflictResolution::Merged { sections } =\u003e {\n                let mut merged = local.clone();\n                merged.sections = sections;\n                merged.modified_at = Utc::now();\n                self.registry.save(\u0026merged)?;\n                client.push_skill(\u0026merged).await?;\n                report.add_merged(\u0026local.id);\n            }\n            ConflictResolution::PartialMerge { merged_sections, conflicts } =\u003e {\n                // Save what we can, record conflicts\n                let mut partial = local.clone();\n                partial.sections = merged_sections;\n                self.registry.save(\u0026partial)?;\n                \n                for conflict in conflicts {\n                    report.add_conflict(\u0026local.id, \u0026conflict.section);\n                }\n            }\n            ConflictResolution::Fork { local_suffix, remote_suffix } =\u003e {\n                // Create forked versions\n                let mut local_fork = local.clone();\n                local_fork.id = SkillId(format!(\"{}{}\", local.id.0, local_suffix));\n                \n                let mut remote_fork = remote.clone();\n                remote_fork.id = SkillId(format!(\"{}{}\", remote.id.0, remote_suffix));\n                \n                self.registry.save(\u0026local_fork)?;\n                self.registry.save(\u0026remote_fork)?;\n                client.push_skill(\u0026local_fork).await?;\n                \n                report.add_forked(\u0026local.id, \u0026local_fork.id, \u0026remote_fork.id);\n            }\n            ConflictResolution::RequiresManual =\u003e {\n                report.add_manual_required(\u0026local.id);\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n#[derive(Debug, Default)]\npub struct SyncReport {\n    pub synced: Vec\u003cSkillId\u003e,\n    pub pushed: Vec\u003cSkillId\u003e,\n    pub pulled: Vec\u003cSkillId\u003e,\n    pub merged: Vec\u003cSkillId\u003e,\n    pub conflicts: Vec\u003c(SkillId, String)\u003e,\n    pub forked: Vec\u003c(SkillId, SkillId, SkillId)\u003e,\n    pub manual_required: Vec\u003cSkillId\u003e,\n    pub errors: Vec\u003c(String, SyncError)\u003e,\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms sync`\n\n```\nSynchronize skills with remote storage\n\nUSAGE:\n    ms sync [OPTIONS] [REMOTE]\n\nARGS:\n    [REMOTE]    Specific remote to sync (default: all enabled remotes)\n\nOPTIONS:\n    --push-only         Only push local changes, don't pull\n    --pull-only         Only pull remote changes, don't push\n    --dry-run           Show what would be synced without making changes\n    --force             Force sync even with conflicts (uses default strategy)\n    -v, --verbose       Show detailed sync progress\n\nEXAMPLES:\n    ms sync                     # Sync with all remotes\n    ms sync origin              # Sync only with 'origin' remote\n    ms sync --pull-only         # Only download new/updated skills\n    ms sync --dry-run           # Preview sync operations\n```\n\n### `ms sync --status`\n\n```\nShow synchronization status\n\nUSAGE:\n    ms sync --status [OPTIONS]\n\nOPTIONS:\n    --remote \u003cNAME\u003e     Check status for specific remote\n    --skill \u003cSKILL\u003e     Check status for specific skill\n    --conflicts         Only show skills with conflicts\n    --pending           Only show skills with pending changes\n\nOUTPUT EXAMPLE:\n    Machine: work-laptop (abc123)\n    Last full sync: 2 hours ago\n\n    Remote: origin (git@github.com:user/skills.git)\n      Status: Connected\n      Skills synced: 42\n      Local ahead: 3\n      Remote ahead: 1\n      Conflicts: 0\n\n    Pending Changes:\n      rust-error-handling    [local ahead]   Modified 5 minutes ago\n      python-testing         [local ahead]   Modified 1 hour ago\n      go-concurrency         [local ahead]   Created today\n      \n    Available from remote:\n      java-spring-boot       [remote only]   By: home-desktop\n```\n\n### `ms remote add`\n\n```\nAdd a sync remote\n\nUSAGE:\n    ms remote add \u003cNAME\u003e \u003cURL\u003e [OPTIONS]\n\nARGS:\n    \u003cNAME\u003e    Name for this remote (e.g., 'origin', 'backup')\n    \u003cURL\u003e     Remote URL\n\nOPTIONS:\n    --type \u003cTYPE\u003e       Remote type: git, s3, custom [default: auto-detect]\n    --auth \u003cMETHOD\u003e     Authentication: ssh, token, aws, basic\n    --token-env \u003cVAR\u003e   Environment variable containing auth token\n    --auto-push         Enable automatic push on skill save\n    --auto-pull         Enable automatic pull on sync check\n    --interval \u003cMIN\u003e    Background sync interval in minutes\n\nEXAMPLES:\n    ms remote add origin git@github.com:user/skills.git\n    ms remote add backup s3://my-bucket/skills --auth aws\n    ms remote add work https://skills.company.com --auth token --token-env SKILLS_TOKEN\n\nOTHER SUBCOMMANDS:\n    ms remote list              List configured remotes\n    ms remote remove \u003cNAME\u003e     Remove a remote\n    ms remote set-url \u003cNAME\u003e    Update remote URL\n    ms remote enable \u003cNAME\u003e     Enable a disabled remote\n    ms remote disable \u003cNAME\u003e    Disable a remote\n```\n\n### `ms conflicts`\n\n```\nManage sync conflicts\n\nUSAGE:\n    ms conflicts \u003cSUBCOMMAND\u003e\n\nSUBCOMMANDS:\n    list                List all unresolved conflicts\n    show \u003cSKILL\u003e        Show detailed conflict for a skill\n    resolve \u003cSKILL\u003e     Resolve conflict for a skill\n    strategy            Configure conflict resolution strategies\n\nEXAMPLES:\n    ms conflicts list\n    ms conflicts show rust-error-handling\n    ms conflicts resolve rust-error-handling --keep-local\n    ms conflicts resolve rust-error-handling --keep-remote\n    ms conflicts resolve rust-error-handling --merge\n    ms conflicts resolve rust-error-handling --manual  # Open in editor\n    \n    # Set default strategy\n    ms conflicts strategy --default latest-wins\n    \n    # Set per-skill strategy\n    ms conflicts strategy rust-error-handling --prefer-machine work-laptop\n```\n\n---\n\n## Git Remote Implementation\n\n```rust\n/// Git-based remote client\npub struct GitRemoteClient {\n    /// Repository URL\n    url: String,\n    \n    /// Local clone path\n    local_path: PathBuf,\n    \n    /// Branch to sync\n    branch: String,\n    \n    /// Git authentication\n    auth: GitAuth,\n}\n\nimpl GitRemoteClient {\n    pub fn new(url: \u0026str, auth: RemoteAuth) -\u003e Result\u003cSelf, SyncError\u003e {\n        let local_path = dirs::cache_dir()\n            .unwrap_or_else(|| PathBuf::from(\".cache\"))\n            .join(\"meta_skill\")\n            .join(\"remotes\")\n            .join(Self::url_to_dirname(url));\n        \n        let git_auth = match auth {\n            RemoteAuth::SshKey { key_path } =\u003e GitAuth::Ssh { key_path },\n            RemoteAuth::Token { token_env_var } =\u003e {\n                let token = std::env::var(\u0026token_env_var)\n                    .map_err(|_| SyncError::MissingAuth(token_env_var))?;\n                GitAuth::Token(token)\n            }\n            _ =\u003e GitAuth::None,\n        };\n        \n        Ok(Self {\n            url: url.to_string(),\n            local_path,\n            branch: \"main\".to_string(),\n            auth: git_auth,\n        })\n    }\n    \n    /// Ensure local clone is up to date\n    async fn ensure_clone(\u0026self) -\u003e Result\u003c(), SyncError\u003e {\n        if self.local_path.exists() {\n            // Fetch latest\n            self.run_git(\u0026[\"fetch\", \"origin\", \u0026self.branch]).await?;\n            self.run_git(\u0026[\"reset\", \"--hard\", \u0026format!(\"origin/{}\", self.branch)]).await?;\n        } else {\n            // Clone fresh\n            std::fs::create_dir_all(\u0026self.local_path)?;\n            self.run_git(\u0026[\"clone\", \"--branch\", \u0026self.branch, \u0026self.url, \".\"]).await?;\n        }\n        Ok(())\n    }\n    \n    /// Commit and push changes\n    async fn commit_and_push(\u0026self, message: \u0026str) -\u003e Result\u003c(), SyncError\u003e {\n        self.run_git(\u0026[\"add\", \"-A\"]).await?;\n        self.run_git(\u0026[\"commit\", \"-m\", message]).await?;\n        self.run_git(\u0026[\"push\", \"origin\", \u0026self.branch]).await?;\n        Ok(())\n    }\n}\n\n#[async_trait]\nimpl RemoteClient for GitRemoteClient {\n    async fn list_skills(\u0026self) -\u003e Result\u003cVec\u003cRemoteSkillInfo\u003e, SyncError\u003e {\n        self.ensure_clone().await?;\n        \n        let mut skills = Vec::new();\n        let skills_dir = self.local_path.join(\"skills\");\n        \n        if skills_dir.exists() {\n            for entry in std::fs::read_dir(\u0026skills_dir)? {\n                let entry = entry?;\n                if entry.path().is_dir() {\n                    if let Some(info) = self.read_skill_info(\u0026entry.path())? {\n                        skills.push(info);\n                    }\n                }\n            }\n        }\n        \n        Ok(skills)\n    }\n    \n    async fn get_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cSkill\u003e, SyncError\u003e {\n        let skill_path = self.local_path.join(\"skills\").join(\u0026id.0);\n        if !skill_path.exists() {\n            return Ok(None);\n        }\n        \n        // Parse skill from directory structure\n        let skill = self.parse_skill_directory(\u0026skill_path)?;\n        Ok(Some(skill))\n    }\n    \n    async fn push_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003c(), SyncError\u003e {\n        self.ensure_clone().await?;\n        \n        let skill_path = self.local_path.join(\"skills\").join(\u0026skill.id.0);\n        std::fs::create_dir_all(\u0026skill_path)?;\n        \n        // Write skill to directory structure\n        self.write_skill_directory(\u0026skill_path, skill)?;\n        \n        // Commit and push\n        let message = format!(\"Update skill: {}\", skill.name);\n        self.commit_and_push(\u0026message).await?;\n        \n        Ok(())\n    }\n    \n    async fn delete_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003c(), SyncError\u003e {\n        self.ensure_clone().await?;\n        \n        let skill_path = self.local_path.join(\"skills\").join(\u0026id.0);\n        if skill_path.exists() {\n            std::fs::remove_dir_all(\u0026skill_path)?;\n            let message = format!(\"Delete skill: {}\", id.0);\n            self.commit_and_push(\u0026message).await?;\n        }\n        \n        Ok(())\n    }\n    \n    async fn get_hash(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cString\u003e, SyncError\u003e {\n        if let Some(skill) = self.get_skill(id).await? {\n            Ok(Some(SyncState::hash_skill(\u0026skill)))\n        } else {\n            Ok(None)\n        }\n    }\n}\n```\n\n---\n\n## S3 Remote Implementation\n\n```rust\nuse aws_sdk_s3::{Client as S3Client, Config};\n\n/// S3-based remote client\npub struct S3RemoteClient {\n    client: S3Client,\n    bucket: String,\n    prefix: String,\n}\n\nimpl S3RemoteClient {\n    pub async fn new(bucket: \u0026str, region: \u0026str, prefix: Option\u003c\u0026str\u003e) -\u003e Result\u003cSelf, SyncError\u003e {\n        let config = aws_config::from_env()\n            .region(aws_sdk_s3::config::Region::new(region.to_string()))\n            .load()\n            .await;\n        \n        let client = S3Client::new(\u0026config);\n        \n        Ok(Self {\n            client,\n            bucket: bucket.to_string(),\n            prefix: prefix.unwrap_or(\"skills\").to_string(),\n        })\n    }\n    \n    fn skill_key(\u0026self, id: \u0026SkillId) -\u003e String {\n        format!(\"{}/{}/skill.json\", self.prefix, id.0)\n    }\n}\n\n#[async_trait]\nimpl RemoteClient for S3RemoteClient {\n    async fn list_skills(\u0026self) -\u003e Result\u003cVec\u003cRemoteSkillInfo\u003e, SyncError\u003e {\n        let mut skills = Vec::new();\n        let mut continuation_token = None;\n        \n        loop {\n            let mut request = self.client\n                .list_objects_v2()\n                .bucket(\u0026self.bucket)\n                .prefix(\u0026self.prefix)\n                .delimiter(\"/\");\n            \n            if let Some(token) = continuation_token {\n                request = request.continuation_token(token);\n            }\n            \n            let response = request.send().await?;\n            \n            if let Some(prefixes) = response.common_prefixes {\n                for prefix in prefixes {\n                    if let Some(p) = prefix.prefix {\n                        // Extract skill ID from prefix\n                        let skill_id = p.trim_end_matches('/').rsplit('/').next()\n                            .map(|s| SkillId(s.to_string()));\n                        \n                        if let Some(id) = skill_id {\n                            if let Ok(Some(hash)) = self.get_hash(\u0026id).await {\n                                skills.push(RemoteSkillInfo {\n                                    id,\n                                    hash,\n                                    modified_at: Utc::now(), // TODO: Get from metadata\n                                    modified_by: \"unknown\".to_string(),\n                                });\n                            }\n                        }\n                    }\n                }\n            }\n            \n            if response.is_truncated == Some(true) {\n                continuation_token = response.next_continuation_token;\n            } else {\n                break;\n            }\n        }\n        \n        Ok(skills)\n    }\n    \n    async fn get_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cSkill\u003e, SyncError\u003e {\n        let key = self.skill_key(id);\n        \n        match self.client.get_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .send()\n            .await\n        {\n            Ok(response) =\u003e {\n                let body = response.body.collect().await?;\n                let skill: Skill = serde_json::from_slice(\u0026body.into_bytes())?;\n                Ok(Some(skill))\n            }\n            Err(e) if e.is_not_found() =\u003e Ok(None),\n            Err(e) =\u003e Err(SyncError::S3Error(e.to_string())),\n        }\n    }\n    \n    async fn push_skill(\u0026self, skill: \u0026Skill) -\u003e Result\u003c(), SyncError\u003e {\n        let key = self.skill_key(\u0026skill.id);\n        let body = serde_json::to_vec_pretty(skill)?;\n        \n        self.client.put_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .body(body.into())\n            .content_type(\"application/json\")\n            .send()\n            .await?;\n        \n        Ok(())\n    }\n    \n    async fn delete_skill(\u0026self, id: \u0026SkillId) -\u003e Result\u003c(), SyncError\u003e {\n        let key = self.skill_key(id);\n        \n        self.client.delete_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .send()\n            .await?;\n        \n        Ok(())\n    }\n    \n    async fn get_hash(\u0026self, id: \u0026SkillId) -\u003e Result\u003cOption\u003cString\u003e, SyncError\u003e {\n        // Use head_object to check if exists and get ETag\n        let key = self.skill_key(id);\n        \n        match self.client.head_object()\n            .bucket(\u0026self.bucket)\n            .key(\u0026key)\n            .send()\n            .await\n        {\n            Ok(response) =\u003e Ok(response.e_tag),\n            Err(e) if e.is_not_found() =\u003e Ok(None),\n            Err(e) =\u003e Err(SyncError::S3Error(e.to_string())),\n        }\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum SyncError {\n    #[error(\"Unknown remote: {0}\")]\n    UnknownRemote(String),\n    \n    #[error(\"Skill not found: {0:?}\")]\n    SkillNotFound(SkillId),\n    \n    #[error(\"Missing authentication: {0}\")]\n    MissingAuth(String),\n    \n    #[error(\"Git operation failed: {0}\")]\n    GitError(String),\n    \n    #[error(\"S3 operation failed: {0}\")]\n    S3Error(String),\n    \n    #[error(\"Network error: {0}\")]\n    NetworkError(String),\n    \n    #[error(\"Serialization error: {0}\")]\n    SerializationError(#[from] serde_json::Error),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Conflict requires manual resolution: {0:?}\")]\n    ManualResolutionRequired(SkillId),\n}\n```\n\n---\n\n## Configuration File\n\nMachine identity and sync configuration stored in `~/.config/meta_skill/sync.toml`:\n\n```toml\n[machine]\nid = \"abc123-def456\"\nname = \"work-laptop\"\ndescription = \"Primary development machine\"\n\n[sync]\ndefault_conflict_strategy = \"latest-wins\"\nauto_sync_interval_minutes = 30\n\n[[remotes]]\nname = \"origin\"\ntype = \"git\"\nurl = \"git@github.com:user/skills.git\"\nenabled = true\nauto_push = true\nauto_pull = true\n\n[[remotes]]\nname = \"backup\"\ntype = \"s3\"\nurl = \"s3://my-skills-bucket/skills\"\nregion = \"us-east-1\"\nenabled = true\nauto_push = true\nauto_pull = false\n\n[conflict_strategies]\n\"work-critical-skill\" = \"keep-local\"\n\"shared-team-skill\" = \"prefer-machine:team-server\"\n```\n\n---\n\n## Testing\n\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_machine_identity_generation() {\n        let identity = MachineIdentity::generate(\"test-machine\".to_string());\n        assert!(!identity.machine_id.is_empty());\n        assert_eq!(identity.machine_name, \"test-machine\");\n    }\n    \n    #[test]\n    fn test_skill_hashing_deterministic() {\n        let skill = Skill {\n            id: SkillId(\"test\".to_string()),\n            name: \"Test Skill\".to_string(),\n            sections: HashMap::from([\n                (\"overview\".to_string(), Section { content: \"content\".to_string() }),\n            ]),\n            ..Default::default()\n        };\n        \n        let hash1 = SyncState::hash_skill(\u0026skill);\n        let hash2 = SyncState::hash_skill(\u0026skill);\n        \n        assert_eq!(hash1, hash2);\n    }\n    \n    #[test]\n    fn test_conflict_resolution_latest_wins() {\n        let resolver = ConflictResolver::new(ConflictStrategy::LatestWins);\n        \n        let older = Skill {\n            id: SkillId(\"test\".to_string()),\n            modified_at: Utc::now() - chrono::Duration::hours(1),\n            ..Default::default()\n        };\n        \n        let newer = Skill {\n            id: SkillId(\"test\".to_string()),\n            modified_at: Utc::now(),\n            ..Default::default()\n        };\n        \n        let resolution = resolver.resolve(\n            \u0026older.id, \u0026older, \u0026newer, \"machine-a\", \"machine-b\"\n        ).unwrap();\n        \n        assert!(matches!(resolution, ConflictResolution::UseRemote));\n    }\n    \n    #[tokio::test]\n    async fn test_sync_engine_local_only_pushes() {\n        // Test that local-only skills get pushed to remote\n        let mut engine = create_test_engine();\n        let skill = create_test_skill(\"local-only\");\n        engine.registry.save(\u0026skill).unwrap();\n        \n        let report = engine.sync_all().await.unwrap();\n        \n        assert!(report.pushed.contains(\u0026skill.id));\n    }\n}\n```\n\n---\n\n## Dependencies\n\n- **Bundle Format and Manifest** (meta_skill-6fi): Sync uses bundle format for transferring skills\n- `serde`, `serde_json`: Serialization\n- `chrono`: Timestamps\n- `sha2`: Content hashing\n- `uuid`: Machine ID generation\n- `git2`: Git operations (optional)\n- `aws-sdk-s3`: S3 operations (optional)\n- `tokio`: Async runtime\n- `thiserror`: Error handling","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:54:00.445843757-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:30:01.68296559-05:00","labels":["multi-machine","phase-5","sync"],"dependencies":[{"issue_id":"meta_skill-ujr","depends_on_id":"meta_skill-6fi","type":"blocks","created_at":"2026-01-13T23:04:13.477055066-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ujr","depends_on_id":"meta_skill-swe","type":"blocks","created_at":"2026-01-13T23:43:35.346232597-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-vc3","title":"[P4] Autonomous Build Mode","description":"# Autonomous Build Mode\n\nLong-running skill generation with checkpoints.\n\n## Tasks\n1. Define BuildSession state machine\n2. Implement checkpointing to disk\n3. Recovery from interrupted sessions\n4. Progress reporting\n5. Quality gates between phases\n\n## Autonomous Mode (from Section 8.7)\n- `ms build --autonomous --duration 4h`\n- Runs unattended for specified duration\n- Checkpoints every N minutes\n- Can be interrupted and resumed\n\n## Build Phases\n1. Discovery: Find relevant sessions\n2. Extraction: Pull patterns from sessions\n3. Aggregation: Cluster similar patterns\n4. Transformation: Generalize patterns\n5. Compilation: Generate skill spec\n6. Validation: Test against held-out sessions\n\n## Checkpoint Format\n```rust\nstruct BuildCheckpoint {\n    session_id: String,\n    phase: BuildPhase,\n    state: serde_json::Value,\n    created_at: DateTime,\n    patterns_so_far: Vec\u003cExtractedPattern\u003e,\n}\n```\n\n## CLI\n- `ms build --autonomous --duration 4h`\n- `ms build --resume \u003csession_id\u003e`\n- `ms build --status` - Check running builds\n\n## Acceptance Criteria\n- Builds run for hours unattended\n- Checkpoints enable resume\n- Quality gates prevent garbage","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:51.39259279-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:51.39259279-05:00","labels":["autonomous","checkpoints","phase-4"],"dependencies":[{"issue_id":"meta_skill-vc3","depends_on_id":"meta_skill-330","type":"blocks","created_at":"2026-01-13T22:26:13.178995276-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-vc3","depends_on_id":"meta_skill-llm","type":"blocks","created_at":"2026-01-13T22:26:13.209862769-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-vqr","title":"[P1] Robot Mode Infrastructure","description":"## Overview\n\nImplement the Robot Mode infrastructure that provides machine-readable JSON output for all ms commands. This follows the xf pattern exactly and enables tight integration with orchestration tools (NTM, BV) and other AI agents. Robot mode is a cornerstone of the agent flywheel ecosystemevery CLI tool supports it to enable programmatic consumption.\n\n## Background \u0026 Rationale\n\n### Why Robot Mode is Essential\n\n1. **Agent Tool Use**: LLMs can reliably parse JSON, not human-formatted tables\n2. **Composability**: Tools can call other tools and process results\n3. **Stability**: JSON schemas are versioned; human output changes frequently  \n4. **Completeness**: JSON includes metadata humans would not want to see\n5. **Error Handling**: Structured errors enable automatic retry/recovery\n\n### The Robot Mode Convention\n\nFollowing the xf and CASS patterns exactly:\n- **stdout = data only**: Valid JSON, nothing else\n- **stderr = diagnostics**: Progress, warnings, errors (human-readable)\n- **Exit code 0 = success**: Operation completed\n- **Exit code non-zero = failure**: With structured error in stdout\n- **Schema documented and versioned**: Breaking changes increment version\n\n### Why This Matters for ms\n\nWithout robot mode:\n- Agents must regex-parse human output (brittle)\n- Version updates break integrations\n- No way to distinguish data from diagnostics\n- Error handling requires string matching\n\nWith robot mode:\n- Agents deserialize structured JSON\n- Schema versioning enables graceful migration\n- Clean separation of data and diagnostics\n- Errors include codes, hints, and recovery suggestions\n\n## Key Data Structures (from Plan Section 4.5)\n\n```rust\nuse serde::{Deserialize, Serialize};\nuse chrono::{DateTime, Utc};\n\n/// Universal wrapper for all robot mode responses\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RobotResponse\u003cT\u003e {\n    /// Response status indicator\n    pub status: RobotStatus,\n    /// ISO-8601 timestamp of response generation\n    pub timestamp: DateTime\u003cUtc\u003e,\n    /// API schema version (for compatibility checking)\n    pub version: String,\n    /// The actual response payload (None if error)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub data: Option\u003cT\u003e,\n    /// Error details (None if success)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub error: Option\u003cRobotError\u003e,\n}\n\n/// Response status enumeration\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum RobotStatus {\n    /// Operation completed successfully\n    Success,\n    /// Operation failed with error\n    Error,\n    /// Operation requires user approval to proceed\n    ApprovalRequired,\n    /// Operation partially completed\n    Partial,\n}\n\n/// Structured error information\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RobotError {\n    /// Machine-readable error code\n    pub code: ErrorCode,\n    /// Human-readable error message\n    pub message: String,\n    /// Optional hint for resolution\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub hint: Option\u003cString\u003e,\n    /// Additional error context\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub context: Option\u003cserde_json::Value\u003e,\n    /// Stack of underlying causes\n    #[serde(skip_serializing_if = \"Vec::is_empty\", default)]\n    pub causes: Vec\u003cString\u003e,\n}\n\n/// Error codes for programmatic handling\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"snake_case\")]\npub enum ErrorCode {\n    // Resource errors\n    SkillNotFound,\n    BundleNotFound,\n    SessionNotFound,\n    FileNotFound,\n    \n    // Validation errors\n    InvalidSkillSpec,\n    InvalidQuery,\n    InvalidConfig,\n    SchemaViolation,\n    \n    // State errors\n    DatabaseCorrupted,\n    GitConflict,\n    LockContention,\n    \n    // External errors\n    CassUnavailable,\n    NetworkError,\n    IoError,\n    \n    // Permission errors\n    PermissionDenied,\n    ApprovalRequired,\n    \n    // Internal errors\n    InternalError,\n    NotImplemented,\n}\n\n/// Robot mode output context\npub struct RobotContext {\n    /// Current API version\n    pub version: String,\n    /// Whether robot mode is enabled\n    pub enabled: bool,\n}\n\nimpl RobotContext {\n    pub const CURRENT_VERSION: \u0026str = \"1.0.0\";\n    \n    pub fn new(enabled: bool) -\u003e Self {\n        Self {\n            version: Self::CURRENT_VERSION.to_string(),\n            enabled,\n        }\n    }\n    \n    /// Create a success response\n    pub fn success\u003cT: Serialize\u003e(\u0026self, data: T) -\u003e RobotResponse\u003cT\u003e {\n        RobotResponse {\n            status: RobotStatus::Success,\n            timestamp: Utc::now(),\n            version: self.version.clone(),\n            data: Some(data),\n            error: None,\n        }\n    }\n    \n    /// Create an error response\n    pub fn error\u003cT\u003e(\u0026self, code: ErrorCode, message: impl Into\u003cString\u003e) -\u003e RobotResponse\u003cT\u003e {\n        RobotResponse {\n            status: RobotStatus::Error,\n            timestamp: Utc::now(),\n            version: self.version.clone(),\n            data: None,\n            error: Some(RobotError {\n                code,\n                message: message.into(),\n                hint: None,\n                context: None,\n                causes: vec\\![],\n            }),\n        }\n    }\n    \n    /// Create an error response with hint\n    pub fn error_with_hint\u003cT\u003e(\n        \u0026self, \n        code: ErrorCode, \n        message: impl Into\u003cString\u003e,\n        hint: impl Into\u003cString\u003e,\n    ) -\u003e RobotResponse\u003cT\u003e {\n        RobotResponse {\n            status: RobotStatus::Error,\n            timestamp: Utc::now(),\n            version: self.version.clone(),\n            data: None,\n            error: Some(RobotError {\n                code,\n                message: message.into(),\n                hint: Some(hint.into()),\n                context: None,\n                causes: vec\\![],\n            }),\n        }\n    }\n}\n\n/// Helper trait for outputting robot mode responses\npub trait RobotOutput {\n    /// Output as JSON to stdout (robot mode) or human format (normal mode)\n    fn output(\u0026self, robot: bool) -\u003e std::io::Result\u003c()\u003e;\n}\n\nimpl\u003cT: Serialize + std::fmt::Display\u003e RobotOutput for RobotResponse\u003cT\u003e {\n    fn output(\u0026self, robot: bool) -\u003e std::io::Result\u003c()\u003e {\n        use std::io::Write;\n        \n        if robot {\n            let json = serde_json::to_string_pretty(self)?;\n            writeln\\!(std::io::stdout(), \"{}\", json)?;\n        } else {\n            match \u0026self.data {\n                Some(data) =\u003e writeln\\!(std::io::stdout(), \"{}\", data)?,\n                None =\u003e {\n                    if let Some(err) = \u0026self.error {\n                        eprintln\\!(\"Error: {}\", err.message);\n                        if let Some(hint) = \u0026err.hint {\n                            eprintln\\!(\"Hint: {}\", hint);\n                        }\n                    }\n                }\n            }\n        }\n        Ok(())\n    }\n}\n```\n\n## Tasks\n\n### Task 1: Define Core Robot Types\n- [ ] Create `src/robot/mod.rs` module\n- [ ] Define `RobotResponse\u003cT\u003e` generic wrapper struct\n- [ ] Define `RobotStatus` enum (success, error, approval_required, partial)\n- [ ] Define `RobotError` struct with code, message, hint, context\n- [ ] Define `ErrorCode` enum with all error categories\n- [ ] Implement Serialize/Deserialize for all types\n\n### Task 2: Implement RobotContext\n- [ ] Create `RobotContext` struct to track robot mode state\n- [ ] Implement `success()` method for success responses\n- [ ] Implement `error()` method for error responses\n- [ ] Implement `error_with_hint()` for actionable errors\n- [ ] Implement `partial()` for partial success cases\n- [ ] Add version constant (CURRENT_VERSION = \"1.0.0\")\n\n### Task 3: Implement --robot Flag Parsing\n- [ ] Add `--robot` flag to clap derive at top level\n- [ ] Propagate robot mode context through command dispatch\n- [ ] Ensure flag works for all subcommands\n- [ ] Add `--robot-pretty` variant for formatted JSON\n- [ ] Document flag in --help output\n\n### Task 4: Implement Output Separation\n- [ ] Route data to stdout only\n- [ ] Route diagnostics to stderr only\n- [ ] Ensure progress bars go to stderr\n- [ ] Ensure warnings go to stderr\n- [ ] Validate no data leaks to stderr in robot mode\n\n### Task 5: Implement Exit Code Protocol\n- [ ] Exit 0 for success\n- [ ] Exit 1 for general errors\n- [ ] Exit 2 for validation errors\n- [ ] Exit 3 for permission errors\n- [ ] Exit 4 for resource not found\n- [ ] Exit 5 for internal errors\n- [ ] Document exit codes in --help\n\n### Task 6: Implement Schema Versioning\n- [ ] Add version field to all responses\n- [ ] Define version bump policy (major/minor/patch)\n- [ ] Create schema documentation generator\n- [ ] Add version mismatch detection helpers\n- [ ] Implement deprecation warnings for old versions\n\n### Task 7: Create Response Builders for Each Command\n- [ ] `RegistryStatus` for `ms status`\n- [ ] `SearchResponse` for `ms search`\n- [ ] `SuggestResponse` for `ms suggest`\n- [ ] `LoadResponse` for `ms load`\n- [ ] `BuildResponse` for `ms build`\n- [ ] `DoctorResponse` for `ms doctor`\n\n## Example Robot Mode Output\n\n### Success Response\n```json\n{\n  \"status\": \"success\",\n  \"timestamp\": \"2026-01-13T12:00:00.000Z\",\n  \"version\": \"1.0.0\",\n  \"data\": {\n    \"query\": \"git commit\",\n    \"results\": [\n      {\n        \"id\": \"git/commit\",\n        \"score\": 0.87,\n        \"scores\": {\"bm25\": 0.9, \"embedding\": 0.8, \"rrf\": 0.87},\n        \"snippet\": \"Best practices for writing commit messages...\",\n        \"layer\": \"global\",\n        \"tags\": [\"git\", \"workflow\"]\n      }\n    ],\n    \"total_count\": 1,\n    \"search_time_ms\": 12\n  }\n}\n```\n\n### Error Response\n```json\n{\n  \"status\": \"error\",\n  \"timestamp\": \"2026-01-13T12:00:00.000Z\",\n  \"version\": \"1.0.0\",\n  \"error\": {\n    \"code\": \"skill_not_found\",\n    \"message\": \"Skill 'foo' not found in registry\",\n    \"hint\": \"Run 'ms list' to see available skills, or 'ms search foo' to find similar skills\",\n    \"context\": {\n      \"searched_layers\": [\"system\", \"global\", \"project\"],\n      \"similar_skills\": [\"food\", \"foo-bar\"]\n    }\n  }\n}\n```\n\n## Acceptance Criteria\n\n1. **All Commands**: Every command supports --robot flag\n2. **Valid JSON**: stdout is always valid JSON in robot mode\n3. **No Leakage**: No diagnostic output goes to stdout\n4. **Exit Codes**: Exit codes are consistent and documented\n5. **Schema Version**: Version field present in all responses\n6. **Error Hints**: Errors include actionable hints\n7. **Type Safety**: All response types derive Serialize/Deserialize\n\n## Testing Requirements\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    \n    #[test]\n    fn test_success_response_serialization() {\n        let ctx = RobotContext::new(true);\n        let response: RobotResponse\u003cVec\u003cString\u003e\u003e = ctx.success(vec\\![\"a\".into(), \"b\".into()]);\n        \n        let json = serde_json::to_string(\u0026response).unwrap();\n        assert\\!(json.contains(r#\"\"status\":\"success\"\"#));\n        assert\\!(json.contains(r#\"\"version\":\"1.0.0\"\"#));\n        assert\\!(json.contains(r#\"\"data\":[\"a\",\"b\"]\"#));\n    }\n    \n    #[test]\n    fn test_error_response_serialization() {\n        let ctx = RobotContext::new(true);\n        let response: RobotResponse\u003c()\u003e = ctx.error_with_hint(\n            ErrorCode::SkillNotFound,\n            \"Skill 'test' not found\",\n            \"Run 'ms list' to see available skills\",\n        );\n        \n        let json = serde_json::to_string(\u0026response).unwrap();\n        assert\\!(json.contains(r#\"\"status\":\"error\"\"#));\n        assert\\!(json.contains(r#\"\"code\":\"skill_not_found\"\"#));\n        assert\\!(json.contains(\"hint\"));\n    }\n    \n    #[test]\n    fn test_error_code_snake_case() {\n        let code = ErrorCode::SkillNotFound;\n        let json = serde_json::to_string(\u0026code).unwrap();\n        assert_eq\\!(json, r#\"\"skill_not_found\"\"#);\n    }\n    \n    #[test]\n    fn test_response_roundtrip() {\n        let ctx = RobotContext::new(true);\n        let original: RobotResponse\u003ci32\u003e = ctx.success(42);\n        \n        let json = serde_json::to_string(\u0026original).unwrap();\n        let deserialized: RobotResponse\u003ci32\u003e = serde_json::from_str(\u0026json).unwrap();\n        \n        assert_eq\\!(original.status, deserialized.status);\n        assert_eq\\!(original.data, deserialized.data);\n        assert_eq\\!(original.version, deserialized.version);\n    }\n    \n    #[test]\n    fn test_status_variants() {\n        assert_eq\\!(\n            serde_json::to_string(\u0026RobotStatus::Success).unwrap(),\n            r#\"\"success\"\"#\n        );\n        assert_eq\\!(\n            serde_json::to_string(\u0026RobotStatus::Error).unwrap(),\n            r#\"\"error\"\"#\n        );\n        assert_eq\\!(\n            serde_json::to_string(\u0026RobotStatus::ApprovalRequired).unwrap(),\n            r#\"\"approval_required\"\"#\n        );\n    }\n    \n    #[test]\n    fn test_error_with_causes() {\n        let err = RobotError {\n            code: ErrorCode::DatabaseCorrupted,\n            message: \"Database integrity check failed\".into(),\n            hint: Some(\"Run 'ms doctor --fix' to repair\".into()),\n            context: None,\n            causes: vec\\![\n                \"Foreign key constraint violation\".into(),\n                \"Row 42 references nonexistent skill\".into(),\n            ],\n        };\n        \n        let json = serde_json::to_string_pretty(\u0026err).unwrap();\n        assert\\!(json.contains(\"causes\"));\n        assert\\!(json.contains(\"Foreign key\"));\n    }\n}\n```\n\n### Integration Tests\n```rust\nuse assert_cmd::Command;\nuse serde_json::Value;\n\n#[test]\nfn test_robot_mode_valid_json() {\n    let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n    let output = cmd.args(\u0026[\"status\", \"--robot\"])\n        .output()\n        .unwrap();\n    \n    let stdout = String::from_utf8_lossy(\u0026output.stdout);\n    let parsed: Value = serde_json::from_str(\u0026stdout).expect(\"stdout must be valid JSON\");\n    \n    assert\\!(parsed[\"status\"].is_string());\n    assert\\!(parsed[\"version\"].is_string());\n    assert\\!(parsed[\"timestamp\"].is_string());\n}\n\n#[test]\nfn test_robot_mode_no_stderr_in_success() {\n    let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n    let output = cmd.args(\u0026[\"list\", \"--robot\"])\n        .output()\n        .unwrap();\n    \n    // stderr should be empty or contain only warnings\n    let stderr = String::from_utf8_lossy(\u0026output.stderr);\n    assert\\!(stderr.is_empty() || stderr.contains(\"warn\"), \n        \"stderr should be empty or contain only warnings in robot mode\");\n}\n\n#[test]\nfn test_robot_mode_error_includes_hint() {\n    let mut cmd = Command::cargo_bin(\"ms\").unwrap();\n    let output = cmd.args(\u0026[\"show\", \"nonexistent-skill\", \"--robot\"])\n        .output()\n        .unwrap();\n    \n    let stdout = String::from_utf8_lossy(\u0026output.stdout);\n    let parsed: Value = serde_json::from_str(\u0026stdout).expect(\"stdout must be valid JSON\");\n    \n    assert_eq\\!(parsed[\"status\"], \"error\");\n    assert\\!(parsed[\"error\"][\"hint\"].is_string());\n}\n```\n\n### Logging Requirements\nAll robot mode operations must log:\n- `DEBUG`: Robot mode enabled/disabled, response construction\n- `INFO`: Response status, timing information\n- `WARN`: Deprecated API usage, schema version mismatches\n- `ERROR`: Serialization failures, output errors\n\nExample log output (goes to stderr):\n```\n[DEBUG] Robot mode enabled, version 1.0.0\n[DEBUG] Executing search command\n[INFO] Search completed in 12ms, returning 5 results\n[DEBUG] Serializing RobotResponse\u003cSearchResponse\u003e\n```\n\n## Integration Examples\n\n```bash\n# Parse search results in shell script\nresults=$(ms search \"git commit\" --robot | jq -r '.data.results[].id')\n\n# Chain with other tools\nfor skill_id in $(ms suggest --robot | jq -r '.data.suggestions[].id'); do\n    content=$(ms load $skill_id --robot | jq -r '.data.content')\n    echo \"=== $skill_id ===\"\n    echo \"$content\"\ndone\n\n# Error handling in scripts\noutput=$(ms show \"$SKILL_ID\" --robot 2\u003e/dev/null)\nif echo \"$output\" | jq -e '.status == \"success\"' \u003e/dev/null; then\n    echo \"$output\" | jq -r '.data'\nelse\n    hint=$(echo \"$output\" | jq -r '.error.hint // \"No hint available\"')\n    echo \"Error: $hint\" \u003e\u00262\n    exit 1\nfi\n```\n\n## References\n\n- Plan Section 4.5: Robot Mode (Comprehensive Specification)\n- Plan Section 0.10: Robot Mode: The AI-Native CLI Pattern\n- xf implementation: /data/projects/xf/src/robot/mod.rs\n- CASS robot mode: /data/projects/coding_agent_session_search/src/output/robot.rs\n- Depends on: meta_skill-5s0 (Rust Project Scaffolding)\n- Blocks: meta_skill-14h (CLI Commands), meta_skill-hhu (CASS Integration)\n\nLabels: [api phase-1 robot-mode]","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:22:05.914271765-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:37:52.568154483-05:00","labels":["api","phase-1","robot-mode"],"dependencies":[{"issue_id":"meta_skill-vqr","depends_on_id":"meta_skill-5s0","type":"blocks","created_at":"2026-01-13T22:22:14.875278949-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-wnk","title":"Snapshot Tests","description":"## Overview\n\nImplement snapshot tests using the insta crate for output verification. This bead implements Section 18.5 of the Testing Strategy, ensuring all CLI outputs, disclosure levels, error messages, and diagnostic outputs remain consistent.\n\n## Requirements\n\n### 1. Snapshot Test Configuration\n\nAdd to `Cargo.toml`:\n```toml\n[dev-dependencies]\ninsta = { version = \"1.34\", features = [\"yaml\", \"json\", \"redactions\"] }\n```\n\nCreate `insta.yaml` in project root:\n```yaml\n# Insta configuration\nbehavior:\n  review: true\n  update_mode: new\n  \nsnapshot_path_template: \"{module}/{function}\"\n```\n\n### 2. CLI Output Format Snapshots\n\nCreate `tests/snapshots/cli_output.rs`:\n\n```rust\nuse insta::{assert_snapshot, assert_json_snapshot, with_settings};\nuse ms::cli::{OutputFormat, run_command};\n\n#[test]\nfn test_list_output_human() {\n    let output = run_command(\u0026[\"list\"], OutputFormat::Human);\n    \n    with_settings!({\n        description =\u003e \"Human-readable list output\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"list_human\", output);\n    });\n}\n\n#[test]\nfn test_list_output_robot_json() {\n    let output = run_command(\u0026[\"list\", \"--robot\"], OutputFormat::Json);\n    \n    with_settings!({\n        description =\u003e \"Robot-mode JSON list output\",\n        omit_expression =\u003e true,\n    }, {\n        assert_json_snapshot!(\"list_robot_json\", output);\n    });\n}\n\n#[test]\nfn test_search_output_human() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"search\", \"rust\"], OutputFormat::Human);\n    \n    with_settings!({\n        description =\u003e \"Human-readable search results\",\n        omit_expression =\u003e true,\n        redactions =\u003e redact_dynamic_fields(),\n    }, {\n        assert_snapshot!(\"search_human\", output);\n    });\n}\n\n#[test]\nfn test_search_output_robot_json() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"search\", \"rust\", \"--robot\"], OutputFormat::Json);\n    \n    with_settings!({\n        description =\u003e \"Robot-mode JSON search results\",\n        omit_expression =\u003e true,\n        redactions =\u003e redact_dynamic_fields(),\n    }, {\n        assert_json_snapshot!(\"search_robot_json\", output);\n    });\n}\n\n#[test]\nfn test_show_output_human() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"show\", \"test-skill\"], OutputFormat::Human);\n    \n    with_settings!({\n        description =\u003e \"Human-readable skill details\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"show_human\", output);\n    });\n}\n\n#[test]\nfn test_show_output_robot_json() {\n    let fixture = setup_test_skills();\n    let output = run_command(\u0026[\"show\", \"test-skill\", \"--robot\"], OutputFormat::Json);\n    \n    with_settings!({\n        description =\u003e \"Robot-mode JSON skill details\",\n        omit_expression =\u003e true,\n    }, {\n        assert_json_snapshot!(\"show_robot_json\", output);\n    });\n}\n\n/// Redact dynamic fields like timestamps and UUIDs\nfn redact_dynamic_fields() -\u003e Vec\u003c(\u0026'static str, \u0026'static str)\u003e {\n    vec![\n        (\".timestamp\", \"[TIMESTAMP]\"),\n        (\".id\", \"[UUID]\"),\n        (\".created_at\", \"[TIMESTAMP]\"),\n        (\".updated_at\", \"[TIMESTAMP]\"),\n    ]\n}\n```\n\n### 3. Disclosure Level Snapshots\n\nCreate `tests/snapshots/disclosure_levels.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::disclosure::{disclose, DisclosureLevel};\nuse ms::skill::SkillSpec;\n\nfn create_test_skill() -\u003e SkillSpec {\n    SkillSpec {\n        name: \"rust-error-handling\".to_string(),\n        description: \"Best practices for error handling in Rust\".to_string(),\n        tags: vec![\"rust\".to_string(), \"errors\".to_string(), \"best-practices\".to_string()],\n        content: r#\"\n# Error Handling in Rust\n\n## Overview\nThis skill covers comprehensive error handling patterns in Rust.\n\n## Key Patterns\n\n### Result Type\nUse Result\u003cT, E\u003e for recoverable errors.\n\n### The ? Operator\nPropagate errors elegantly with the ? operator.\n\n### Custom Error Types\nCreate domain-specific error types.\n\n## Examples\n```rust\nfn read_file(path: \u0026str) -\u003e Result\u003cString, std::io::Error\u003e {\n    std::fs::read_to_string(path)\n}\n```\n\n## Context\nThis skill is useful when building robust Rust applications.\n\n## Dependencies\n- rust-basics\n- rust-types\n\"#.to_string(),\n        dependencies: vec![\"rust-basics\".to_string()],\n        ..Default::default()\n    }\n}\n\n#[test]\nfn test_disclosure_minimal() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Minimal);\n    \n    with_settings!({\n        description =\u003e \"Minimal disclosure: name and brief description only\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_minimal\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_overview() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Overview);\n    \n    with_settings!({\n        description =\u003e \"Overview disclosure: name, description, tags, high-level structure\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_overview\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_standard() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Standard);\n    \n    with_settings!({\n        description =\u003e \"Standard disclosure: everything except implementation details\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_standard\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_full() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Full);\n    \n    with_settings!({\n        description =\u003e \"Full disclosure: complete content including code examples\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_full\", output);\n    });\n}\n\n#[test]\nfn test_disclosure_complete() {\n    let skill = create_test_skill();\n    let output = disclose(\u0026skill, DisclosureLevel::Complete);\n    \n    with_settings!({\n        description =\u003e \"Complete disclosure: everything including metadata and provenance\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"disclosure_complete\", output);\n    });\n}\n```\n\n### 4. Error Message Snapshots\n\nCreate `tests/snapshots/error_messages.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::errors::MsError;\n\n#[test]\nfn test_error_skill_not_found() {\n    let error = MsError::SkillNotFound {\n        name: \"nonexistent-skill\".to_string(),\n        suggestions: vec![\"similar-skill\".to_string(), \"other-skill\".to_string()],\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when skill is not found with suggestions\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_skill_not_found\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_skill_not_found_no_suggestions() {\n    let error = MsError::SkillNotFound {\n        name: \"xyz-unknown\".to_string(),\n        suggestions: vec![],\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when skill not found without suggestions\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_skill_not_found_no_suggestions\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_invalid_skill_name() {\n    let error = MsError::InvalidSkillName {\n        name: \"Invalid Skill Name!\".to_string(),\n        reason: \"Skill names must be lowercase with hyphens\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error for invalid skill name format\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_invalid_skill_name\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_config_not_found() {\n    let error = MsError::ConfigNotFound {\n        path: \"/home/user/.config/ms/config.toml\".to_string(),\n        hint: \"Run 'ms init' to create a default configuration\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when config file is missing\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_config_not_found\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_database_error() {\n    let error = MsError::DatabaseError {\n        operation: \"insert skill\".to_string(),\n        details: \"UNIQUE constraint failed: skills.name\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error for database operation failure\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_database_error\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_network_error() {\n    let error = MsError::NetworkError {\n        url: \"https://api.skills.example.com/v1/search\".to_string(),\n        reason: \"Connection timed out\".to_string(),\n        retry_hint: \"Check your internet connection and try again\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error for network operation failure\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_network_error\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_parse_error() {\n    let error = MsError::ParseError {\n        file: \"skills/my-skill/SKILL.md\".to_string(),\n        line: 15,\n        column: 8,\n        message: \"Unexpected token 'invalid'\".to_string(),\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when parsing SKILL.md fails\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_parse_error\", error.to_string());\n    });\n}\n\n#[test]\nfn test_error_dependency_cycle() {\n    let error = MsError::DependencyCycle {\n        skill: \"skill-a\".to_string(),\n        cycle: vec![\"skill-a\".to_string(), \"skill-b\".to_string(), \"skill-c\".to_string(), \"skill-a\".to_string()],\n    };\n    \n    with_settings!({\n        description =\u003e \"Error when dependency cycle is detected\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"error_dependency_cycle\", error.to_string());\n    });\n}\n```\n\n### 5. Diagnostic Output Snapshots\n\nCreate `tests/snapshots/diagnostics.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::diagnostics::{DiagnosticReport, HealthCheck, IndexStats};\n\n#[test]\nfn test_diagnostic_health_all_ok() {\n    let report = DiagnosticReport {\n        checks: vec![\n            HealthCheck { name: \"database\".to_string(), status: \"ok\".to_string(), details: None },\n            HealthCheck { name: \"index\".to_string(), status: \"ok\".to_string(), details: None },\n            HealthCheck { name: \"config\".to_string(), status: \"ok\".to_string(), details: None },\n        ],\n        warnings: vec![],\n        errors: vec![],\n    };\n    \n    with_settings!({\n        description =\u003e \"Health check output when all systems are OK\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_health_all_ok\", report.render());\n    });\n}\n\n#[test]\nfn test_diagnostic_health_with_warnings() {\n    let report = DiagnosticReport {\n        checks: vec![\n            HealthCheck { name: \"database\".to_string(), status: \"ok\".to_string(), details: None },\n            HealthCheck { name: \"index\".to_string(), status: \"warning\".to_string(), \n                         details: Some(\"Index is 3 days old, consider re-indexing\".to_string()) },\n            HealthCheck { name: \"config\".to_string(), status: \"ok\".to_string(), details: None },\n        ],\n        warnings: vec![\"Index may be stale\".to_string()],\n        errors: vec![],\n    };\n    \n    with_settings!({\n        description =\u003e \"Health check output with warnings\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_health_with_warnings\", report.render());\n    });\n}\n\n#[test]\nfn test_diagnostic_health_with_errors() {\n    let report = DiagnosticReport {\n        checks: vec![\n            HealthCheck { name: \"database\".to_string(), status: \"error\".to_string(), \n                         details: Some(\"Database file is corrupted\".to_string()) },\n            HealthCheck { name: \"index\".to_string(), status: \"error\".to_string(), \n                         details: Some(\"Index directory not found\".to_string()) },\n            HealthCheck { name: \"config\".to_string(), status: \"ok\".to_string(), details: None },\n        ],\n        warnings: vec![],\n        errors: vec![\n            \"Database integrity check failed\".to_string(),\n            \"Search functionality unavailable\".to_string(),\n        ],\n    };\n    \n    with_settings!({\n        description =\u003e \"Health check output with errors\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_health_with_errors\", report.render());\n    });\n}\n\n#[test]\nfn test_diagnostic_index_stats() {\n    let stats = IndexStats {\n        total_skills: 150,\n        indexed_skills: 148,\n        pending_skills: 2,\n        index_size_bytes: 1_234_567,\n        last_indexed: \"2024-01-15T10:30:00Z\".to_string(),\n        average_index_time_ms: 45,\n    };\n    \n    with_settings!({\n        description =\u003e \"Index statistics output\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"diagnostic_index_stats\", stats.render());\n    });\n}\n```\n\n### 6. SKILL.md Compilation Snapshots\n\nCreate `tests/snapshots/skill_compilation.rs`:\n\n```rust\nuse insta::{assert_snapshot, with_settings};\nuse ms::compiler::{compile_skill, CompilationOptions};\n\n#[test]\nfn test_skill_compilation_minimal() {\n    let input = r#\"---\nname: minimal-skill\ndescription: A minimal skill for testing\n---\n# Minimal Skill\n\nJust a simple skill.\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for minimal SKILL.md\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_minimal\", output);\n    });\n}\n\n#[test]\nfn test_skill_compilation_with_code() {\n    let input = r#\"---\nname: code-skill\ndescription: Skill with code examples\ntags: [rust, examples]\n---\n# Code Examples\n\n## Rust Example\n```rust\nfn main() {\n    println!(\"Hello, world!\");\n}\n```\n\n## Python Example\n```python\nprint(\"Hello, world!\")\n```\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for skill with code blocks\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_with_code\", output);\n    });\n}\n\n#[test]\nfn test_skill_compilation_with_dependencies() {\n    let input = r#\"---\nname: dependent-skill\ndescription: Skill with dependencies\ndependencies:\n  - rust-basics\n  - error-handling\n---\n# Dependent Skill\n\nThis skill builds on rust-basics and error-handling.\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for skill with dependencies\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_with_dependencies\", output);\n    });\n}\n\n#[test]\nfn test_skill_compilation_full() {\n    let input = r#\"---\nname: complete-skill\ndescription: A complete skill with all sections\ntags: [complete, testing, example]\ndependencies:\n  - prerequisite-skill\ncontext:\n  when: Building production Rust applications\n  why: To ensure robust error handling\n---\n# Complete Skill\n\n## Overview\nThis is a comprehensive skill example.\n\n## Patterns\n\n### Pattern 1: Error Propagation\nUse the ? operator for clean error propagation.\n\n```rust\nfn read_config() -\u003e Result\u003cConfig, Error\u003e {\n    let content = std::fs::read_to_string(\"config.toml\")?;\n    let config: Config = toml::from_str(\u0026content)?;\n    Ok(config)\n}\n```\n\n### Pattern 2: Custom Error Types\nDefine domain-specific errors.\n\n## Examples\n\n### Example 1: Basic Usage\n```rust\nlet result = read_config();\nmatch result {\n    Ok(config) =\u003e println!(\"Loaded: {:?}\", config),\n    Err(e) =\u003e eprintln!(\"Error: {}\", e),\n}\n```\n\n## Caveats\n- Not suitable for performance-critical hot paths\n- Requires Rust 1.0 or later\n\"#;\n    \n    let output = compile_skill(input, CompilationOptions::default());\n    \n    with_settings!({\n        description =\u003e \"Compiled output for complete SKILL.md\",\n        omit_expression =\u003e true,\n    }, {\n        assert_snapshot!(\"compilation_full\", output);\n    });\n}\n```\n\n### 7. Snapshot Test Organization\n\n```\ntests/\n snapshots/\n    mod.rs\n    cli_output.rs\n    disclosure_levels.rs\n    error_messages.rs\n    diagnostics.rs\n    skill_compilation.rs\n snapshots/\n     cli_output/\n         list_human.snap\n         list_robot_json.snap\n         search_human.snap\n         ...\n     disclosure_levels/\n         disclosure_minimal.snap\n         disclosure_overview.snap\n         ...\n     error_messages/\n         error_skill_not_found.snap\n         ...\n```\n\n### 8. CI Integration\n\nAdd to CI pipeline:\n```yaml\nsnapshot-tests:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: actions/checkout@v4\n    \n    - name: Run snapshot tests\n      run: cargo test --test snapshots\n    \n    - name: Check for uncommitted snapshot changes\n      run: |\n        if [ -n \"$(git status --porcelain tests/snapshots/)\" ]; then\n          echo \"Snapshot files have changed! Review and commit the changes.\"\n          git diff tests/snapshots/\n          exit 1\n        fi\n```\n\n### 9. Snapshot Review Workflow\n\n```bash\n# Run tests and review new/changed snapshots\ncargo insta test\n\n# Review pending snapshots interactively\ncargo insta review\n\n# Accept all pending snapshots\ncargo insta accept\n\n# Reject all pending snapshots\ncargo insta reject\n```\n\n## Acceptance Criteria\n\n1. [ ] insta crate configured with YAML/JSON support\n2. [ ] CLI output snapshots for human and robot modes\n3. [ ] Disclosure level snapshots (minimal, overview, standard, full, complete)\n4. [ ] Error message snapshots for all error types\n5. [ ] Diagnostic output snapshots\n6. [ ] SKILL.md compilation snapshots\n7. [ ] Snapshot tests organized by category\n8. [ ] CI integration to detect uncommitted changes\n9. [ ] Documentation for snapshot review workflow\n10. [ ] Redactions configured for dynamic fields\n\n## Dependencies\n\n- meta_skill-vqr (Robot Mode Infrastructure) - provides output formatting","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-13T22:58:48.418427242-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:59:12.911205318-05:00","labels":["output","snapshots","testing"],"dependencies":[{"issue_id":"meta_skill-wnk","depends_on_id":"meta_skill-vqr","type":"blocks","created_at":"2026-01-13T22:58:53.133050445-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-wnk","depends_on_id":"meta_skill-9ok","type":"parent-child","created_at":"2026-01-13T22:59:18.232202773-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-wwx","title":"Beads Viewer (bv) Integration for Skill Graph Analysis","description":"# Beads Viewer (bv) Integration for Skill Graph Analysis\n\n## Overview\n\nIntegrate beads_viewer (bv) as the graph analysis engine for ms skill dependency visualization, bottleneck detection, and execution planning. Rather than implementing graph algorithms from scratch, ms leverages bv's battle-tested, SIMD-optimized graph analysis with 9 pre-computed metrics.\n\n**Location**: `/data/projects/beads_viewer`\n**Documentation**: `/data/projects/beads_viewer/README.md`\n\n## Why bv (not custom implementation)\n\n| Aspect | Custom Implementation | bv Integration |\n|--------|----------------------|----------------|\n| **Maturity** | New, untested | Production-ready, well-tested |\n| **Performance** | Unknown | Two-phase async (instant + 500ms timeout) |\n| **Metrics** | Must build each one | 9 pre-computed (PageRank, betweenness, HITS, etc.) |\n| **AI Interface** | Must design | Robot protocol ready (--robot-* flags) |\n| **Caching** | Build from scratch | Hash-based caching built-in |\n| **Cycle Detection** | Implement Tarjan's | Tarjan's variant included |\n\n## Use Cases for ms\n\n### 1. Skill Dependency Graph Analysis\nSkills can depend on other skills (e.g., \"docker-compose skill\" depends on \"docker skill\"). bv can:\n- Identify \"keystone\" skills (high PageRank) that are foundational\n- Detect cycles in skill dependencies (invalid configurations)\n- Find bottleneck skills that block many others\n- Compute execution order via topological sort\n\n### 2. Skill Pack Optimization\nWhen packing skills for token-limited contexts:\n- Use PageRank to prioritize foundational skills\n- Use HITS to identify Hubs (composite skills) vs Authorities (utility skills)\n- Use critical path to ensure dependencies are included\n\n### 3. Skill Suggestion Prioritization\nWhen suggesting skills to users:\n- High PageRank skills are more universally useful\n- Low out-degree skills have fewer prerequisites\n- Eigenvector centrality identifies strategically important skills\n\n### 4. Skill Collection Health\nUse bv's --robot-label-health to assess skill collection health:\n- Staleness detection (skills that haven't been updated)\n- Blocked skill chains\n- Velocity scoring\n\n## Architecture\n\n```rust\n/// bv-based skill graph analyzer\nstruct BvSkillAnalyzer {\n    /// Path to bv binary\n    bv_path: PathBuf,\n    /// Temporary JSONL file for skill graph\n    temp_graph_path: PathBuf,\n    /// Cache for computed metrics\n    metrics_cache: HashMap\u003cString, SkillMetrics\u003e,\n}\n\nimpl BvSkillAnalyzer {\n    /// Convert skill collection to beads.jsonl format for bv analysis\n    fn skills_to_beads_jsonl(\u0026self, skills: \u0026[Skill]) -\u003e Result\u003cPathBuf\u003e {\n        // Each skill becomes a \"bead\" with:\n        // - id: skill hash\n        // - title: skill name\n        // - dependencies: skill prerequisites\n        // - labels: skill domains (e.g., docker, k8s, git)\n        // - priority: skill confidence score\n    }\n\n    /// Run bv --robot-insights on skill graph\n    fn analyze_insights(\u0026self) -\u003e Result\u003cBvInsights\u003e {\n        // Call: bv --robot-insights --path \u003ctemp_graph\u003e\n        // Parse JSON response\n    }\n\n    /// Run bv --robot-plan for skill execution order\n    fn compute_execution_plan(\u0026self) -\u003e Result\u003cVec\u003cSkillId\u003e\u003e {\n        // Call: bv --robot-plan\n        // Extract topological order\n    }\n\n    /// Run bv --robot-triage for prioritized skill suggestions\n    fn triage_skills(\u0026self) -\u003e Result\u003cSkillTriage\u003e {\n        // Call: bv --robot-triage\n        // Map recommendations to skills\n    }\n\n    /// Check for cycles in skill dependencies\n    fn detect_cycles(\u0026self) -\u003e Result\u003cVec\u003cVec\u003cSkillId\u003e\u003e\u003e {\n        // Extract cycles from insights\n        // These indicate invalid skill configurations\n    }\n\n    /// Get keystone skills (high PageRank)\n    fn keystone_skills(\u0026self, limit: usize) -\u003e Result\u003cVec\u003c(SkillId, f64)\u003e\u003e {\n        // High PageRank = foundational skills\n    }\n\n    /// Get bottleneck skills (high betweenness)\n    fn bottleneck_skills(\u0026self, limit: usize) -\u003e Result\u003cVec\u003c(SkillId, f64)\u003e\u003e {\n        // High betweenness = gateway skills\n    }\n}\n```\n\n## CLI Commands\n\n```bash\n# Analyze skill graph\nms graph insights          # Full graph analysis (PageRank, betweenness, cycles)\nms graph insights --json   # JSON output for programmatic use\n\n# Find keystone skills\nms graph keystones         # Top foundational skills\nms graph keystones --limit 10\n\n# Find bottleneck skills\nms graph bottlenecks       # Skills that gate many others\n\n# Check for cycles\nms graph cycles            # Detect circular dependencies\nms graph cycles --fix      # Suggest cycle resolution\n\n# Execution planning\nms graph plan              # Optimal skill loading order\nms graph plan --for \"deploy to k8s\"  # Plan for specific task\n\n# Health check\nms graph health            # Overall skill collection health\nms graph health --by-domain  # Per-domain health\n```\n\n## Implementation Details\n\n### Skill-to-Bead Mapping\n\n```rust\n/// Convert a Skill to bv-compatible bead format\nfn skill_to_bead(skill: \u0026Skill) -\u003e serde_json::Value {\n    json!({\n        \"id\": skill.hash(),\n        \"title\": skill.name(),\n        \"description\": skill.description(),\n        \"status\": if skill.is_active() { \"open\" } else { \"closed\" },\n        \"priority\": skill.confidence_to_priority(), // P0-P4\n        \"labels\": skill.domains(),\n        \"dependencies\": skill.prerequisites().map(|p| p.hash()).collect::\u003cVec\u003c_\u003e\u003e(),\n        \"created_at\": skill.created_at(),\n        \"updated_at\": skill.updated_at(),\n    })\n}\n\n/// Priority mapping from confidence\nfn confidence_to_priority(confidence: f64) -\u003e u8 {\n    match confidence {\n        c if c \u003e= 0.9 =\u003e 0,  // P0 - Critical (very high confidence)\n        c if c \u003e= 0.7 =\u003e 1,  // P1 - High\n        c if c \u003e= 0.5 =\u003e 2,  // P2 - Medium\n        c if c \u003e= 0.3 =\u003e 3,  // P3 - Low\n        _ =\u003e 4,              // P4 - Backlog\n    }\n}\n```\n\n### Robot Output Parsing\n\n```rust\n/// Parse bv --robot-insights output\n#[derive(Deserialize)]\nstruct BvInsights {\n    bottlenecks: Vec\u003cBvMetric\u003e,\n    keystones: Vec\u003cBvMetric\u003e,\n    influencers: Vec\u003cBvMetric\u003e,\n    hubs: Vec\u003cBvMetric\u003e,\n    authorities: Vec\u003cBvMetric\u003e,\n    cycles: Vec\u003cVec\u003cString\u003e\u003e,\n    cluster_density: f64,\n    status: BvStatus,\n    data_hash: String,\n}\n\n#[derive(Deserialize)]\nstruct BvMetric {\n    id: String,\n    value: f64,\n}\n\n#[derive(Deserialize)]\nstruct BvStatus {\n    page_rank: MetricStatus,\n    betweenness: MetricStatus,\n    hits: MetricStatus,\n    eigenvector: MetricStatus,\n    critical_path: MetricStatus,\n    cycles: MetricStatus,\n}\n\n#[derive(Deserialize)]\nenum MetricStatus {\n    Computed { elapsed_ms: u64 },\n    Approx { elapsed_ms: u64, sample_size: usize },\n    Timeout { elapsed_ms: u64 },\n    Skipped { reason: String },\n}\n```\n\n## Tasks\n\n1. [ ] Detect bv installation and version\n2. [ ] Implement skill-to-bead JSONL conversion\n3. [ ] Implement BvSkillAnalyzer wrapper\n4. [ ] Parse all --robot-* outputs (insights, plan, triage)\n5. [ ] Build ms graph CLI commands\n6. [ ] Integrate with skill packer for priority ordering\n7. [ ] Integrate with skill suggester for recommendations\n8. [ ] Add cycle detection with resolution suggestions\n9. [ ] Implement metrics caching (use bv's data_hash)\n10. [ ] Handle bv unavailable case (graceful degradation)\n\n## Testing Requirements\n\n- bv integration tests (JSON parsing, command invocation)\n- Skill-to-bead conversion accuracy\n- Cycle detection correctness\n- Metrics caching validity\n- Graceful degradation when bv unavailable\n- Performance benchmarks for large skill graphs\n\n## Acceptance Criteria\n\n- bv detected and integrated\n- Skill graphs converted to beads.jsonl format\n- All 9 metrics available for skill analysis\n- Cycles detected and reported with suggestions\n- ms graph CLI commands functional\n- Graceful fallback when bv not installed\n- Metrics cached for performance\n\n## Dependencies\n\n- Phase 4 foundation (skill storage, basic operations)\n- Skill dependency tracking must be in place\n- Optional: Skill packer integration for ordering optimization\n\n## References\n\n- bv repository: /data/projects/beads_viewer\n- bv README: /data/projects/beads_viewer/README.md\n- bv robot protocol documentation (in README)\n- Plan Section 5.x (graph analysis integration)\n\nLabels: [phase-4 integration graph-analysis bv skill-deps]","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:12:10.179611368-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:12:10.179611368-05:00","labels":["bv","graph-analysis","integration","phase-4"],"dependencies":[{"issue_id":"meta_skill-wwx","depends_on_id":"meta_skill-jka","type":"blocks","created_at":"2026-01-13T23:12:25.776356588-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-wwx","depends_on_id":"meta_skill-qs1","type":"blocks","created_at":"2026-01-13T23:12:25.808335935-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-x7k","title":"Skill Tests (ms test)","description":"# Skill Tests (ms test)\n\n**Phase 6 - Section 18.9**\n\nSkills include executable tests to validate correctness. Tests are stored under `tests/` in each skill directory and run via `ms test`. This ensures skills remain accurate and functional as they evolve.\n\n---\n\n## Overview\n\nSkills can become outdated or contain errors. Skill tests provide:\n\n1. **Validation**: Verify skill content is accurate and commands work\n2. **Regression Prevention**: Catch breaks when skills are updated\n3. **Quality Assurance**: Ensure skills meet quality standards before publishing\n4. **CI Integration**: Run tests in continuous integration pipelines\n\n---\n\n## Test File Format\n\nTests are written in YAML for readability and stored in `\u003cskill\u003e/tests/`:\n\n### Basic Test Structure\n\n```yaml\n# \u003cskill\u003e/tests/basic_load.yaml\nname: \"Skill loads correctly\"\ndescription: \"Verify the skill can be loaded and parsed\"\nskill: rust-error-handling\n\nsetup:\n  # Optional setup steps\n  - mkdir: { path: \"/tmp/test-workspace\" }\n  - write_file: \n      path: \"/tmp/test-workspace/test.rs\"\n      content: |\n        fn main() {\n            println!(\"test\");\n        }\n\nsteps:\n  - load_skill:\n      level: standard\n      \n  - assert:\n      skill_loaded: true\n      sections_present:\n        - overview\n        - error-types\n        - best-practices\n\n  - run:\n      cmd: \"rustc --version\"\n      \n  - assert:\n      exit_code: 0\n      stdout_contains: \"rustc\"\n\ncleanup:\n  - remove: { path: \"/tmp/test-workspace\" }\n\ntimeout: 30s\ntags: [smoke, load]\n```\n\n### Test Schema\n\n```yaml\n# Test file schema\nname: string                    # Test name (required)\ndescription: string             # What this test validates (optional)\nskill: string                   # Skill ID to test (required)\n\nsetup: Step[]                   # Setup steps (optional)\nsteps: Step[]                   # Test steps (required)\ncleanup: Step[]                 # Cleanup steps (optional)\n\ntimeout: duration               # Test timeout (default: 60s)\ntags: string[]                  # Tags for filtering\nskip_if: Condition[]            # Conditions to skip test\nrequires: Requirement[]         # System requirements\n```\n\n### Step Types\n\n```yaml\n# Load a skill\n- load_skill:\n    level: minimal | standard | comprehensive | full\n    budget: 2000                # Optional token budget\n    context:                    # Optional suggestion context\n      file_types: [\".rs\"]\n      recent_errors: [\"E0382\"]\n\n# Run a command\n- run:\n    cmd: \"cargo build\"\n    cwd: \"/tmp/workspace\"       # Working directory\n    env:                        # Environment variables\n      RUST_BACKTRACE: \"1\"\n    stdin: \"input text\"         # Optional stdin\n    timeout: 10s                # Command timeout\n\n# Assert conditions\n- assert:\n    exit_code: 0\n    stdout_contains: \"Success\"\n    stdout_not_contains: \"error\"\n    stderr_empty: true\n    file_exists: \"/tmp/output.txt\"\n    file_contains:\n      path: \"/tmp/output.txt\"\n      text: \"expected content\"\n    skill_loaded: true\n    sections_present: [\"overview\", \"examples\"]\n    tokens_used_lt: 2000\n    retrieval_rank_le: 3\n\n# Write a file\n- write_file:\n    path: \"/tmp/test.rs\"\n    content: |\n      fn main() {}\n\n# Create directory\n- mkdir:\n    path: \"/tmp/workspace\"\n    parents: true               # Like mkdir -p\n\n# Remove file/directory\n- remove:\n    path: \"/tmp/workspace\"\n    recursive: true\n\n# Copy file\n- copy:\n    from: \"fixtures/input.rs\"\n    to: \"/tmp/workspace/input.rs\"\n\n# Sleep (for async operations)\n- sleep:\n    duration: 1s\n\n# Set variable for later use\n- set:\n    name: \"output_path\"\n    value: \"/tmp/result.txt\"\n\n# Use variable\n- run:\n    cmd: \"cat ${output_path}\"\n\n# Conditional execution\n- if:\n    condition:\n      platform: linux\n    then:\n      - run: { cmd: \"ls -la\" }\n    else:\n      - run: { cmd: \"dir\" }\n```\n\n---\n\n## Extended Test Types\n\n### Retrieval Tests\n\nTest that skills are retrieved correctly for given queries:\n\n```yaml\n# \u003cskill\u003e/tests/retrieval_test.yaml\nname: \"Retrieval for error handling query\"\ntype: retrieval\nskill: rust-error-handling\n\nquery: \"How do I handle errors in Rust?\"\ncontext:\n  file_types: [\".rs\"]\n  project_type: \"rust\"\n\nexpect:\n  - skill: rust-error-handling\n    rank_le: 2                  # Should be in top 2 results\n    sections_include:\n      - error-types\n      - best-practices\n      \n  - skill: rust-result-option   # Related skill should also appear\n    rank_le: 5\n```\n\n```rust\n/// Retrieval test definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RetrievalTest {\n    /// Test name\n    pub name: String,\n    \n    /// Skill being tested\n    pub skill: String,\n    \n    /// Search query\n    pub query: String,\n    \n    /// Suggestion context\n    pub context: SuggestionContext,\n    \n    /// Expected results\n    pub expect: Vec\u003cExpectedResult\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExpectedResult {\n    /// Expected skill in results\n    pub skill: String,\n    \n    /// Maximum acceptable rank (1 = top result)\n    pub rank_le: Option\u003cu32\u003e,\n    \n    /// Minimum acceptable rank\n    pub rank_ge: Option\u003cu32\u003e,\n    \n    /// Sections that should be included\n    pub sections_include: Vec\u003cString\u003e,\n    \n    /// Minimum relevance score\n    pub score_ge: Option\u003cf64\u003e,\n}\n\nimpl RetrievalTest {\n    /// Run the retrieval test\n    pub fn run(\u0026self, searcher: \u0026HybridSearcher) -\u003e Result\u003cRetrievalTestResult, TestError\u003e {\n        // Perform search\n        let results = searcher.search(\u0026self.query, 10)?;\n        \n        let mut passed = true;\n        let mut failures = Vec::new();\n        \n        for expected in \u0026self.expect {\n            // Find the skill in results\n            let position = results.iter().position(|r| r.skill.id.0 == expected.skill);\n            \n            match position {\n                Some(pos) =\u003e {\n                    let rank = pos + 1; // 1-indexed\n                    \n                    // Check rank constraints\n                    if let Some(max_rank) = expected.rank_le {\n                        if rank \u003e max_rank as usize {\n                            passed = false;\n                            failures.push(format!(\n                                \"Skill '{}' at rank {} but expected \u003c= {}\",\n                                expected.skill, rank, max_rank\n                            ));\n                        }\n                    }\n                    \n                    if let Some(min_rank) = expected.rank_ge {\n                        if rank \u003c min_rank as usize {\n                            passed = false;\n                            failures.push(format!(\n                                \"Skill '{}' at rank {} but expected \u003e= {}\",\n                                expected.skill, rank, min_rank\n                            ));\n                        }\n                    }\n                    \n                    // Check sections\n                    let result = \u0026results[pos];\n                    for section in \u0026expected.sections_include {\n                        if !result.skill.sections.contains_key(section) {\n                            passed = false;\n                            failures.push(format!(\n                                \"Skill '{}' missing expected section '{}'\",\n                                expected.skill, section\n                            ));\n                        }\n                    }\n                }\n                None =\u003e {\n                    passed = false;\n                    failures.push(format!(\n                        \"Skill '{}' not found in top 10 results\",\n                        expected.skill\n                    ));\n                }\n            }\n        }\n        \n        Ok(RetrievalTestResult {\n            test_name: self.name.clone(),\n            passed,\n            failures,\n            actual_results: results.iter()\n                .map(|r| (r.skill.id.0.clone(), r.score))\n                .collect(),\n        })\n    }\n}\n```\n\n### Packing Tests\n\nTest that skills pack efficiently within token budgets:\n\n```yaml\n# \u003cskill\u003e/tests/packing_test.yaml\nname: \"Efficient packing under budget\"\ntype: packing\nskill: rust-error-handling\n\nbudget: 2000\ncontract:\n  must_include:\n    - overview\n    - error-types/result\n  should_include:\n    - best-practices\n  nice_to_have:\n    - examples\n\nexpect:\n  tokens_used_le: 1800          # Should use less than budget\n  must_sections_present: true   # All must_include sections present\n  should_sections_percent_ge: 80  # At least 80% of should_include\n```\n\n```rust\n/// Packing test definition\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PackingTest {\n    /// Test name\n    pub name: String,\n    \n    /// Skill being tested\n    pub skill: String,\n    \n    /// Token budget\n    pub budget: usize,\n    \n    /// Pack contract\n    pub contract: PackContract,\n    \n    /// Expected outcomes\n    pub expect: PackExpectation,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PackContract {\n    /// Sections that MUST be included\n    pub must_include: Vec\u003cString\u003e,\n    \n    /// Sections that SHOULD be included if space allows\n    pub should_include: Vec\u003cString\u003e,\n    \n    /// Sections that are nice to have\n    pub nice_to_have: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct PackExpectation {\n    /// Maximum tokens used\n    pub tokens_used_le: Option\u003cusize\u003e,\n    \n    /// Minimum tokens used (test isn't leaving budget on table)\n    pub tokens_used_ge: Option\u003cusize\u003e,\n    \n    /// All must_include sections present\n    pub must_sections_present: bool,\n    \n    /// Minimum percentage of should_include sections\n    pub should_sections_percent_ge: Option\u003cf64\u003e,\n    \n    /// Content quality score\n    pub quality_score_ge: Option\u003cf64\u003e,\n}\n\nimpl PackingTest {\n    /// Run the packing test\n    pub fn run(\u0026self, packer: \u0026SkillPacker, skill: \u0026Skill) -\u003e Result\u003cPackingTestResult, TestError\u003e {\n        // Pack the skill\n        let packed = packer.pack(skill, self.budget, \u0026self.contract)?;\n        \n        let mut passed = true;\n        let mut failures = Vec::new();\n        \n        // Check token budget\n        if let Some(max_tokens) = self.expect.tokens_used_le {\n            if packed.tokens_used \u003e max_tokens {\n                passed = false;\n                failures.push(format!(\n                    \"Used {} tokens but expected \u003c= {}\",\n                    packed.tokens_used, max_tokens\n                ));\n            }\n        }\n        \n        if let Some(min_tokens) = self.expect.tokens_used_ge {\n            if packed.tokens_used \u003c min_tokens {\n                passed = false;\n                failures.push(format!(\n                    \"Used {} tokens but expected \u003e= {} (underutilizing budget)\",\n                    packed.tokens_used, min_tokens\n                ));\n            }\n        }\n        \n        // Check must_include sections\n        if self.expect.must_sections_present {\n            for section in \u0026self.contract.must_include {\n                if !packed.sections_included.contains(section) {\n                    passed = false;\n                    failures.push(format!(\n                        \"Must-include section '{}' not present\",\n                        section\n                    ));\n                }\n            }\n        }\n        \n        // Check should_include percentage\n        if let Some(min_percent) = self.expect.should_sections_percent_ge {\n            let included_count = self.contract.should_include.iter()\n                .filter(|s| packed.sections_included.contains(*s))\n                .count();\n            let percent = (included_count as f64 / self.contract.should_include.len() as f64) * 100.0;\n            \n            if percent \u003c min_percent {\n                passed = false;\n                failures.push(format!(\n                    \"Only {:.1}% of should_include sections present, expected \u003e= {:.1}%\",\n                    percent, min_percent\n                ));\n            }\n        }\n        \n        Ok(PackingTestResult {\n            test_name: self.name.clone(),\n            passed,\n            failures,\n            tokens_used: packed.tokens_used,\n            sections_included: packed.sections_included,\n        })\n    }\n}\n```\n\n---\n\n## Core Data Structures\n\n### Skill Test Harness\n\n```rust\nuse std::path::PathBuf;\nuse std::collections::HashMap;\nuse std::time::{Duration, Instant};\n\n/// Test execution harness\npub struct SkillTestHarness {\n    /// Skill registry for loading skills\n    skill_registry: Registry,\n    \n    /// Temporary workspace for test execution\n    temp_workspace: PathBuf,\n    \n    /// Environment variables for tests\n    env: HashMap\u003cString, String\u003e,\n    \n    /// Test timeout\n    default_timeout: Duration,\n    \n    /// Searcher for retrieval tests\n    searcher: Option\u003cHybridSearcher\u003e,\n    \n    /// Packer for packing tests\n    packer: Option\u003cSkillPacker\u003e,\n}\n\nimpl SkillTestHarness {\n    pub fn new(skill_registry: Registry) -\u003e Result\u003cSelf, TestError\u003e {\n        let temp_workspace = tempfile::tempdir()?.into_path();\n        \n        Ok(Self {\n            skill_registry,\n            temp_workspace,\n            env: HashMap::new(),\n            default_timeout: Duration::from_secs(60),\n            searcher: None,\n            packer: None,\n        })\n    }\n    \n    /// Run all tests for a skill\n    pub fn run_skill_tests(\u0026self, skill_id: \u0026str) -\u003e Result\u003cSkillTestReport, TestError\u003e {\n        let skill = self.skill_registry.get(\u0026SkillId(skill_id.to_string()))?;\n        let test_dir = skill.path.join(\"tests\");\n        \n        if !test_dir.exists() {\n            return Ok(SkillTestReport {\n                skill_id: skill_id.to_string(),\n                tests_run: 0,\n                passed: 0,\n                failed: 0,\n                skipped: 0,\n                results: Vec::new(),\n                duration: Duration::ZERO,\n            });\n        }\n        \n        let mut report = SkillTestReport::new(skill_id);\n        let start = Instant::now();\n        \n        // Find all test files\n        for entry in std::fs::read_dir(\u0026test_dir)? {\n            let entry = entry?;\n            let path = entry.path();\n            \n            if path.extension().map(|e| e == \"yaml\" || e == \"yml\").unwrap_or(false) {\n                let result = self.run_test_file(\u0026path)?;\n                report.add_result(result);\n            }\n        }\n        \n        report.duration = start.elapsed();\n        Ok(report)\n    }\n    \n    /// Run a single test file\n    pub fn run_test_file(\u0026self, path: \u0026Path) -\u003e Result\u003cTestResult, TestError\u003e {\n        let content = std::fs::read_to_string(path)?;\n        let test: TestDefinition = serde_yaml::from_str(\u0026content)?;\n        \n        // Check skip conditions\n        if self.should_skip(\u0026test) {\n            return Ok(TestResult {\n                name: test.name,\n                status: TestStatus::Skipped,\n                duration: Duration::ZERO,\n                output: None,\n                failures: Vec::new(),\n            });\n        }\n        \n        // Check requirements\n        if let Some(missing) = self.check_requirements(\u0026test) {\n            return Ok(TestResult {\n                name: test.name,\n                status: TestStatus::Skipped,\n                duration: Duration::ZERO,\n                output: Some(format!(\"Missing requirement: {}\", missing)),\n                failures: Vec::new(),\n            });\n        }\n        \n        // Dispatch based on test type\n        match test.test_type.as_deref() {\n            Some(\"retrieval\") =\u003e self.run_retrieval_test(\u0026test),\n            Some(\"packing\") =\u003e self.run_packing_test(\u0026test),\n            _ =\u003e self.run_standard_test(\u0026test),\n        }\n    }\n    \n    /// Run a standard test\n    fn run_standard_test(\u0026self, test: \u0026TestDefinition) -\u003e Result\u003cTestResult, TestError\u003e {\n        let start = Instant::now();\n        let mut context = TestContext::new(\u0026self.temp_workspace, \u0026self.env);\n        let mut failures = Vec::new();\n        \n        // Run setup\n        if let Some(setup) = \u0026test.setup {\n            for step in setup {\n                if let Err(e) = self.execute_step(step, \u0026mut context) {\n                    return Ok(TestResult {\n                        name: test.name.clone(),\n                        status: TestStatus::Failed,\n                        duration: start.elapsed(),\n                        output: Some(format!(\"Setup failed: {}\", e)),\n                        failures: vec![format!(\"Setup: {}\", e)],\n                    });\n                }\n            }\n        }\n        \n        // Run test steps\n        for step in \u0026test.steps {\n            match self.execute_step(step, \u0026mut context) {\n                Ok(()) =\u003e {}\n                Err(e) =\u003e {\n                    failures.push(e.to_string());\n                }\n            }\n        }\n        \n        // Run cleanup (always, even if test failed)\n        if let Some(cleanup) = \u0026test.cleanup {\n            for step in cleanup {\n                let _ = self.execute_step(step, \u0026mut context);\n            }\n        }\n        \n        let status = if failures.is_empty() {\n            TestStatus::Passed\n        } else {\n            TestStatus::Failed\n        };\n        \n        Ok(TestResult {\n            name: test.name.clone(),\n            status,\n            duration: start.elapsed(),\n            output: context.last_output.clone(),\n            failures,\n        })\n    }\n    \n    /// Execute a single test step\n    fn execute_step(\u0026self, step: \u0026TestStep, context: \u0026mut TestContext) -\u003e Result\u003c(), TestError\u003e {\n        match step {\n            TestStep::LoadSkill { level, budget, .. } =\u003e {\n                let skill = self.skill_registry.get(\u0026SkillId(context.skill_id.clone()))?;\n                context.loaded_skill = Some(skill);\n                context.skill_loaded = true;\n                Ok(())\n            }\n            \n            TestStep::Run { cmd, cwd, env, timeout, .. } =\u003e {\n                let working_dir = cwd.as_ref()\n                    .map(PathBuf::from)\n                    .unwrap_or_else(|| context.workspace.clone());\n                \n                let timeout = timeout.unwrap_or(Duration::from_secs(30));\n                \n                let mut command = std::process::Command::new(\"sh\");\n                command.arg(\"-c\").arg(cmd);\n                command.current_dir(\u0026working_dir);\n                \n                // Set environment\n                for (k, v) in \u0026context.env {\n                    command.env(k, v);\n                }\n                if let Some(env) = env {\n                    for (k, v) in env {\n                        command.env(k, v);\n                    }\n                }\n                \n                let output = command.output()?;\n                \n                context.last_exit_code = Some(output.status.code().unwrap_or(-1));\n                context.last_stdout = Some(String::from_utf8_lossy(\u0026output.stdout).to_string());\n                context.last_stderr = Some(String::from_utf8_lossy(\u0026output.stderr).to_string());\n                context.last_output = context.last_stdout.clone();\n                \n                Ok(())\n            }\n            \n            TestStep::Assert(assertions) =\u003e {\n                self.check_assertions(assertions, context)\n            }\n            \n            TestStep::WriteFile { path, content } =\u003e {\n                let path = self.expand_path(path, context);\n                if let Some(parent) = path.parent() {\n                    std::fs::create_dir_all(parent)?;\n                }\n                std::fs::write(\u0026path, content)?;\n                Ok(())\n            }\n            \n            TestStep::Mkdir { path, parents } =\u003e {\n                let path = self.expand_path(path, context);\n                if *parents {\n                    std::fs::create_dir_all(\u0026path)?;\n                } else {\n                    std::fs::create_dir(\u0026path)?;\n                }\n                Ok(())\n            }\n            \n            TestStep::Remove { path, recursive } =\u003e {\n                let path = self.expand_path(path, context);\n                if path.is_dir() \u0026\u0026 *recursive {\n                    std::fs::remove_dir_all(\u0026path)?;\n                } else if path.is_dir() {\n                    std::fs::remove_dir(\u0026path)?;\n                } else {\n                    std::fs::remove_file(\u0026path)?;\n                }\n                Ok(())\n            }\n            \n            TestStep::Copy { from, to } =\u003e {\n                let from_path = self.expand_path(from, context);\n                let to_path = self.expand_path(to, context);\n                std::fs::copy(\u0026from_path, \u0026to_path)?;\n                Ok(())\n            }\n            \n            TestStep::Sleep { duration } =\u003e {\n                std::thread::sleep(*duration);\n                Ok(())\n            }\n            \n            TestStep::Set { name, value } =\u003e {\n                context.variables.insert(name.clone(), value.clone());\n                Ok(())\n            }\n            \n            TestStep::If { condition, then_steps, else_steps } =\u003e {\n                if self.evaluate_condition(condition, context) {\n                    for step in then_steps {\n                        self.execute_step(step, context)?;\n                    }\n                } else if let Some(else_steps) = else_steps {\n                    for step in else_steps {\n                        self.execute_step(step, context)?;\n                    }\n                }\n                Ok(())\n            }\n        }\n    }\n    \n    /// Check assertions\n    fn check_assertions(\u0026self, assertions: \u0026Assertions, context: \u0026TestContext) -\u003e Result\u003c(), TestError\u003e {\n        if let Some(expected_code) = assertions.exit_code {\n            if let Some(actual_code) = context.last_exit_code {\n                if actual_code != expected_code {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"Exit code: expected {}, got {}\",\n                        expected_code, actual_code\n                    )));\n                }\n            }\n        }\n        \n        if let Some(pattern) = \u0026assertions.stdout_contains {\n            if let Some(stdout) = \u0026context.last_stdout {\n                if !stdout.contains(pattern) {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"stdout does not contain '{}'\",\n                        pattern\n                    )));\n                }\n            }\n        }\n        \n        if let Some(pattern) = \u0026assertions.stdout_not_contains {\n            if let Some(stdout) = \u0026context.last_stdout {\n                if stdout.contains(pattern) {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"stdout contains '{}' but should not\",\n                        pattern\n                    )));\n                }\n            }\n        }\n        \n        if assertions.stderr_empty == Some(true) {\n            if let Some(stderr) = \u0026context.last_stderr {\n                if !stderr.trim().is_empty() {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"stderr not empty: {}\",\n                        stderr\n                    )));\n                }\n            }\n        }\n        \n        if let Some(path) = \u0026assertions.file_exists {\n            let path = self.expand_path(path, context);\n            if !path.exists() {\n                return Err(TestError::AssertionFailed(format!(\n                    \"File does not exist: {}\",\n                    path.display()\n                )));\n            }\n        }\n        \n        if assertions.skill_loaded == Some(true) \u0026\u0026 !context.skill_loaded {\n            return Err(TestError::AssertionFailed(\n                \"Skill not loaded\".to_string()\n            ));\n        }\n        \n        if let Some(sections) = \u0026assertions.sections_present {\n            if let Some(skill) = \u0026context.loaded_skill {\n                for section in sections {\n                    if !skill.sections.contains_key(section) {\n                        return Err(TestError::AssertionFailed(format!(\n                            \"Section '{}' not present in skill\",\n                            section\n                        )));\n                    }\n                }\n            }\n        }\n        \n        if let Some(max_tokens) = assertions.tokens_used_lt {\n            if let Some(tokens) = context.tokens_used {\n                if tokens \u003e= max_tokens {\n                    return Err(TestError::AssertionFailed(format!(\n                        \"Tokens used ({}) \u003e= limit ({})\",\n                        tokens, max_tokens\n                    )));\n                }\n            }\n        }\n        \n        Ok(())\n    }\n}\n\n/// Test execution context\npub struct TestContext {\n    pub workspace: PathBuf,\n    pub skill_id: String,\n    pub env: HashMap\u003cString, String\u003e,\n    pub variables: HashMap\u003cString, String\u003e,\n    pub loaded_skill: Option\u003cSkill\u003e,\n    pub skill_loaded: bool,\n    pub tokens_used: Option\u003cusize\u003e,\n    pub last_exit_code: Option\u003ci32\u003e,\n    pub last_stdout: Option\u003cString\u003e,\n    pub last_stderr: Option\u003cString\u003e,\n    pub last_output: Option\u003cString\u003e,\n}\n\n/// Test result\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct TestResult {\n    pub name: String,\n    pub status: TestStatus,\n    pub duration: Duration,\n    pub output: Option\u003cString\u003e,\n    pub failures: Vec\u003cString\u003e,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\npub enum TestStatus {\n    Passed,\n    Failed,\n    Skipped,\n    Timeout,\n}\n\n/// Aggregate report for a skill's tests\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct SkillTestReport {\n    pub skill_id: String,\n    pub tests_run: usize,\n    pub passed: usize,\n    pub failed: usize,\n    pub skipped: usize,\n    pub results: Vec\u003cTestResult\u003e,\n    pub duration: Duration,\n}\n\nimpl SkillTestReport {\n    pub fn new(skill_id: \u0026str) -\u003e Self {\n        Self {\n            skill_id: skill_id.to_string(),\n            tests_run: 0,\n            passed: 0,\n            failed: 0,\n            skipped: 0,\n            results: Vec::new(),\n            duration: Duration::ZERO,\n        }\n    }\n    \n    pub fn add_result(\u0026mut self, result: TestResult) {\n        self.tests_run += 1;\n        match result.status {\n            TestStatus::Passed =\u003e self.passed += 1,\n            TestStatus::Failed =\u003e self.failed += 1,\n            TestStatus::Skipped =\u003e self.skipped += 1,\n            TestStatus::Timeout =\u003e self.failed += 1,\n        }\n        self.results.push(result);\n    }\n    \n    pub fn success(\u0026self) -\u003e bool {\n        self.failed == 0\n    }\n}\n```\n\n---\n\n## CLI Commands\n\n### `ms test \u003cskill\u003e`\n\n```\nRun tests for a skill\n\nUSAGE:\n    ms test \u003cSKILL\u003e [OPTIONS]\n\nARGS:\n    \u003cSKILL\u003e    Skill ID or name\n\nOPTIONS:\n    --test \u003cNAME\u003e       Run specific test by name\n    --tags \u003cTAGS\u003e       Only run tests with these tags\n    --exclude-tags \u003cT\u003e  Skip tests with these tags\n    --timeout \u003cSECS\u003e    Override default timeout\n    -v, --verbose       Show detailed output\n    --fail-fast         Stop on first failure\n\nOUTPUT EXAMPLE:\n    Running tests for: rust-error-handling\n    \n    tests/basic_load.yaml\n      [PASS] Skill loads correctly (0.12s)\n      \n    tests/commands.yaml\n      [PASS] rustc available (0.08s)\n      [PASS] cargo build works (1.23s)\n      [FAIL] clippy check (0.45s)\n            Assertion failed: exit_code expected 0, got 1\n            \n    tests/retrieval.yaml\n      [PASS] Error handling query (0.34s)\n      [SKIP] Advanced query (missing: rust-nightly)\n\n    Results: 4 passed, 1 failed, 1 skipped (2.22s)\n```\n\n### `ms test --all`\n\n```\nRun tests for all skills\n\nUSAGE:\n    ms test --all [OPTIONS]\n\nOPTIONS:\n    --parallel \u003cN\u003e      Run tests in parallel [default: 4]\n    --tags \u003cTAGS\u003e       Only run tests with these tags\n    --type \u003cTYPE\u003e       Only run tests of type: standard, retrieval, packing\n    --fail-fast         Stop on first failure\n    --report \u003cFILE\u003e     Write report to file\n\nOUTPUT EXAMPLE:\n    Running tests for all skills...\n    \n    rust-error-handling        [4/5 passed]  FAIL\n    rust-async                 [3/3 passed]  PASS\n    python-testing             [6/6 passed]  PASS\n    go-concurrency             [2/2 passed]  PASS\n    typescript-types           [5/5 passed]  PASS\n    \n    Summary: 20/21 tests passed across 5 skills\n    Failed: rust-error-handling/tests/commands.yaml:clippy check\n```\n\n### `ms test --type retrieval`\n\n```\nRun retrieval tests\n\nUSAGE:\n    ms test --type retrieval [OPTIONS]\n\nOPTIONS:\n    --skill \u003cSKILL\u003e     Test specific skill\n    --query \u003cQUERY\u003e     Test with specific query\n    --show-results      Show actual search results\n\nOUTPUT EXAMPLE:\n    Running retrieval tests...\n    \n    rust-error-handling\n      Query: \"How do I handle errors in Rust?\"\n      Expected: rust-error-handling at rank \u003c= 2\n      Actual: rank 1, score 0.92\n      [PASS]\n      \n      Query: \"Result vs Option in Rust\"\n      Expected: rust-error-handling at rank \u003c= 3\n      Actual: rank 4, score 0.71\n      [FAIL] Expected rank \u003c= 3, got 4\n    \n    Results: 1 passed, 1 failed\n```\n\n### `ms test --ci --junit`\n\n```\nRun tests in CI mode with JUnit output\n\nUSAGE:\n    ms test --ci [OPTIONS]\n\nOPTIONS:\n    --junit \u003cFILE\u003e      Write JUnit XML report\n    --html \u003cFILE\u003e       Write HTML report\n    --coverage          Include coverage information\n    --strict            Fail on any warnings\n    --timeout \u003cSECS\u003e    CI timeout [default: 300]\n\nEXAMPLE:\n    ms test --all --ci --junit test-results.xml\n\n    # In CI pipeline:\n    - name: Run skill tests\n      run: ms test --all --ci --junit results.xml\n      \n    - name: Upload test results\n      uses: actions/upload-artifact@v3\n      with:\n        name: test-results\n        path: results.xml\n```\n\n---\n\n## JUnit XML Output\n\n```rust\nimpl SkillTestReport {\n    /// Generate JUnit XML format\n    pub fn to_junit_xml(\u0026self) -\u003e String {\n        let mut xml = String::from(r#\"\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\"#);\n        xml.push_str(\"\\n\u003ctestsuites\u003e\\n\");\n        \n        xml.push_str(\u0026format!(\n            r#\"  \u003ctestsuite name=\"{}\" tests=\"{}\" failures=\"{}\" skipped=\"{}\" time=\"{:.3}\"\u003e\"#,\n            self.skill_id,\n            self.tests_run,\n            self.failed,\n            self.skipped,\n            self.duration.as_secs_f64()\n        ));\n        xml.push('\\n');\n        \n        for result in \u0026self.results {\n            xml.push_str(\u0026format!(\n                r#\"    \u003ctestcase name=\"{}\" time=\"{:.3}\"\u003e\"#,\n                result.name,\n                result.duration.as_secs_f64()\n            ));\n            \n            match result.status {\n                TestStatus::Failed | TestStatus::Timeout =\u003e {\n                    xml.push_str(\"\\n      \u003cfailure message=\\\"Test failed\\\"\u003e\");\n                    for failure in \u0026result.failures {\n                        xml.push_str(\u0026format!(\"\\n        {}\", failure));\n                    }\n                    xml.push_str(\"\\n      \u003c/failure\u003e\\n    \");\n                }\n                TestStatus::Skipped =\u003e {\n                    xml.push_str(\"\\n      \u003cskipped/\u003e\\n    \");\n                }\n                TestStatus::Passed =\u003e {}\n            }\n            \n            xml.push_str(\"\u003c/testcase\u003e\\n\");\n        }\n        \n        xml.push_str(\"  \u003c/testsuite\u003e\\n\");\n        xml.push_str(\"\u003c/testsuites\u003e\\n\");\n        \n        xml\n    }\n}\n```\n\n---\n\n## Error Handling\n\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum TestError {\n    #[error(\"Test file not found: {0}\")]\n    FileNotFound(PathBuf),\n    \n    #[error(\"Test parse error: {0}\")]\n    ParseError(#[from] serde_yaml::Error),\n    \n    #[error(\"Assertion failed: {0}\")]\n    AssertionFailed(String),\n    \n    #[error(\"Command failed: {0}\")]\n    CommandFailed(String),\n    \n    #[error(\"Timeout after {0:?}\")]\n    Timeout(Duration),\n    \n    #[error(\"IO error: {0}\")]\n    IoError(#[from] std::io::Error),\n    \n    #[error(\"Skill not found: {0}\")]\n    SkillNotFound(String),\n    \n    #[error(\"Missing requirement: {0}\")]\n    MissingRequirement(String),\n}\n```\n\n---\n\n## Dependencies\n\n- **Testing Strategy** (meta_skill-9ok): Overall testing approach and patterns\n- `serde`, `serde_yaml`: Test file parsing\n- `tempfile`: Temporary workspaces\n- `chrono`: Duration handling\n- Command execution utilities","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-13T23:01:54.751632719-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:29:59.365537105-05:00","labels":["phase-6","skill-tests","testing","validation"],"dependencies":[{"issue_id":"meta_skill-x7k","depends_on_id":"meta_skill-9ok","type":"blocks","created_at":"2026-01-13T23:04:15.813119863-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-y73","title":"Phase 3: Disclosure \u0026 Suggestions","description":"# Phase 3: Disclosure \u0026 Suggestions\n\nProgressive disclosure system and context-aware skill suggestions.\n\n## Core Components\n1. **Disclosure levels** - minimal (~100 tokens)  full (complete)\n2. **Micro-slicing** - Pre-slice skills into atomic blocks\n3. **Token packing** - Constrained optimization to fit budget\n4. **Conditional predicates** - Version-specific content filtering\n5. **Context-aware suggestions** - Based on cwd, files, commands\n6. **Swarm planning** - Multi-agent pack coordination (for NTM)\n7. **Meta-skills** - Composed slice bundles\n8. **Suggestion cooldowns** - Context fingerprints to prevent spam\n\n## Key Design Decisions\n- Slices are atomic: rule, command, example, pitfall\n- Packer uses constrained optimization, not greedy\n- Mandatory policy slices cannot be omitted\n- Predicates evaluated at load time, not by agent\n\n## Success Criteria\n- `ms load \u003cskill\u003e --level overview` works\n- `ms load \u003cskill\u003e --pack 2000` respects token budget\n- `ms suggest --cwd .` returns context-relevant skills\n- Predicate filtering strips irrelevant version content","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-13T22:20:53.633796121-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:53.633796121-05:00","dependencies":[{"issue_id":"meta_skill-y73","depends_on_id":"meta_skill-4ih","type":"blocks","created_at":"2026-01-13T22:21:01.852123545-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-yu1","title":"Phase 5: Bundles \u0026 Distribution","description":"# Phase 5: Bundles \u0026 Distribution\n\nSkill sharing, bundling, and multi-machine synchronization.\n\n## Core Components\n1. **Bundle format** - YAML manifest with skills + metadata\n2. **GitHub integration** - Publish/install from GitHub releases\n3. **Local modification safety** - Preserve customizations on update\n4. **Sync engine** - Three-way merge with conflict resolution\n5. **Multi-machine sync** - Machine identity, sync state tracking\n6. **One-URL sharing** - Share entire skill set via single URL\n7. **Backup system** - Automatic backups with retention\n\n## Key Design Decisions\n- Three-tier storage: upstream, local mods, merged\n- Conflict resolution is proposal-first\n- Patches stored, not full copies\n- SHA256 verification on install\n\n## Success Criteria\n- `ms bundle create my-skills` packages skills\n- `ms bundle publish --repo user/skills` pushes to GitHub\n- `ms bundle install user/skills` works\n- Local modifications survive updates\n- Conflict resolution surfaces differences clearly","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-13T22:20:55.238029107-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:20:55.238029107-05:00","dependencies":[{"issue_id":"meta_skill-yu1","depends_on_id":"meta_skill-4ki","type":"blocks","created_at":"2026-01-13T22:21:01.903929388-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-z2r","title":"CASS Mining: Performance Profiling Patterns","description":"Deep dive into CASS sessions about perf record, jemalloc allocation profiling, cargo bench profiling, RUSTFLAGS for frame pointers. Extract actionable skill patterns.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-13T17:47:14.537502827-05:00","created_by":"ubuntu","updated_at":"2026-01-13T18:18:35.692964305-05:00","closed_at":"2026-01-13T18:18:35.692964305-05:00","close_reason":"Section 30 (Performance Profiling Patterns) added to plan. Covers: methodology (hot path analysis, inefficiency patterns), SIMD/vectorization, Criterion benchmarks, profiling builds, I/O optimization, caching, and parallelism patterns.","labels":["cass-mining"]}
{"id":"meta_skill-z3c","title":"Skill Pruning \u0026 Evolution","description":"# Skill Pruning \u0026 Evolution\n\n## Section Reference\nSection 7.5 - Skill Pruning \u0026 Evolution\n\n## Overview\n\nAs the skill registry grows, ms must keep skills lean and current without destructive deletions. Pruning is **proposal-first**: identify candidates, suggest merges or deprecations, and require explicit confirmation before applying changes.\n\n## Why Pruning Matters\n\nWithout active pruning:\n- Registry becomes cluttered with stale/unused skills\n- Duplicate skills confuse users and agents\n- Quality degrades as outdated skills persist\n- Search results become noisy\n\nWith proposal-first pruning:\n- Users maintain control over deletions\n- Valuable skills aren't accidentally removed\n- Evolution happens through merge/deprecate, not delete\n- Full audit trail of changes\n\n## Pruning Signals\n\n### Low Usage\n```rust\nstruct UsageSignal {\n    skill_id: String,\n    uses_last_30_days: u32,\n    threshold: u32,  // e.g., \u003c5 uses\n}\n```\n\n### Low Quality Score\n```rust\nstruct QualitySignal {\n    skill_id: String,\n    quality_score: f32,\n    threshold: f32,  // e.g., \u003c0.3\n}\n```\n\n### High Similarity\n```rust\nstruct SimilaritySignal {\n    skill_a: String,\n    skill_b: String,\n    similarity: f32,\n    threshold: f32,  // e.g., \u003e= 0.8\n}\n```\n\n### Toolchain Mismatch\n```rust\nstruct ToolchainSignal {\n    skill_id: String,\n    expected_tools: Vec\u003cString\u003e,\n    missing_tools: Vec\u003cString\u003e,\n}\n```\n\n## Pruning Actions (Non-Destructive)\n\n### Propose Merge\nCombine two similar skills into one:\n```rust\nstruct MergeProposal {\n    source_skills: Vec\u003cString\u003e,\n    target_name: String,\n    auto_draft: SkillSpec,\n    rationale: String,\n}\n```\n\n### Propose Deprecate\nMark as deprecated with replacement alias:\n```rust\nstruct DeprecateProposal {\n    skill_id: String,\n    replacement_id: Option\u003cString\u003e,\n    rationale: String,\n}\n```\n\n### Propose Split\nBreak overly broad skill into focused children:\n```rust\nstruct SplitProposal {\n    source_skill: String,\n    children: Vec\u003cSkillSpec\u003e,\n    rationale: String,\n}\n```\n\n## CLI Interface\n\n```bash\n# Analyze registry for pruning candidates\nms prune --analyze\n\n# Show detailed proposals\nms prune --proposals\n\n# Interactive review of proposals\nms prune --review\n\n# Apply specific proposal\nms prune --apply merge:rust-errors-v1,rust-errors-v2\n\n# Dry-run mode\nms prune --apply merge:a,b --dry-run\n\n# Generate beads for review\nms prune --emit-beads\n```\n\n## Robot Mode Output\n\n```json\n{\n  \"status\": \"proposals_ready\",\n  \"proposals\": [\n    {\n      \"type\": \"merge\",\n      \"sources\": [\"rust-errors-v1\", \"rust-errors-v2\"],\n      \"target\": \"rust-error-handling\",\n      \"rationale\": \"High similarity (0.92), low individual usage\",\n      \"draft_path\": \".ms/proposals/merge-001.yaml\"\n    },\n    {\n      \"type\": \"deprecate\",\n      \"skill\": \"old-testing-guide\",\n      \"replacement\": \"modern-testing\",\n      \"rationale\": \"No usage in 60 days, superseded\"\n    }\n  ],\n  \"stats\": {\n    \"total_skills\": 150,\n    \"candidates\": 12,\n    \"merge_proposals\": 3,\n    \"deprecate_proposals\": 7,\n    \"split_proposals\": 2\n  }\n}\n```\n\n## Beads Integration\n\nWhen --emit-beads is used, pruning creates beads for tracking:\n```\nms-prune-001: Merge rust-errors-v1 + v2 [P2]\nms-prune-002: Deprecate old-testing-guide [P3]\n...\n```\n\n## Acceptance Criteria\n\n1. [ ] Usage tracking for pruning signals\n2. [ ] Quality score integration\n3. [ ] Similarity detection (via embeddings)\n4. [ ] Toolchain mismatch detection\n5. [ ] Merge proposal generation with auto-draft\n6. [ ] Deprecate proposal with alias\n7. [ ] Split proposal with child drafts\n8. [ ] CLI: ms prune --analyze\n9. [ ] Interactive review mode\n10. [ ] Dry-run support\n11. [ ] Beads emission for tracking\n12. [ ] Robot mode JSON output\n\n## Dependencies\n\n- Depends on: meta_skill-e5e (Skill Quality Scoring)\n- Depends on: meta_skill-r6k (Skill Alias System)\n- Depends on: meta_skill-ch6 (Hash Embeddings for similarity)","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-13T23:33:24.691038351-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:33:24.691038351-05:00","labels":["evolution","maintenance","phase-3"],"dependencies":[{"issue_id":"meta_skill-z3c","depends_on_id":"meta_skill-e5e","type":"blocks","created_at":"2026-01-13T23:33:31.76006781-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-z3c","depends_on_id":"meta_skill-r6k","type":"blocks","created_at":"2026-01-13T23:33:31.792780008-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-z3c","depends_on_id":"meta_skill-ch6","type":"blocks","created_at":"2026-01-13T23:33:31.822151681-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-z49","title":"[P4] Session Marking System","description":"# Session Marking System\n\nMark sessions as exemplary or anti-pattern for training.\n\n## Tasks\n1. Define SessionMark enum (exemplary, anti-pattern, neutral)\n2. Store marks in SQLite\n3. CLI for marking sessions\n4. Weight marks in extraction\n5. Propagate marks to patterns\n\n## Marking Types\n- exemplary: High-quality work, use for positive examples\n- anti-pattern: What not to do, use for pitfall extraction\n- neutral: Default, neither good nor bad\n\n## Storage\n```sql\nCREATE TABLE session_marks (\n    session_id TEXT PRIMARY KEY,\n    mark TEXT NOT NULL,\n    marked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    reason TEXT\n);\n```\n\n## CLI\n- `ms mark \u003csession_id\u003e exemplary --reason \"Clean workflow\"`\n- `ms mark \u003csession_id\u003e anti-pattern --reason \"Bad error handling\"`\n- `ms marks list` - Show marked sessions\n\n## Impact on Extraction\n- Exemplary: Higher weight in pattern aggregation\n- Anti-pattern: Used for counterexample extraction\n- Neutral: Standard weight\n\n## Acceptance Criteria\n- Sessions can be marked\n- Marks affect extraction weighting\n- Anti-patterns become pitfalls","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-13T22:25:47.552709662-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:47.552709662-05:00","labels":["curation","marking","phase-4"],"dependencies":[{"issue_id":"meta_skill-z49","depends_on_id":"meta_skill-hhu","type":"blocks","created_at":"2026-01-13T22:26:13.020300992-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-zno","title":"[P5] Multi-Machine Sync","description":"# Multi-Machine Sync\n\nSynchronize skills across multiple machines.\n\n## Tasks\n1. Machine identity (unique ID per installation)\n2. Sync state tracking\n3. Conflict detection across machines\n4. Push/pull operations\n5. Optional Git-based sync backend\n\n## Machine Identity\n- Generate UUID on first run\n- Store in .ms/machine_id\n- Include in sync metadata\n\n## Sync State\n```sql\nCREATE TABLE sync_state (\n    skill_id TEXT PRIMARY KEY,\n    local_version TEXT,\n    remote_version TEXT,\n    last_synced TIMESTAMP,\n    machine_id TEXT\n);\n```\n\n## Sync Protocol\n1. `ms sync pull` - Fetch changes from remote\n2. `ms sync push` - Push local changes to remote\n3. `ms sync status` - Show pending changes\n4. Auto-sync on bundle operations\n\n## Git Backend (Optional)\n- Store skills in Git repo\n- Sync via git pull/push\n- Leverage Git merge for conflicts\n\n## Acceptance Criteria\n- Sync works across machines\n- Conflicts detected and resolved\n- Git backend optional but supported","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T22:27:05.947965319-05:00","created_by":"ubuntu","updated_at":"2026-01-13T23:42:14.612186991-05:00","closed_at":"2026-01-13T23:42:14.612186991-05:00","close_reason":"Duplicate of meta_skill-ujr (Multi-Machine Synchronization)","labels":["multi-machine","phase-5","sync"],"dependencies":[{"issue_id":"meta_skill-zno","depends_on_id":"meta_skill-swe","type":"blocks","created_at":"2026-01-13T22:27:15.45696713-05:00","created_by":"ubuntu"}]}
{"id":"meta_skill-ztm","title":"[P4] ms build Command","description":"# ms build Command\n\nMain interface for skill generation from CASS.\n\n## Subcommands\n- `ms build --from-cass \"topic\"` - Extract from sessions\n- `ms build --guided` - Interactive TUI\n- `ms build --autonomous` - Long-running mode\n- `ms build --resume \u003cid\u003e` - Resume interrupted\n- `ms build --dry-run` - Preview without saving\n\n## Options\n- --topic: Search topic for sessions\n- --sessions: Specific session IDs\n- --min-quality: Quality threshold\n- --duration: Autonomous duration\n- --output: Output directory\n- --robot: JSON output\n\n## Workflow\n1. Query CASS for relevant sessions\n2. Score and filter sessions\n3. Extract patterns\n4. Transform to general rules\n5. Compile skill spec\n6. Save and index\n\n## Output\n- skill.spec.yaml in output directory\n- Compiled SKILL.md\n- provenance.json with evidence links\n\n## Acceptance Criteria\n- All modes work correctly\n- Skills saved and indexed\n- Provenance preserved","status":"open","priority":0,"issue_type":"feature","created_at":"2026-01-13T22:25:53.762279786-05:00","created_by":"ubuntu","updated_at":"2026-01-13T22:25:53.762279786-05:00","labels":["build","cli","phase-4"],"dependencies":[{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-330","type":"blocks","created_at":"2026-01-13T22:26:13.235514942-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-ans","type":"blocks","created_at":"2026-01-13T22:26:13.261778288-05:00","created_by":"ubuntu"},{"issue_id":"meta_skill-ztm","depends_on_id":"meta_skill-1p7","type":"blocks","created_at":"2026-01-13T22:26:13.288116355-05:00","created_by":"ubuntu"}]}
